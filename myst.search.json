{"version":"1","records":[{"hierarchy":{"lvl1":"OSDF Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"OSDF Cookbook"},"content":"\n\n\n\n\n\n\n\n\n\nThis Project Pythia Cookbook covers using the Open Science Data Federation (OSDF), a service for streaming scientific data across the globe.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":2},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Motivation"},"content":"Have you ever been frustrated by the complications of accessing scientific data?  Why can’t it “just work”, like watching a Netflix movie?\n\nThe OSDF is a service that simplifies the streaming of a wide range of scientific datasets with a goal that data access “just works”.  It\nis meant to improve data availability for researchers working at any scale from individual laptops to distributed computing services\nsuch as the OSG’s \n\nOSPool.\n\nThis cookbook gives motivating use cases from the geoscience community, including using datasets from NSF NCAR’s \n\nGeoscience Data Exchange (GDEX) and the datasets of AWS \n\nOpenData.","type":"content","url":"/#motivation","position":3},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":4},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Authors"},"content":"Harsha R. Hampapura\n\n\nBrian Bockelman\n\n\nAlexander Hoelzemann\n\n\nCarrie Wall\n\n\nEmma Turetsky\n\n\nAmandha Wingert Barok\n\n\nAashish Panta\n\n\nJoanmarie Del Vecchio\n\n\nJustin Hiemstra\n\n\nDouglas Schuster\n\n\nRiley Conroy\n\n\nKibiwott Koech","type":"content","url":"/#authors","position":5},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":6},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":7},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":8},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Structure"},"content":"This cookbook is broken up into two pieces - some background knowledge on the OSDF service itself\nand then a series of motivating examples from different repositories accessible via the OSDF.","type":"content","url":"/#structure","position":9},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"OSDF Fundamentals","lvl2":"Structure"},"type":"lvl3","url":"/#osdf-fundamentals","position":10},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"OSDF Fundamentals","lvl2":"Structure"},"content":"What is the OSDF?  Who supports it? How can it benefit from my science?  A dive into the infrastructure itself.","type":"content","url":"/#osdf-fundamentals","position":11},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using Datasets from NCAR’s Geoscience Data Exchange","lvl2":"Structure"},"type":"lvl3","url":"/#using-datasets-from-ncars-geoscience-data-exchange","position":12},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using Datasets from NCAR’s Geoscience Data Exchange","lvl2":"Structure"},"content":"NSF NCAR’s \n\nGeoscience Data Exchange (GDEX) contains a large collection of meteorological, atmospheric composition, and oceanographic observations, and operational and reanalysis model outputs, integrated with NSF NCAR High Performance Compute services to support atmospheric and geosciences research. This chapter demonstrates how to use common data science tools when streaming from the GDEX.","type":"content","url":"/#using-datasets-from-ncars-geoscience-data-exchange","position":13},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using Datasets from FIU’s Envistor","lvl2":"Structure"},"type":"lvl3","url":"/#using-datasets-from-fius-envistor","position":14},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using Datasets from FIU’s Envistor","lvl2":"Structure"},"content":"Florida International University (FIU) runs the \n\nEnvistor project, aggregating climate datasets from the south Florida region.","type":"content","url":"/#using-datasets-from-fius-envistor","position":15},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using NOAA’s SONAR Fisheries Datasets","lvl2":"Structure"},"type":"lvl3","url":"/#using-noaas-sonar-fisheries-datasets","position":16},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using NOAA’s SONAR Fisheries Datasets","lvl2":"Structure"},"content":"NOAA maintains a copy of its SONAR-based datasets of Atlanta fisheries data in the popular Zarr format.  This chapter shows how to load and use the datasets and fuse it with other products.","type":"content","url":"/#using-noaas-sonar-fisheries-datasets","position":17},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using Sentinel Data from AWS","lvl2":"Structure"},"type":"lvl3","url":"/#using-sentinel-data-from-aws","position":18},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using Sentinel Data from AWS","lvl2":"Structure"},"content":"All of AWS OpenData is connected to the OSDF!  This chapter includes examples of streaming Sentinel-2 data, stored in AWS’s OpenData program, to your notebook.","type":"content","url":"/#using-sentinel-data-from-aws","position":19},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":20},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\nBinder or on your local machine.","type":"content","url":"/#running-the-notebooks","position":21},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":22},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you’ll be able to execute\nand even change the example programs. You’ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.\n\nNote, not all Cookbook chapters are executable. If you do not see\nthe rocket ship icon, such as on this page, you are not viewing an\nexecutable book chapter.","type":"content","url":"/#running-on-binder","position":23},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":24},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will need to follow this workflow:\n\nClone the https://github.com/ProjectPythia/osdf-cookbook repository: git clone https://github.com/ProjectPythia/osdf-cookbook.git\n\nMove into the osdf-cookbook directorycd osdf-cookbook\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate osdf-cookbook\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":25},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)"},"type":"lvl1","url":"/notebooks/osdf-intro","position":0},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)"},"content":"\n\nHave you ever pondered why accessing large-scale scientific data is so complicated, while accessing large-scale volumes of movies is so simple on Netflix?\n\nEach data repository has its own website or a set of unique tools for accessing data.  Users are often encouraged to download datasets locally and then do local computations, as repositories prioritize long-term storage and preservation rather than fast or distributed access.\n\nHow does Netflix do it without making you download the whole movie ahead of time?  They leverage a content distribution network (CDN), which caches copies of the most popular movies at opportune locations on the Internet closer to users. They also let you stream your favorite shows so you can start watching while later sections of the show are still downloading.\n\nThe \n\nOSDF, an NSF-funded infrastructure providing a CDN for science, makes this kind of streaming possible for scientific data.  It is connected to popular open science repositories and has hardware embedded across US and international networks and at large computing sites.\n\nThis cookbook provides examples of using the OSDF’s streaming to power science use cases in earth sciences.","type":"content","url":"/notebooks/osdf-intro","position":1},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl2":"Do First, Understand Later"},"type":"lvl2","url":"/notebooks/osdf-intro#do-first-understand-later","position":2},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl2":"Do First, Understand Later"},"content":"How Do You Use the OSDF?\n\nThe service is powered by the same protocol as the web, HTTPS.  Thus, the simplest use case is to download an object by using the browser.\n\nClick on this link:\n\nhttps://​osdf​-director​.osg​-htc​.org​/ospool​/uc​-shared​/public​/OSG​-Staff​/validation​/test​.txt\n\nIf a new tab opened with the text “Hello, World” – congratulations, you used the OSDF!\n\nOSDF is often used in conjunction with computing workflows and downloads occur as part of a script.  For this, a command line client - \n\npelican is utilized.  Try running the following:\n\npelican object get osdf:///routeviews/chicago/route-views.chicago/bgpdata/2025.03/RIBS/rib.20250319.0400.bz2 ./\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDepending on the speed of your Internet connection, you may see a progress bar as the download proceeds.\n\nCongratulations, you’re now the proud owner of 72MB of Internet routing data!\n\nFor both of these cases, we downloaded the entire object.  What happens if the dataset contains data for the entire planet but you are only interested in the state of Nebraska?  It’s more effective to stream the subset.  For that, we will use the \n\nPelican Python library; this library will be used throughout the remaining chapters of this cookbook.","type":"content","url":"/notebooks/osdf-intro#do-first-understand-later","position":3},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl2":"About the OSDF"},"type":"lvl2","url":"/notebooks/osdf-intro#about-the-osdf","position":4},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl2":"About the OSDF"},"content":"You don’t need to know how Netflix is built to press “play”.  Similarly, you don’t need to understand the guts of the OSDF to use it in your science.  However, a few key concepts are useful!\n\nOSDF Infrastructure: The map below shows the distributed pieces of the OSDF:\n\nEach “O” on the map is an origin; the origin service connects an existing repository to the OSDF service, making some datasets available and protecting the repository from overload.  Origins are typically placed nearby where the data lives; the origin for the NCAR Geoscience Data Exchange (GDEX) is in the same datacenter as the GDEX.\n\nEach “C” is a cache.  The cache makes temporary copies of objects upon access so, on subsequent accesses, the object comes from the cache and not from the repository.  This reduces the load on the repository and, ideally, increases scalability.\n\nUnified Namespace: The OSDF provides a unified namespace for all available objects.  Each repository receives a unique prefix (the IceCube experiment’s data is available from /icecube; NCAR’s GDEX is available from /ncar/gdex) and the object can be referenced from within the prefix.\n\nFrom our RouteViews example above, we were interested in accessing the object named chicago/route-views.chicago/bgpdata/2025.03/RIBS/rib.20250319.0400.bz2.  Since the prefix for RouteViews is /routeviews, the entire OSDF name is:osdf://routeviews/chicago/route-views.chicago/bgpdata/2025.03/RIBS/rib.20250319.0400.bz2\n\n“Objects” vs “Files”: You may have noticed that this notebook refers to downloading/streaming “objects” instead of “files”. What’s the difference, and why does OSDF bother making this distinction?\n\nBoth objects and files are ways for computers to store data, and in practice, the earth science calculations in this cookbook use them the same way—regardless of where the data comes from.\n\nThe key difference is the way we typically think about accessing or retrieving that data: When you open files like Word documents or images on your computer, you probably click through folders or directories to find them. But when you’re working with data over the internet, that folder-based structure doesn’t always apply.\n\nIn the OSDF, an object is simply a piece of data that can be shared, like a file—but without needing to think about where it’s stored or how it’s organized on someone else’s computer. “Object” is a more flexible term that works better when data is stored in large systems across many locations.\n\nWarning\n\nImmutable Objects: The OSDF assumes that objects are immutable; once created, they aren’t permitted to be changed.  This allows OSDF to effectively make copies of the objects in the caches.\n\nThis typically works well with scientific datasets: you rarely change your data after you record it!  However, if you start using OSDF more heavily, this is an important requirement to be aware of.","type":"content","url":"/notebooks/osdf-intro#about-the-osdf","position":5},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl2":"Finding My Objects"},"type":"lvl2","url":"/notebooks/osdf-intro#finding-my-objects","position":6},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl2":"Finding My Objects"},"content":"How do you find the object you’re interested in?\n\nTypically, dataset providers connected to the OSDF provide a search, data catalog, or STAC catalog publishing OSDF-style URLs.  You can determine this from the provider’s website; additionally, OSDF maintains a \n\nlist of known links you can peruse.\n\nExplore this cookbook: This cookbook provides examples for how to use OSDF to access:\n\nNCAR’s Geoscience Data Exchange.\n\nAWS’s OpenData Program.\n\nThe Envistor platform at Florida International University.\n\n","type":"content","url":"/notebooks/osdf-intro#finding-my-objects","position":7},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/osdf-intro#summary","position":8},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl2":"Summary"},"content":"In this notebook we gave a basic introduction to the OSDF and Pelican.","type":"content","url":"/notebooks/osdf-intro#summary","position":9},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/osdf-intro#whats-next","position":10},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl3":"What’s next?","lvl2":"Summary"},"content":"In the next notebook, we will show a more indepth overview of the Pelican FSSpec client PelicanFS as well as a few usage examples.","type":"content","url":"/notebooks/osdf-intro#whats-next","position":11},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF"},"type":"lvl1","url":"/notebooks/pelicanfs","position":0},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF"},"content":"\n\n","type":"content","url":"/notebooks/pelicanfs","position":1},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/pelicanfs#overview","position":2},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Overview"},"content":"Now that you’ve learned about the OSDF and the Pelican command line client, you may be wondering how you can easily access that data from within a notebook using python.\n\nYou can do this using PelicanFS, which is an FSSPec implementation of the Pelican client.","type":"content","url":"/notebooks/pelicanfs#overview","position":3},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"This notebook will contain:","lvl2":"Overview"},"type":"lvl3","url":"/notebooks/pelicanfs#this-notebook-will-contain","position":4},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"This notebook will contain:","lvl2":"Overview"},"content":"A brief explanation of FSSPec and PelicanFS\n\nA real-world example using FSSPec, Pelican, Xarray, and Zarr\n\nOther common access patterns\n\nFAQs\n\n","type":"content","url":"/notebooks/pelicanfs#this-notebook-will-contain","position":5},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/pelicanfs#prerequisites","position":6},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Prerequisites"},"content":"To better understand this notebook, please familiarize yourself with the following concepts:\n\nConcepts\n\nImportance\n\nNotes\n\nIntro to OSDF\n\nNecessary\n\n\n\nUnderstanding of Xarray\n\nHelpful\n\nTo better understand the example workflow\n\nOverview of FSSpec\n\nHelpful\n\nTo better understand the FSSpec library\n\nTime to learn: 20-30 minutes\n\n\n\n","type":"content","url":"/notebooks/pelicanfs#prerequisites","position":7},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/pelicanfs#imports","position":8},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Imports"},"content":"\n\nimport xarray as xr\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport metpy.calc as mpcalc\nfrom metpy.units import units\nimport fsspec\nimport intake\n\n\n\n","type":"content","url":"/notebooks/pelicanfs#imports","position":9},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"What are PelicanFS and FSSPec?"},"type":"lvl2","url":"/notebooks/pelicanfs#what-are-pelicanfs-and-fsspec","position":10},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"What are PelicanFS and FSSPec?"},"content":"First, let’s understand PelicanFS and how it integrates with FSSpec\n\n","type":"content","url":"/notebooks/pelicanfs#what-are-pelicanfs-and-fsspec","position":11},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"FSSPec","lvl2":"What are PelicanFS and FSSPec?"},"type":"lvl3","url":"/notebooks/pelicanfs#fsspec","position":12},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"FSSPec","lvl2":"What are PelicanFS and FSSPec?"},"content":"FileSystem Spec (fsspec) is a python library which endeavors to provide a unified interface to many different storage backends. This includes, but is not limited to, POSIX, https, and S3. It’s used by various data processing libraries such as xarray, pandas, and intake, just to name a few.\n\nTo learn more about FSSPec, visit its \n\ninformation page.","type":"content","url":"/notebooks/pelicanfs#fsspec","position":13},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl4":"Schemes","lvl3":"FSSPec","lvl2":"What are PelicanFS and FSSPec?"},"type":"lvl4","url":"/notebooks/pelicanfs#schemes","position":14},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl4":"Schemes","lvl3":"FSSPec","lvl2":"What are PelicanFS and FSSPec?"},"content":"FSSpec figures out how to interact with data from different storage backends through the scheme in the data path. For example, FSSpec knows to use the “Hyper Text Transfer Protocol” interface whenever it sees URLs with the https: scheme. This lets users interact with data from a variety of storage technologies without forcing them to know how those technologies work under the hood.","type":"content","url":"/notebooks/pelicanfs#schemes","position":15},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"PelicanFS","lvl2":"What are PelicanFS and FSSPec?"},"type":"lvl3","url":"/notebooks/pelicanfs#pelicanfs","position":16},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"PelicanFS","lvl2":"What are PelicanFS and FSSPec?"},"content":"PelicanFS is an implementation of FSSpec that introduces two new schemes to FSSpec: pelican and osdf. PelicanFS enables you to use the pelican:// scheme to access data via Pelican Federations like the OSDF in any software that already understands FSSpec. To use it, you must specify the federation host name. A Pelican path looks like:\n\npelican://<federation-host-name>/<namespace-path>\n\nThe osdf scheme is a specific instance of the pelican scheme that knows how to access the OSDF. A path using the osdf scheme should not provide the federation root. An OSDF path looks like:\n\nosdf:///<namespace-path>\n\nPelicanFS teaches FSSpec how to interact with the Pelican protocol using the above pelican:-schemed or osdf:-schemed URLs.\n\nNote\n\nNotice the three ‘/’ after “osdf:”. This is required for a properly-formed osdf path.\n\nIf you’d like to understand more about how pelican works, check out the documentation \n\nhere.","type":"content","url":"/notebooks/pelicanfs#pelicanfs","position":17},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Putting it all together","lvl2":"What are PelicanFS and FSSPec?"},"type":"lvl3","url":"/notebooks/pelicanfs#putting-it-all-together","position":18},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Putting it all together","lvl2":"What are PelicanFS and FSSPec?"},"content":"What does this mean in practice?\n\nIf you want to access data from the OSDF using FSSpec or any library that uses FSSpec, build the osdf-schemed URL for the data and use that URL as your data path and then FSSpec and PelicanFS will do all the work to resolve it behind the scenes.\n\n\n\n","type":"content","url":"/notebooks/pelicanfs#putting-it-all-together","position":19},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"A PelicanFS Example using Real Data"},"type":"lvl2","url":"/notebooks/pelicanfs#a-pelicanfs-example-using-real-data","position":20},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"A PelicanFS Example using Real Data"},"content":"The following is an example that shows how PelicanFS works on real world data using FSSPec and Xarray to access Zarr data from AWS.\n\nThis portion of the notebook is based off of the \n\nProject Pythia HRRR AWS Cookbook\n\n","type":"content","url":"/notebooks/pelicanfs#a-pelicanfs-example-using-real-data","position":21},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Setting the Proper Path","lvl2":"A PelicanFS Example using Real Data"},"type":"lvl3","url":"/notebooks/pelicanfs#setting-the-proper-path","position":22},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Setting the Proper Path","lvl2":"A PelicanFS Example using Real Data"},"content":"The data for this tutorial is part of AWS Open Data, hosted in the us-west-1 region. The OSDF provides access to that region using the /aws-opendata/us-west-1 namespace.\n\nLet’s first create a path which uses the osdf scheme.\n\n# Set the date, hour, variable, and level for the HRRR data\ndate = '20211016'\nhour = '21'\nvar = 'TMP'\nlevel = '2m_above_ground'\n\n# Construct object paths for the Zarr datasets using the osdf scheme\nnamespace_object1 = f'osdf:///aws-opendata/us-west-1/hrrrzarr/sfc/{date}/{date}_{hour}z_anl.zarr/{level}/{var}/{level}/'\nnamespace_object2 = f'osdf:///aws-opendata/us-west-1/hrrrzarr/sfc/{date}/{date}_{hour}z_anl.zarr/{level}/{var}/'\n\n\n\n","type":"content","url":"/notebooks/pelicanfs#setting-the-proper-path","position":23},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Using FSSpec to access the data","lvl2":"A PelicanFS Example using Real Data"},"type":"lvl3","url":"/notebooks/pelicanfs#using-fsspec-to-access-the-data","position":24},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Using FSSpec to access the data","lvl2":"A PelicanFS Example using Real Data"},"content":"Now we can access the data using XArray as usual. The two objects will be accessed using fsspec’s get_mapper function, which knows to use PelicanFS because we created the path using the osdf scheme.\n\n# Get mappers for the Zarr datasets\n\nobject1 = fsspec.get_mapper(namespace_object1)\nobject2 = fsspec.get_mapper(namespace_object2)\n\n# Open the datasets\nds = xr.open_mfdataset([object1, object2], engine='zarr', decode_timedelta=True)\n\n# Display the dataset\nds\n\n\n\n","type":"content","url":"/notebooks/pelicanfs#using-fsspec-to-access-the-data","position":25},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Continue the workflow","lvl2":"A PelicanFS Example using Real Data"},"type":"lvl3","url":"/notebooks/pelicanfs#continue-the-workflow","position":26},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Continue the workflow","lvl2":"A PelicanFS Example using Real Data"},"content":"As you can see, Xarray streamed the data correctly into the datasets. To prove the workflow works, the next cell continues the computation and generates two plots. This tutorial will not go in depth as to what this code is accomplishing.\n\nIf you’d like to know more about the following workflow, please refer to the \n\nProject Pythia HRRR AWS Cookbook\n\n# Define coordinates for projection\nlon1 = -97.5\nlat1 = 38.5\nslat = 38.5\n\n# Define the Lambert Conformal projection\nprojData = ccrs.LambertConformal(\n    central_longitude=lon1,\n    central_latitude=lat1,\n    standard_parallels=[slat, slat],\n    globe=ccrs.Globe(\n        semimajor_axis=6371229,\n        semiminor_axis=6371229\n    )\n)\n\n# Display dataset coordinates\nds.coords\n\n# Extract temperature data\nairTemp = ds.TMP\n\n# Display the temperature data\nairTemp\n\n# Convert temperature units to Celsius\nairTemp = airTemp.metpy.convert_units('degC')\n\n# Display the converted temperature data\nairTemp\n\n# Extract projection coordinates\nx = airTemp.projection_x_coordinate\ny = airTemp.projection_y_coordinate\n\n# Plot temperature data\nairTemp.plot(figsize=(11, 8.5))\n\n# Compute minimum and maximum temperatures\nminTemp = airTemp.min().compute()\nmaxTemp = airTemp.max().compute()\n\n# Display minimum and maximum temperature values\nminTemp.values, maxTemp.values\n\n# Define contour levels\nfint = np.arange(np.floor(minTemp.values), np.ceil(maxTemp.values) + 2, 2)\n\n# Define plot bounds and resolution\nlatN = 50.4\nlatS = 24.25\nlonW = -123.8\nlonE = -71.2\nres = '50m'\n\n# Create a figure and axis with projection\nfig = plt.figure(figsize=(18, 12))\nax = plt.subplot(1, 1, 1, projection=projData)\nax.set_extent([lonW, lonE, latS, latN], crs=ccrs.PlateCarree())\nax.add_feature(cfeature.COASTLINE.with_scale(res))\nax.add_feature(cfeature.STATES.with_scale(res))\n\n# Add the title\ntl1 = 'HRRR 2m temperature ($^\\\\circ$C)'\ntl2 = f'Analysis valid at: {hour}00 UTC {date}'\nplt.title(f'{tl1}\\n{tl2}', fontsize=16)\n\n# Contour fill\nCF = ax.contourf(x, y, airTemp, levels=fint, cmap=plt.get_cmap('coolwarm'))\n\n# Make a colorbar for the ContourSet returned by the contourf call\ncbar = fig.colorbar(CF, shrink=0.5)\ncbar.set_label(r'2m Temperature ($^\\circ$C)', size='large')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/notebooks/pelicanfs#continue-the-workflow","position":27},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Other Ways to Access"},"type":"lvl2","url":"/notebooks/pelicanfs#other-ways-to-access","position":28},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Other Ways to Access"},"content":"There are other common ways to access data and use data with FSSpec and PelicanFS. This section will will cover the following topics\n\nUsing an Intake Catalog\n\nDirectly Accessing Data\n\n","type":"content","url":"/notebooks/pelicanfs#other-ways-to-access","position":29},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Intake Catalog","lvl2":"Other Ways to Access"},"type":"lvl3","url":"/notebooks/pelicanfs#intake-catalog","position":30},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Intake Catalog","lvl2":"Other Ways to Access"},"content":"In order to use PelicanFS with an Intake catalog, the paths in the catalog itself need to use the osdf or pelican schemes.\n\nHere’s an example using the catalog located at \n\nhttps://​data​.gdex​.ucar​.edu​/d850001​/catalogs​/osdf​/cmip6​-aws​/cmip6​-osdf​-zarr​.json\n\nAn entry in the catalog’s csv file looks like:\n\nHighResMIP,CMCC,CMCC-CM2-HR4,highresSST-present,r1i1p1f1,Amon,ta,gn,osdf:///aws-opendata/us-west-2/cmip6-pds/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/highresSST-present/r1i1p1f1/Amon/ta/gn/v20170706/,,20170706\n\nNotice how the path is using the ‘osdf’ scheme and the ‘/aws-opendata/us-west-2’ namespace. If all the paths in the csv file are formatted like this, then you can use the Intake catalog exactly as usual.\n\nHere is a workflow and plot which uses an Intake catalog and the osdf scheme. If you want to understand more about the underlying workflow, please look at the \n\nGlobal Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data notebook.\n\ngdex_url    =  'https://data.gdex.ucar.edu/'\ncat_url     = gdex_url +  'd850001/catalogs/osdf/cmip6-aws/cmip6-osdf-zarr.json'\n\ncol = intake.open_esm_datastore(cat_url)\n\nexpts = ['historical']\n\nquery = dict(\n    experiment_id=expts,\n    table_id='Amon',\n    variable_id=['tas'],\n    member_id = 'r1i1p1f1',\n    #activity_id = 'CMIP',\n)\n\ncol_subset = col.search(require_all_on=[\"source_id\"], **query)\n\nds = xr.open_zarr(col_subset.df['zstore'][0])\n\nds.tas.isel(time=0).plot()\n\n\n\n","type":"content","url":"/notebooks/pelicanfs#intake-catalog","position":31},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Direct Access of Data","lvl2":"Other Ways to Access"},"type":"lvl3","url":"/notebooks/pelicanfs#direct-access-of-data","position":32},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Direct Access of Data","lvl2":"Other Ways to Access"},"content":"You can also access the data directly using normal file system calls.\n\nFor example, let’s say you want to read in a csv object from the OSDF. Just use the same pattern we’ve shown before of\n\nosdf:///<namespace-path>\n\nfor your path.\n\nwith fsspec.open('osdf:///ndp/burnpro3d/YosemiteBurnExample/burnpro3d-yosemite-example.csv') as ex_csv:\n    content = ex_csv.read()\n    print(content.decode())\n\n\n\n","type":"content","url":"/notebooks/pelicanfs#direct-access-of-data","position":33},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/pelicanfs#summary","position":34},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Summary"},"content":"In this notebook we demonstrated how to use PelicanFS and gave an overview of a few different common usages. The main example showed how to use PelicanFS and Xarray to open a Zarr store. We also showed how to use PelicanFS and an intake catalog.","type":"content","url":"/notebooks/pelicanfs#summary","position":35},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/pelicanfs#whats-next","position":36},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"What’s next?","lvl2":"Summary"},"content":"The following notebooks all demonstrate various workflows which will use PelicanFS to access data from the OSDF.","type":"content","url":"/notebooks/pelicanfs#whats-next","position":37},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data"},"type":"lvl1","url":"/notebooks/cesm2-oceanheat","position":0},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data"},"content":"\n\n","type":"content","url":"/notebooks/cesm2-oceanheat","position":1},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Overview"},"type":"lvl3","url":"/notebooks/cesm2-oceanheat#overview","position":2},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Overview"},"content":"This notebook is an adpation of a \n\nworkflow in the NCAR gallery of the Pangeo collection\n\nThis notebook illustrates how to compute surface ocean heat content using potential temperature data from \n\nCESM2 Large Ensemble Dataset (Community Earth System Model 2) hosted on NCAR’s GDEX.\n\nThis data is open access and is accessed via OSDF using the pelicanFS package and demonstrates how you can stream data from NCAR’s GDEX\n\nPlease refer to the first chapter of this cookbook to learn more about OSDF, pelican or pelicanFS\n\n","type":"content","url":"/notebooks/cesm2-oceanheat#overview","position":3},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Prerequisites"},"type":"lvl3","url":"/notebooks/cesm2-oceanheat#prerequisites","position":4},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Intake-ESM\n\nNecessary\n\nUsed for searching CMIP6 data\n\nUnderstanding of Zarr\n\nHelpful\n\nFamiliarity with metadata structure\n\nMatplotlib\n\nHelpful\n\nPackage used for plotting\n\nPelicanFS\n\nNecessary\n\nThe python package used to stream data in this notebook\n\nOSDF\n\nHelpful\n\nOSDF is used to stream data in this notebook\n\nTime to learn: 20 mins\n\n\n\n","type":"content","url":"/notebooks/cesm2-oceanheat#prerequisites","position":5},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Imports"},"type":"lvl3","url":"/notebooks/cesm2-oceanheat#imports","position":6},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Imports"},"content":"\n\nimport intake\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nimport seaborn as sns\nimport re\nimport matplotlib.pyplot as plt\nimport dask\nfrom dask.distributed import LocalCluster\nimport pelicanfs \nimport cf_units as cf\n\n\n\n# Load Catalog URL\ncat_url = 'https://stratus.gdex.ucar.edu/d010092/catalogs/d010092-osdf-zarr.json'\n\n\n\n","type":"content","url":"/notebooks/cesm2-oceanheat#imports","position":7},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Set up local dask cluster"},"type":"lvl3","url":"/notebooks/cesm2-oceanheat#set-up-local-dask-cluster","position":8},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Set up local dask cluster"},"content":"\n\nBefore we do any computation let us first set up a local cluster using dask\n\ncluster = LocalCluster()          \nclient = cluster.get_client()\n\n\n\n# Scale the cluster\nn_workers = 5\ncluster.scale(n_workers)\ncluster\n\n\n\n","type":"content","url":"/notebooks/cesm2-oceanheat#set-up-local-dask-cluster","position":9},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Data Loading"},"type":"lvl3","url":"/notebooks/cesm2-oceanheat#data-loading","position":10},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Data Loading"},"content":"","type":"content","url":"/notebooks/cesm2-oceanheat#data-loading","position":11},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl4":"Load CESM2 LENS data from NCAR’s GDEX","lvl3":"Data Loading"},"type":"lvl4","url":"/notebooks/cesm2-oceanheat#load-cesm2-lens-data-from-ncars-gdex","position":12},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl4":"Load CESM2 LENS data from NCAR’s GDEX","lvl3":"Data Loading"},"content":"Load CESM2 LENS zarr data from GDEx using an intake-ESM catalog that has OSDF links\n\nFor more details regarding the dataset. See, \n\nhttps://​gdex​.ucar​.edu​/datasets​/d010092​/#\n\ncol = intake.open_esm_datastore(cat_url)\ncol\n\n\n\n# Uncomment this line to see all the variables\n# cesm_cat.df['variable'].values\n\n\n\ncesm_temp = col.search(variable ='TEMP', frequency ='monthly')\ncesm_temp\n\n\n\ncesm_temp.df['path'].values\n\n\n\nNote\n\nNote that all the file paths start with \n\nhttps://​data​-osdf​.gdex​.ucar​.edu indicating that the data will be streamed via OSDF\n\ndsets_cesm = cesm_temp.to_dataset_dict()\n\n\n\n\n\n\n\ndsets_cesm.keys()\n\n\n\nhistorical       = dsets_cesm['ocn.historical.monthly.cmip6']\nfuture_smbb      = dsets_cesm['ocn.ssp370.monthly.smbb']\nfuture_cmip6     = dsets_cesm['ocn.ssp370.monthly.cmip6']\n\n\n\n","type":"content","url":"/notebooks/cesm2-oceanheat#load-cesm2-lens-data-from-ncars-gdex","position":13},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl4":"Change units","lvl3":"Data Loading"},"type":"lvl4","url":"/notebooks/cesm2-oceanheat#change-units","position":14},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl4":"Change units","lvl3":"Data Loading"},"content":"\n\norig_units = cf.Unit(historical.z_t.attrs['units'])\norig_units\n\n\n\ndef change_units(ds, variable_str, variable_bounds_str, target_unit_str):\n    orig_units = cf.Unit(ds[variable_str].attrs['units'])\n    target_units = cf.Unit(target_unit_str)\n    variable_in_new_units = xr.apply_ufunc(orig_units.convert, ds[variable_bounds_str], target_units, dask='parallelized', output_dtypes=[ds[variable_bounds_str].dtype])\n    return variable_in_new_units\n\n\n\ndepth_levels_in_m = change_units(historical, 'z_t', 'z_t', 'm')\nhist_temp_in_degK = change_units(historical, 'TEMP', 'TEMP', 'degK')\nfut_cmip6_temp_in_degK = change_units(future_cmip6, 'TEMP', 'TEMP', 'degK')\nfut_smbb_temp_in_degK = change_units(future_smbb, 'TEMP', 'TEMP', 'degK')\n#\nhist_temp_in_degK  = hist_temp_in_degK.assign_coords(z_t=(\"z_t\", depth_levels_in_m['z_t'].data))\nhist_temp_in_degK[\"z_t\"].attrs[\"units\"] = \"m\"\nhist_temp_in_degK\n\n\n\ndepth_levels_in_m.isel(z_t=slice(0, -1))\n\n\n\nCompute depth level deltas using the difference of z_t levels\n\ndepth_level_deltas = depth_levels_in_m.isel(z_t=slice(1, None)).values - depth_levels_in_m.isel(z_t=slice(0, -1)).values\n# Optionally, if you want to keep it as an xarray DataArray, re-wrap the result\ndepth_level_deltas = xr.DataArray(depth_level_deltas, dims=[\"z_t\"], coords={\"z_t\": depth_levels_in_m.z_t.isel(z_t=slice(0, -1))})\ndepth_level_deltas  \n\n\n\n","type":"content","url":"/notebooks/cesm2-oceanheat#change-units","position":15},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl2":"Compute Ocean Heat content for ocean surface"},"type":"lvl2","url":"/notebooks/cesm2-oceanheat#compute-ocean-heat-content-for-ocean-surface","position":16},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl2":"Compute Ocean Heat content for ocean surface"},"content":"Ocean surface is considered to be the top 100m\n\nThe formula for this is: H = \\rho C \\int_0^z T(z) dz\n\nWhere H is ocean heat content, the value we are trying to calculate,\n\n\\rho is the density of sea water, 1026 kg/m^3  ,\n\nC is the specific heat of sea water, 3990 J/(kg K)  ,\n\nz is the depth limit of the calculation in meters,\n\nand T(z) is the temperature at each depth in degrees Kelvin.\n\ndef calc_ocean_heat(delta_level, temperature):\n    rho = 1026 #kg/m^3\n    c_p = 3990 #J/(kg K)\n    weighted_temperature = delta_level * temperature\n    heat = weighted_temperature.sum(dim=\"z_t\")*rho*c_p\n    return heat\n\n\n\n# Remember that the coordinate z_t still has values in cm\nhist_temp_ocean_surface = hist_temp_in_degK.where(hist_temp_in_degK['z_t'] < 1e4,drop=True)\nhist_temp_ocean_surface\n\n\n\ndepth_level_deltas_surface = depth_level_deltas.where(depth_level_deltas['z_t'] <1e4, drop= True)\ndepth_level_deltas_surface\n\n\n\nhist_ocean_heat = calc_ocean_heat(depth_level_deltas_surface,hist_temp_ocean_surface)\nhist_ocean_heat\n\n\n\n","type":"content","url":"/notebooks/cesm2-oceanheat#compute-ocean-heat-content-for-ocean-surface","position":17},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl4":"Plot Ocean Heat","lvl2":"Compute Ocean Heat content for ocean surface"},"type":"lvl4","url":"/notebooks/cesm2-oceanheat#plot-ocean-heat","position":18},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl4":"Plot Ocean Heat","lvl2":"Compute Ocean Heat content for ocean surface"},"content":"\n\nAttention\n\nHas the surface ocean heat content increased with time due to Global Warming ?\nLet us plot the difference between the annual, ensemble mean heat content between the years 2014 and 1850 to check!\n\n%%time\n# Compute annual and ensemble mean\nhist_oceanheat_ann_mean = hist_ocean_heat.mean('member_id').groupby('time.year').mean()\nhist_oceanheat_ann_mean \n\n\n\n\n\nhist_oceanheat_ano = \\\nhist_oceanheat_ann_mean.sel(year=2014) - hist_oceanheat_ann_mean.sel(year=1850)\n\n\n\n%%time\nhist_oceanheat_ano.plot()\n\n\n\n\n\nIndeed! The surface ocean is trapping heat as the globe warms!\n\n\n\n","type":"content","url":"/notebooks/cesm2-oceanheat#plot-ocean-heat","position":19},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Summary","lvl2":"Compute Ocean Heat content for ocean surface"},"type":"lvl3","url":"/notebooks/cesm2-oceanheat#summary","position":20},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Summary","lvl2":"Compute Ocean Heat content for ocean surface"},"content":"In this notebook we used sea temperature data from the Community Earth System Model (CESM2) Large Ensemble dataset to compute surface ocean heat and convince ourselves that the surface ocean is trapping extra heat as the globe warms. We used an intake-ESM catalog backed by pelican links to stream data from NCAR’s Geoscience Data Exchange via NCAR’s OSDF origin!","type":"content","url":"/notebooks/cesm2-oceanheat#summary","position":21},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl4":"What’s next?","lvl3":"Summary","lvl2":"Compute Ocean Heat content for ocean surface"},"type":"lvl4","url":"/notebooks/cesm2-oceanheat#whats-next","position":22},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl4":"What’s next?","lvl3":"Summary","lvl2":"Compute Ocean Heat content for ocean surface"},"content":"In the next notebook, we will see how to load data from multiple OSDF origins into a workflow. We will stream CMIP6 model data from AWS and observational data from NCAR’s GDEX.\n\n","type":"content","url":"/notebooks/cesm2-oceanheat#whats-next","position":23},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Resources and references","lvl2":"Compute Ocean Heat content for ocean surface"},"type":"lvl3","url":"/notebooks/cesm2-oceanheat#resources-and-references","position":24},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Resources and references","lvl2":"Compute Ocean Heat content for ocean surface"},"content":"Original \n\nnotebook on the pangeo NCAR gallery\n\nCESM2 Large Ensemble Dataset (Community Earth System Model 2) hosted on NCAR’s GDEX.","type":"content","url":"/notebooks/cesm2-oceanheat#resources-and-references","position":25},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data"},"type":"lvl1","url":"/notebooks/cmip6-gmst","position":0},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data"},"content":"\n\n","type":"content","url":"/notebooks/cmip6-gmst","position":1},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/cmip6-gmst#overview","position":2},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Overview"},"content":"In this notebook we will compute the Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data and compare it with observations. This notebook is heavily inspired by the \n\nGMST example in the CMIP6 cookbook and we thank the authors for their workflow.\n\nWe will get the CMIP6 temperature data from the AWS open data program via the us-west-2 origin\n\nIn order to do this, we will use an intake-ESM catalog (hosted on NCAR’s GDEX) that uses pelicanFS backed links instead of https or s3 links\n\nWe will grab observational data hosted on NCAR’s GDEX, which is accessible via the NCAR origin\n\nPlease refer to the first chapter of this cookbook to learn more about OSDF, pelican or pelicanFS\n\nThis notebook demonstrates that you can seamlessly stream data from multiple OSDF origins in your workflow\n\n","type":"content","url":"/notebooks/cmip6-gmst#overview","position":3},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/cmip6-gmst#prerequisites","position":4},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Intake-ESM\n\nNecessary\n\nUsed for searching CMIP6 data\n\nUnderstanding of Zarr\n\nHelpful\n\nFamiliarity with metadata structure\n\nSeaborn\n\nHelpful\n\nUsed for plotting\n\nPelicanFS\n\nNecessary\n\nThe python package used to stream data in this notebook\n\nOSDF\n\nHelpful\n\nOSDF is used to stream data in this notebook\n\nTime to learn: 20 mins\n\n","type":"content","url":"/notebooks/cmip6-gmst#prerequisites","position":5},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/cmip6-gmst#imports","position":6},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Imports"},"content":"\n\nfrom matplotlib import pyplot as plt\nimport xarray as xr\nimport numpy as np\nfrom dask.diagnostics import progress\nfrom tqdm.autonotebook import tqdm\nimport intake\nimport fsspec\nimport seaborn as sns\nimport aiohttp\nimport dask\nfrom dask.distributed import LocalCluster\nimport pelicanfs \n\n\n\nWe will use an intake-ESM catalog hosted on NCAR’s Geoscience Data Exchange. This is nothing but the AWS cmip6 catalog modified to use OSDF\n\n# Load catalog URL\ngdex_url    =  'https://data.gdex.ucar.edu/'\ncat_url     = gdex_url +  'd850001/catalogs/osdf/cmip6-aws/cmip6-osdf-zarr.json'\nprint(cat_url)\n\n\n\n","type":"content","url":"/notebooks/cmip6-gmst#imports","position":7},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Set up local dask cluster"},"type":"lvl2","url":"/notebooks/cmip6-gmst#set-up-local-dask-cluster","position":8},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Set up local dask cluster"},"content":"\n\nBefore we do any computation let us first set up a local cluster using dask\n\ncluster = LocalCluster()          \nclient = cluster.get_client()\n\n\n\n# Scale the cluster\nn_workers = 3\ncluster.scale(n_workers)\ncluster\n\n\n\n","type":"content","url":"/notebooks/cmip6-gmst#set-up-local-dask-cluster","position":9},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Data Loading"},"type":"lvl2","url":"/notebooks/cmip6-gmst#data-loading","position":10},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Data Loading"},"content":"","type":"content","url":"/notebooks/cmip6-gmst#data-loading","position":11},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Load CMIP6 data from AWS","lvl2":"Data Loading"},"type":"lvl3","url":"/notebooks/cmip6-gmst#load-cmip6-data-from-aws","position":12},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Load CMIP6 data from AWS","lvl2":"Data Loading"},"content":"\n\ncol = intake.open_esm_datastore(cat_url)\ncol\n\n\n\n# there is currently a significant amount of data for these runs\nexpts = ['historical', 'ssp245', 'ssp370']\n\nquery = dict(\n    experiment_id=expts,\n    table_id='Amon',\n    variable_id=['tas'],\n    member_id = 'r1i1p1f1',\n    #activity_id = 'CMIP',\n)\n\ncol_subset = col.search(require_all_on=[\"source_id\"], **query)\ncol_subset\n\n\n\nLet us inspect the zarr store paths to see if we are using the pelican protocol.\n\nWe see that zstore column has paths that start with ‘osdf:///’ instead of ‘https://’ which tells us that we are not using a simple ‘https’ GET request to fetch the data.\n\nIn order to know more about the pelican protocol, please refer to the first chapter of this cookbook.\n\ncol_subset.df\n\n\n\n","type":"content","url":"/notebooks/cmip6-gmst#load-cmip6-data-from-aws","position":13},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Grab some Observational time series data for comparison with ensemble spread","lvl2":"Data Loading"},"type":"lvl3","url":"/notebooks/cmip6-gmst#grab-some-observational-time-series-data-for-comparison-with-ensemble-spread","position":14},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Grab some Observational time series data for comparison with ensemble spread","lvl2":"Data Loading"},"content":"The observational data we will use is the HadCRUT5 dataset from the UK Met Office\n\nThe data has been downloaded to NCAR’s Geoscience Data Exchange (GDEX) from \n\nhttps://​www​.metoffice​.gov​.uk​/hadobs​/hadcrut5/\n\nWe will use an OSDF to access this copy from the GDEX. Again the links will start with ‘osdf:///’\n\n%%time\nobs_url    = 'osdf:///ncar/gdex/d850001/HadCRUT.5.0.2.0.analysis.summary_series.global.monthly.nc'\n#\nobs_ds = xr.open_dataset(obs_url, engine='h5netcdf').tas_mean\nobs_ds\n\n\n\n\n\n","type":"content","url":"/notebooks/cmip6-gmst#grab-some-observational-time-series-data-for-comparison-with-ensemble-spread","position":15},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Some helpful functions","lvl2":"Data Loading"},"type":"lvl3","url":"/notebooks/cmip6-gmst#some-helpful-functions","position":16},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Some helpful functions","lvl2":"Data Loading"},"content":"\n\ndef drop_all_bounds(ds):\n    drop_vars = [vname for vname in ds.coords\n                 if (('_bounds') in vname ) or ('_bnds') in vname]\n    return ds.drop_vars(drop_vars)\n\ndef open_dset(df):\n    assert len(df) == 1\n    mapper = fsspec.get_mapper(df.zstore.values[0])\n    #path = df.zstore.values[0][7:]+\".zmetadata\"\n    ds = xr.open_zarr(mapper, consolidated=True)\n    return drop_all_bounds(ds)\n\ndef open_delayed(df):\n    return dask.delayed(open_dset)(df)\n\nfrom collections import defaultdict\ndsets = defaultdict(dict)\n\nfor group, df in col_subset.df.groupby(by=['source_id', 'experiment_id']):\n    dsets[group[0]][group[1]] = open_delayed(df)\n\n\n\ndsets_ = dask.compute(dict(dsets))[0]\n\n\n\n#calculate global means\ndef get_lat_name(ds):\n    for lat_name in ['lat', 'latitude']:\n        if lat_name in ds.coords:\n            return lat_name\n    raise RuntimeError(\"Couldn't find a latitude coordinate\")\n\ndef global_mean(ds):\n    lat = ds[get_lat_name(ds)]\n    weight = np.cos(np.deg2rad(lat))\n    weight /= weight.mean()\n    other_dims = set(ds.dims) - {'time'}\n    return (ds * weight).mean(other_dims)\n\n\n\n#calculate global means\ndef get_lat_name(ds):\n    for lat_name in ['lat', 'latitude']:\n        if lat_name in ds.coords:\n            return lat_name\n    raise RuntimeError(\"Couldn't find a latitude coordinate\")\n\ndef global_mean(ds):\n    lat = ds[get_lat_name(ds)]\n    weight = np.cos(np.deg2rad(lat))\n    weight /= weight.mean()\n    other_dims = set(ds.dims) - {'time'}\n    return (ds * weight).mean(other_dims)\n\n\n\n","type":"content","url":"/notebooks/cmip6-gmst#some-helpful-functions","position":17},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"GMST computation"},"type":"lvl2","url":"/notebooks/cmip6-gmst#gmst-computation","position":18},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"GMST computation"},"content":"\n\nexpt_da = xr.DataArray(expts, dims='experiment_id', name='experiment_id',\n                       coords={'experiment_id': expts})\n\ndsets_aligned = {}\n\nfor k, v in tqdm(dsets_.items()):\n    expt_dsets = v.values()\n    if any([d is None for d in expt_dsets]):\n        print(f\"Missing experiment for {k}\")\n        continue\n\n    for ds in expt_dsets:\n        ds.coords['year'] = ds.time.dt.year\n\n    # workaround for\n    # https://github.com/pydata/xarray/issues/2237#issuecomment-620961663\n    dsets_ann_mean = [v[expt].pipe(global_mean).swap_dims({'time': 'year'})\n                             .drop_vars('time').coarsen(year=12).mean()\n                      for expt in expts]\n\n    # align everything with the 4xCO2 experiment\n    dsets_aligned[k] = xr.concat(dsets_ann_mean, join='outer',dim=expt_da)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n%%time\nwith progress.ProgressBar():\n    dsets_aligned_ = dask.compute(dsets_aligned)[0]\n\n\n\n\n\n\n\nsource_ids = list(dsets_aligned_.keys())\nsource_da = xr.DataArray(source_ids, dims='source_id', name='source_id',\n                         coords={'source_id': source_ids})\n\nbig_ds = xr.concat([ds.reset_coords(drop=True)\n                    for ds in dsets_aligned_.values()],\n                    dim=source_da)\n\nbig_ds\n\n\n\nHint\n\nNote that even though the variable is called tas, the DataArray big_ds actually has the global and annual mean of surface temperatures! If you are wondering why this is the case, take a look at all the functions that were applied to obtain dsets_ann_mean!\n\n# Compute annual mean temperatures anomalies of observational data\nobs_gmsta = obs_ds.resample(time='YS').mean(dim='time')\n# obs_gmsta\n\n\n\n","type":"content","url":"/notebooks/cmip6-gmst#gmst-computation","position":19},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Compute anomlaies and plot","lvl2":"GMST computation"},"type":"lvl3","url":"/notebooks/cmip6-gmst#compute-anomlaies-and-plot","position":20},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Compute anomlaies and plot","lvl2":"GMST computation"},"content":"We will compute the temperature anomalies w.r.t 1960-1990 baseline period\n\nConvert xarray datasets to pandas dataframes\n\nUse Seaborn to plot GMSTA\n\ndf_all = big_ds.to_dataframe().reset_index()\ndf_all.head()\n\n\n\n# Define the baseline period\nbaseline_df = df_all[(df_all[\"year\"] >= 1960) & (df_all[\"year\"] <= 1990)]\n\n# Compute the baseline mean\nbaseline_mean = baseline_df[\"tas\"].mean()\n\n# Compute anomalies\ndf_all[\"tas_anomaly\"] = df_all[\"tas\"] - baseline_mean\ndf_all\n\n\n\nobs_df = obs_gmsta.to_dataframe(name='tas_anomaly').reset_index()\n\n\n\n# Convert 'time' to 'year' (keeping only the year)\nobs_df['year'] = obs_df['time'].dt.year\n\n# Drop the original 'time' column since we extracted 'year'\nobs_df = obs_df[['year', 'tas_anomaly']]\nobs_df\n\n\n\nAlmost there! Let us now use seaborn to plot all the anomalies\n\ng = sns.relplot(data=df_all, x=\"year\", y=\"tas_anomaly\",\n                hue='experiment_id', kind=\"line\", errorbar=\"sd\", aspect=2, palette=\"Set2\")  # Adjust the color palette)\n\n# Get the current axis from the FacetGrid\nax = g.ax\n\n# Overlay the observational data in red\nsns.lineplot(data=obs_df, x=\"year\", y=\"tas_anomaly\",color=\"red\", \n             linestyle=\"dashed\", linewidth=2,label=\"Observations\", ax=ax)\n\n# Adjust the legend to include observations\nax.legend(title=\"Experiment ID + Observations\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n","type":"content","url":"/notebooks/cmip6-gmst#compute-anomlaies-and-plot","position":21},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/cmip6-gmst#summary","position":22},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Summary"},"content":"In this notebook, we used surface air temperature data from several CMIP6 models for the ‘historical’, ‘SSP245’ and ‘SSP370’ runs to compute Global Mean Surface Temperature Anomaly (GMSTA) relative to the 1960-1990 baseline period and compare it with anomalies computed from the HadCRUT monthly surface temperature dataset. We used a modified intake-ESM catalog and pelicanFS to ‘stream/download’ temperature data from two different OSDF origins. The CMIP6 model data was streamed from the AWS OpenData origin in the us-west-2 region and the observational data was streamed from NCAR’s OSDF origin.\n\n","type":"content","url":"/notebooks/cmip6-gmst#summary","position":23},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/cmip6-gmst#resources-and-references","position":24},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Resources and references"},"content":"Original notebook in the Pangeo Gallery by Henri Drake and Ryan Abernathey\n\nCMIP6 cookbook by Ryan Abernathey, Henri Drake, Robert Ford and Max Grover\n\nCoupled Model Intercomparison Project 6 was accessed from \n\nhttps://​registry​.opendata​.aws​/cmip6 using a modified intake-ESM catalog hosted on NCAR’s GDEX\n\nWe thank the UK Met Office Hadley Center for providing the observational data","type":"content","url":"/notebooks/cmip6-gmst#resources-and-references","position":25},{"hierarchy":{"lvl1":"Introduction to NSF NCAR’s Geoscience Data Exhcange"},"type":"lvl1","url":"/notebooks/ncar-intro","position":0},{"hierarchy":{"lvl1":"Introduction to NSF NCAR’s Geoscience Data Exhcange"},"content":"\n\n\n\n","type":"content","url":"/notebooks/ncar-intro","position":1},{"hierarchy":{"lvl1":"Introduction to NSF NCAR’s Geoscience Data Exhcange","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/ncar-intro#overview","position":2},{"hierarchy":{"lvl1":"Introduction to NSF NCAR’s Geoscience Data Exhcange","lvl2":"Overview"},"content":"NSF NCAR’s Geoscience Data Exchange (GDEX) is a large, publicly accessible collection of atmospheric, oceanic, and related geophysical data managed at the National Center for Atmposheric Research (NCAR) sponsored by the National Science Foundation (NSF).\n\nCurrently the Geoscience Data Exchange can be visited using the link \n\nhttps://​gdex​.ucar​.edu","type":"content","url":"/notebooks/ncar-intro#overview","position":3},{"hierarchy":{"lvl1":"Introduction to NSF NCAR’s Geoscience Data Exhcange","lvl3":"In this tutorial you will learn:","lvl2":"Overview"},"type":"lvl3","url":"/notebooks/ncar-intro#in-this-tutorial-you-will-learn","position":4},{"hierarchy":{"lvl1":"Introduction to NSF NCAR’s Geoscience Data Exhcange","lvl3":"In this tutorial you will learn:","lvl2":"Overview"},"content":"Purpose of GDEX\n\nTypes of data\n\nFeatures and tools\n\nAccess and connection to OSDF\n\n\n\n","type":"content","url":"/notebooks/ncar-intro#in-this-tutorial-you-will-learn","position":5},{"hierarchy":{"lvl1":"Introduction to NSF NCAR’s Geoscience Data Exhcange","lvl2":"Purpose of GDEX"},"type":"lvl2","url":"/notebooks/ncar-intro#purpose-of-gdex","position":6},{"hierarchy":{"lvl1":"Introduction to NSF NCAR’s Geoscience Data Exhcange","lvl2":"Purpose of GDEX"},"content":"GDEX provides free access to curated data for research, with an emphasis on datasets which provide high value to NCAR and member university researchers. Additionally, GDEX provides value added services and tools to help scientists discover, access, and manipulate data. There is also an increasing empahsis on enabling users to stream data to their local computational environment or apply computations directly to data.\n\n","type":"content","url":"/notebooks/ncar-intro#purpose-of-gdex","position":7},{"hierarchy":{"lvl1":"Introduction to NSF NCAR’s Geoscience Data Exhcange","lvl2":"Types of Data"},"type":"lvl2","url":"/notebooks/ncar-intro#types-of-data","position":8},{"hierarchy":{"lvl1":"Introduction to NSF NCAR’s Geoscience Data Exhcange","lvl2":"Types of Data"},"content":"The GDEX data has a wide variety of datasets that cover many different domains of geosciences including:\n\nGlobal and regional reanalysis datasets (e.g., \n\nERA5, \n\nNCEP/NCAR, \n\nJRA-3Q)\n\nNumerical weather prediction model output (e.g. \n\nGFS)\n\nObservational data (e.g., surface, radiosonde, satellite)\n\nClimate model simulations (e.g., \n\nCESM)\n\nOceanographic datasets (e.g. \n\nICOADS)","type":"content","url":"/notebooks/ncar-intro#types-of-data","position":9},{"hierarchy":{"lvl1":"Introduction to NSF NCAR’s Geoscience Data Exhcange","lvl3":"File Formats","lvl2":"Types of Data"},"type":"lvl3","url":"/notebooks/ncar-intro#file-formats","position":10},{"hierarchy":{"lvl1":"Introduction to NSF NCAR’s Geoscience Data Exhcange","lvl3":"File Formats","lvl2":"Types of Data"},"content":"NetCDF4\n\nGrib\n\nProprietary binary\n\nBUFR\n\nZarr\n\nKerchunk\n\n","type":"content","url":"/notebooks/ncar-intro#file-formats","position":11},{"hierarchy":{"lvl1":"Introduction to NSF NCAR’s Geoscience Data Exhcange","lvl2":"Features and Tools"},"type":"lvl2","url":"/notebooks/ncar-intro#features-and-tools","position":12},{"hierarchy":{"lvl1":"Introduction to NSF NCAR’s Geoscience Data Exhcange","lvl2":"Features and Tools"},"content":"\n\nSearch and filtering tools for finding datasets\n\nData subsetting and format conversion\n\nDocumentation and metadata for reproducibility\n\nAPIs and scripts for automated access\n\n","type":"content","url":"/notebooks/ncar-intro#features-and-tools","position":13},{"hierarchy":{"lvl1":"Introduction to NSF NCAR’s Geoscience Data Exhcange","lvl2":"Access and OSDF"},"type":"lvl2","url":"/notebooks/ncar-intro#access-and-osdf","position":14},{"hierarchy":{"lvl1":"Introduction to NSF NCAR’s Geoscience Data Exhcange","lvl2":"Access and OSDF"},"content":"All datasets are open and free to download, however registration (via ORCID) is required for subset requests.\n\nIf you made it this far, you might wonder what the GDEX has to do with the Open Science Data Federation (OSDF)?\n\nA:\nGDEX is a member of the OSDF and its data holdings are served via an origin.\nThe implications are that subsequent requests of the same data will result in lower latency as data will be stored at a geographically close cache.\n\n\n\n","type":"content","url":"/notebooks/ncar-intro#access-and-osdf","position":15},{"hierarchy":{"lvl1":"Introduction to NSF NCAR’s Geoscience Data Exhcange","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/ncar-intro#summary","position":16},{"hierarchy":{"lvl1":"Introduction to NSF NCAR’s Geoscience Data Exhcange","lvl2":"Summary"},"content":"In essence, NSF NCAR’s GDEX is a vital resource for the climate and weather research community, providing long-term, reliable access to high-quality Earth system data.\n\n","type":"content","url":"/notebooks/ncar-intro#summary","position":17},{"hierarchy":{"lvl1":"Introduction to NSF NCAR’s Geoscience Data Exhcange","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/ncar-intro#resources-and-references","position":18},{"hierarchy":{"lvl1":"Introduction to NSF NCAR’s Geoscience Data Exhcange","lvl2":"Resources and references"},"content":"https://​gdex​.ucar​.edu/\n\nContact \n\ndatahelp@ucar.edu if you want to learn more about GDEX.","type":"content","url":"/notebooks/ncar-intro#resources-and-references","position":19},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters"},"type":"lvl1","url":"/notebooks/envistor-foundations","position":0},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters"},"content":"This notebook introduces a user-facing science workflow that explores salinity patterns in South Florida using curated buoy data published through the EnviStor smart data pipeline and made accessible via PelicanFS as part of the Open Science Data Federation (OSDF).\n\nEnviStor is an AI-assisted, modular data pipeline developed at Florida International University (FIU) to process and publish environmental datasets. It automates tasks like file type detection, metadata generation, geospatial transformations, and dataset publication. For this use case, EnviStor ingested buoy data, processed and cleaned it, and published it through the OSDF federation, making it available to science users via PelicanFS.\n\n\n\n","type":"content","url":"/notebooks/envistor-foundations","position":1},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/envistor-foundations#overview","position":2},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Overview"},"content":"This notebook introduces a real-world scientific workflow built around the question: “What are the salinity patterns in South Florida?” Using curated data from FIU’s buoy network—processed by the EnviStor smart pipeline and accessed through OSDF using PelicanFS, we explore how environmental datasets can be made reproducible, discoverable, and usable by domain scientists.\n\nBelow is a breakdown of what this notebook covers:\n\n1. What is Salinity?\nA brief introduction to salinity, its units, and its role in coastal science.\n\n2. Why South Florida?\nContext around why this region is important for salinity monitoring, including environmental and societal implications.\n\n3. About the Dataset\nDescription of the FIU CREST buoy network, the variables collected, and how the data is prepared through EnviStor.\n\n4. Data Access with PelicanFS\nExplanation of how the dataset is accessed directly from OSDF using PelicanFS, and how that supports open science.\n\n5. Research Question\nA clear framing of the central question that will be explored in the analysis notebook.\n\n6. What’s Next?\nA preview of what readers will do in the second notebook: loading, analyzing, and visualizing the data.\n\nBy the end of this notebook, the reader should understand the scientific motivation, the data context, and how curated environmental data made available via OSDF can be leveraged in reproducible Jupyter-based workflows.\n\n","type":"content","url":"/notebooks/envistor-foundations#overview","position":3},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/envistor-foundations#prerequisites","position":4},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Prerequisites"},"content":"To fully understand the context and content of this notebook, readers should be familiar with a few foundational concepts and tools. While this notebook itself is explanatory and doesn’t run any code, it introduces ideas and data structures that are implemented in the accompanying technical notebook.\n\nBelow is a table of key concepts and how important they are for this material:\n\nConcepts\n\nImportance\n\nNotes\n\nWhat is Salinity?\n\nNecessary\n\nCore environmental concept used in the notebook\n\nIntro to Buoy-based monitoring\n\nHelpful\n\nUnderstanding how buoy data is collected\n\nPelicanFS Overview\n\nNecessary\n\nExplains how the curated data is accessed from OSDF\n\nWhat is OSDF\n\nHelpful\n\nHigh-level context for how EnviStor data is shared\n\nTime to learn: 20-30 minutes. (~5–10 minutes per external concept; extra time optional for those new to Pelican/OSDF)\n\n\n\n","type":"content","url":"/notebooks/envistor-foundations#prerequisites","position":5},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"What is Salinity?"},"type":"lvl2","url":"/notebooks/envistor-foundations#what-is-salinity","position":6},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"What is Salinity?"},"content":"Salinity measures how much salt is dissolved in water, typically in Practical Salinity Units (PSU).\n\nSalinity influences ocean circulation, marine life, and freshwater availability. In coastal regions like South Florida, it’s a key indicator of environmental change, particularly for detecting saltwater intrusion into freshwater systems.\n\n","type":"content","url":"/notebooks/envistor-foundations#what-is-salinity","position":7},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Why South Florida?"},"type":"lvl2","url":"/notebooks/envistor-foundations#why-south-florida","position":8},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Why South Florida?"},"content":"South Florida is particularly sensitive to salinity changes due to its mix of freshwater inflows, tidal dynamics, sea level rise, and storm surge events. Monitoring salinity helps scientists detect:\n\nSaltwater intrusion into aquifers\n\nSeasonal or storm-related shifts in water quality\n\nLong-term climate-driven changes\n\n","type":"content","url":"/notebooks/envistor-foundations#why-south-florida","position":9},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"About the Dataset"},"type":"lvl2","url":"/notebooks/envistor-foundations#about-the-dataset","position":10},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"About the Dataset"},"content":"This dataset comes from FIU’s CREST buoy network, which continuously measures water quality parameters including salinity, temperature, turbidity, and more.\n\nFor this analysis, we selected three stations:\n\nBuoy 2: NW Biscayne Bay\n\nBuoy 3: Haulover Inlet\n\nBuoy 3-2: Little River\n\nThese datasets were processed through the EnviStor smart pipeline, which cleaned, standardized, and published them into the Open Science Data Federation (OSDF). The data is now directly accessible using PelicanFS, a high-performance data access layer.\n\n","type":"content","url":"/notebooks/envistor-foundations#about-the-dataset","position":11},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Data Access with PelicanFS"},"type":"lvl2","url":"/notebooks/envistor-foundations#data-access-with-pelicanfs","position":12},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Data Access with PelicanFS"},"content":"The curated data is available in the /envistor namespace on OSDF. We will access it directly in the next notebook using PelicanFS, a high-performance file system interface developed by the OSG and Pathfinders community.\n\nNo pre-downloaded files are needed — all data will be loaded live from OSDF via PelicanFS.\n\nNote\n\nIf you’re running this notebook as part of the Pythia Cook-off, PelicanFS is already set up in the environment — no extra steps are needed.\n\nHowever, if you’re adapting this workflow for use outside the Cook-off platform, you’ll need to install and mount PelicanFS on your system to access data from OSDF. For instructions, see the \n\nPelican documentation.\n\n","type":"content","url":"/notebooks/envistor-foundations#data-access-with-pelicanfs","position":13},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Research Question"},"type":"lvl2","url":"/notebooks/envistor-foundations#research-question","position":14},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Research Question"},"content":"We aim to explore:“How does water salinity vary across time and space in South Florida coastal waters?”\n\nThis question is central to understanding environmental patterns related to:\n\nSeasonality (e.g., wet vs. dry season)\n\nFreshwater discharge (e.g., from canals or rivers)\n\nSaltwater intrusion (e.g., due to sea level rise or storm surge)\n\nTo answer it, we will compare salinity data collected from three distinct coastal monitoring stations (NW Biscayne Bay, Haulover Inlet, and Little River) over a multi-month period. These stations span both urban and natural areas, helping us examine spatial variation.\n\nBy plotting and analyzing this data over time, we can begin to identify how salinity responds to both natural processes and human-driven impacts — providing insights useful for coastal management and long-term monitoring.\n\n\n\n","type":"content","url":"/notebooks/envistor-foundations#research-question","position":15},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/envistor-foundations#summary","position":16},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Summary"},"content":"In this notebook, we explored the motivation, context, and scientific importance of analyzing salinity patterns in South Florida. We introduced the curated buoy datasets processed through the EnviStor smart data pipeline and made accessible via PelicanFS, part of the Open Science Data Federation. The reader now understands why salinity matters, how the data was collected and prepared, and what question the upcoming technical notebook will answer. This context sets the stage for a reproducible, user-facing environmental analysis powered by FAIR data infrastructure.","type":"content","url":"/notebooks/envistor-foundations#summary","position":17},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl3":"What’s Next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/envistor-foundations#whats-next","position":18},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl3":"What’s Next?","lvl2":"Summary"},"content":"In the next notebook, we will:\n\nLoad salinity datasets for three buoy stations (NW Biscayne Bay, Haulover Inlet, and Little River) directly from OSDF via PelicanFS.\n\nCombine and clean the data: We’ll merge the datasets into a single time series, convert timestamps, label each station, and handle any missing or invalid salinity readings.\n\nResample to daily averages to reduce noise and highlight broader trends.\n\nVisualize the results using line plots to show salinity over time for each location, making it easy to compare patterns across stations.\n\nAnalyze temporal and spatial patterns, such as seasonal variations, sudden shifts, or location-specific behaviors that may suggest environmental changes like saltwater intrusion or storm impact.\n\nThis notebook will give users a complete, reproducible workflow that demonstrates how curated environmental data can be analyzed using standard Python tools — all made possible by the EnviStor pipeline and OSDF infrastructure.","type":"content","url":"/notebooks/envistor-foundations#whats-next","position":19},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida"},"type":"lvl1","url":"/notebooks/envistor-technical","position":0},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida"},"content":"This notebook demonstrates how to analyze salinity patterns in South Florida using curated buoy data processed by the EnviStor smart data pipeline and made available through PelicanFS, a high-performance file system interface for the Open Science Data Federation (OSDF).\n\nThe data used here comes from three monitoring stations managed by FIU, each located in a distinct part of South Florida’s coastal waters. These datasets were curated and made analysis-ready by the EnviStor pipeline.\n\nThe central question is:\n\nWhat are the salinity patterns in South Florida?\n\nWe’ll answer this by loading the data, cleaning and merging it, and visualizing salinity trends over time across multiple locations.\n\n\n\n","type":"content","url":"/notebooks/envistor-technical","position":1},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/envistor-technical#imports","position":2},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Imports"},"content":"\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pelicanfs.core import OSDFFileSystem\nfrom io import BytesIO\n\nsns.set(style=\"whitegrid\")\n\n\n\n","type":"content","url":"/notebooks/envistor-technical#imports","position":3},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Load the Curated Salinity Datasets"},"type":"lvl2","url":"/notebooks/envistor-technical#load-the-curated-salinity-datasets","position":4},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Load the Curated Salinity Datasets"},"content":"We’ll load data from three buoy stations directly from PelicanFS using the OSDFFileSystem() interface. These Excel files have already been curated and prepared by the EnviStor smart pipeline, and are stored in the /envistor namespace in OSDF.\n\nEach file corresponds to a different buoy location:\n\nBiscayne Bay\n\nHaulover Inlet\n\nLittle River\n\nWe use BytesIO to read the content as a stream before passing it to pandas.read_excel(). Each resulting DataFrame includes a \"Station\" column to identify its source location.\n\npelfs = OSDFFileSystem()\nfile_buoy1 = pelfs.cat('/envistor/CREST_Buoy_2_NW_Biscayne_Bay_-_S_of_Biscayne_Canal_082720-112221.xlsx')\nfile_buoy2 = pelfs.cat('/envistor/CREST_Buoy_3_Haulover_Inlet_100518_-_073020_updated.xlsx')\nfile_buoy3 = pelfs.cat('/envistor/CREST_Buoy_3-2_Little_River_042121-050624.xlsx')\n\nexcel_file1 = BytesIO(file_buoy1)\ndf_file_buoy1 = pd.read_excel(excel_file1)\ndf_file_buoy1['Station'] = 'Buoy - Biscayne Bay'\n\nexcel_file2 = BytesIO(file_buoy2)\ndf_file_buoy2 = pd.read_excel(excel_file2)\ndf_file_buoy2['Station'] = 'Buoy - Haulover Inlet'\n\nexcel_file3 = BytesIO(file_buoy3)\ndf_file_buoy3 = pd.read_excel(excel_file3)\ndf_file_buoy3['Station'] = 'Little River'\n\n\n\n\n\n\n","type":"content","url":"/notebooks/envistor-technical#load-the-curated-salinity-datasets","position":5},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Clean and Combine the Data"},"type":"lvl2","url":"/notebooks/envistor-technical#clean-and-combine-the-data","position":6},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Clean and Combine the Data"},"content":"Now that we’ve loaded the individual DataFrames, we’ll prepare them for analysis.\n\nHere’s what we do:\n\nCreate a timestamp: Combine the \"Date\" and \"Time\" columns into a single datetime column.\n\nStandardize salinity: Rename the \"Sal_psu\" column to \"Salinity\" and convert its values to numeric (in case of string or error-prone entries).\n\nMerge datasets: Concatenate the three cleaned DataFrames into one (df_all) so we can analyze salinity trends across all buoy stations together. We also drop any rows with missing salinity values and set the datetime column as the index to enable time-based operations later.\n\nfor df in [df_file_buoy1, df_file_buoy2, df_file_buoy3]:\n    df[\"datetime\"] = pd.to_datetime(df[\"Date\"].astype(str) + \" \" + df[\"Time\"].astype(str))\n\n    df.rename(columns={\"Sal_psu\": \"Salinity\"}, inplace=True)\n    df[\"Salinity\"] = pd.to_numeric(df[\"Salinity\"], errors=\"coerce\")\n\ndf_all = pd.concat([df_file_buoy1, df_file_buoy2, df_file_buoy3], ignore_index=True)\ndf_all.dropna(subset=[\"Salinity\"], inplace=True)\ndf_all.set_index(\"datetime\", inplace=True)\n\n\n\n\n","type":"content","url":"/notebooks/envistor-technical#clean-and-combine-the-data","position":7},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Resample and Aggregate"},"type":"lvl2","url":"/notebooks/envistor-technical#resample-and-aggregate","position":8},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Resample and Aggregate"},"content":"Salinity readings are taken multiple times per day. To reveal broader trends, we resample the data to daily averages.\n\nThis step:\n\nReduces noise\n\nMakes it easier to compare across time\n\nPrepares the data for visualization\n\nWe group by station and resample by day ('1D').\n\ndf_daily = (\n    df_all\n    .groupby(\"Station\")\n    .resample(\"1D\")\n    [\"Salinity\"]\n    .mean()\n    .reset_index()\n)\n\n\n\n\n","type":"content","url":"/notebooks/envistor-technical#resample-and-aggregate","position":9},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Visualize the Salinity Patterns"},"type":"lvl2","url":"/notebooks/envistor-technical#visualize-the-salinity-patterns","position":10},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Visualize the Salinity Patterns"},"content":"Now we can plot daily salinity patterns to compare how they evolve over time across the three locations.\n\nWe’ll use Seaborn for a clean, readable line chart.\n\nplt.figure(figsize=(14, 6))\nsns.lineplot(data=df_daily, x=\"datetime\", y=\"Salinity\", hue=\"Station\")\nplt.title(\"Daily Average Salinity in South Florida (by Buoy Station)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Salinity (psu)\")\nplt.legend(title=\"Station\")\nplt.tight_layout()\nplt.show()\n\n\n\n\nNote\n\n💡 Why this is possible\n\nThis visualization is only possible thanks to the EnviStor Smart Pipeline, which curated, cleaned, and enriched the buoy data, and published it to OSDF.\n\nAdditionally, the Pelican platform allowed us to access the data on-demand using PelicanFS — no local downloads or manual data wrangling required. This is a great example of how data infrastructure can directly support scientific insight.\n\n","type":"content","url":"/notebooks/envistor-technical#visualize-the-salinity-patterns","position":11},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Interpret the Results"},"type":"lvl2","url":"/notebooks/envistor-technical#interpret-the-results","position":12},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Interpret the Results"},"content":"With the visualization in hand, we can start identifying key salinity patterns in South Florida:\n\nBiscayne Bay (blue) shows a clear seasonal fluctuation, with higher salinity in dry months and noticeable drops likely linked to storm events or freshwater inflow.\n\nHaulover Inlet (orange) tends to have consistently higher salinity levels, suggesting stronger tidal mixing and less influence from freshwater discharge.\n\nLittle River (green) displays the most variability — sharp dips and spikes in salinity hint at frequent freshwater input, possibly from canals, rain events, or upstream runoff.\n\nThese differences illustrate how geographic location and local hydrology impact salinity levels. By comparing trends across stations, we gain insight into how dynamic and localized coastal salinity can be.\n\nNote\n\nThis type of analysis can support environmental monitoring, resource management, and research on saltwater intrusion and estuarine health.\n\n","type":"content","url":"/notebooks/envistor-technical#interpret-the-results","position":13},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Next Steps"},"type":"lvl2","url":"/notebooks/envistor-technical#next-steps","position":14},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Next Steps"},"content":"This notebook provided a foundation for analyzing coastal salinity patterns using curated data from the EnviStor pipeline. If you’re interested in extending this work, here are a few ideas:\n\nIncorporate other environmental variables: Analyze how temperature, turbidity, or dissolved oxygen vary alongside salinity to build a more holistic view of water quality.\n\nAdd spatial analysis: Use GIS tools or libraries (e.g., Cartopy or Folium) to visualize station locations and explore spatial gradients in salinity.\n\nCompare across years: Investigate long-term salinity trends and identify anomalies across different seasons or years.\n\nInclude other datasets from EnviStor: Expand the workflow by pulling additional datasets from OSDF, such as ocean currents, rainfall, or metadata-enriched observations.\n\nDevelop alert thresholds: Identify salinity levels that may signal ecological stress or risk, potentially integrating this with decision-making tools.\n\nHint\n\nCurious about how the data got so clean? Check out the \n\nEnviStor smart pipeline’s role in preparing these files — from metadata tagging to anomaly filtering — to better understand the power of backend automation.","type":"content","url":"/notebooks/envistor-technical#next-steps","position":15},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data"},"type":"lvl1","url":"/notebooks/sonarai-foundations","position":0},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data"},"content":"This notebook provides a workflow to access NOAA water column sonar data and integrate and co-visualize with water temperature from a near-by National Data Buoy Center buoy, made accessible via PelicanFS as part of the Open Science Data Federation (OSDF).\n\nBy integrating these two datasets, we aim to provide context on the oceanographic conditions when the sonar data were collected, and more specifically provide information on the environment that the ensonified marine organisms inhabited.\n\n\n\n","type":"content","url":"/notebooks/sonarai-foundations","position":1},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/sonarai-foundations#overview","position":2},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Overview"},"content":"Below describes the main topics this notebook will cover\n\nWhat are water column sonar data? An overview of these data, how they are collected, who collects them, their role supporting federal resource management.\n\nWhy including oceanographic data is important. Context on what influences the marine organisms captured by the sonar systems.\n\nAbout the sonar dataset. Specifics on the sonar data selected for this notebook, including the format, preprocessing steps, and importance\n\nAbout the buoy dataset. Specifics on the location and temporal resolution for the temperature datasets selected for this notebook.\n\nAbout the solar dataset. Specifics on how solar elevation are extracted for our study area.\n\nAccess from PelicanFS. Where, how, and why to access data from PelicanFS.\n\nWhat are our science drivers? Additional scientific motivation that is driving the project’s goals and impact to the scientific community\n\nWhat’s next? A brief look to the content of the next notebook focused on visualization.\n\nBy the end, readers will have a better understanding of what sonar data are, why integrating other oceangraphic data are important, and how to do that -- all in a Jupyter workflow that leverages data accessible through the OSDF data!\n\n","type":"content","url":"/notebooks/sonarai-foundations#overview","position":3},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/sonarai-foundations#prerequisites","position":4},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nWhat is split-beam sonar?\n\nNecessary\n\nFoundational concepts for the sonar data\n\nSonar applications for NOAA\n\nHelpful\n\nAdditional information to understand how and why NOAA collects water column sonar data\n\nUses of buoy data\n\nHelpful\n\nInformation on ocean buoys and what they are used for\n\nWhat is diel vertical migration?\n\nHelpful\n\nInformation on this daily movement of zooplankton (very small fish!) in the water column, that is captured by sonar data\n\nWhat is the Pelican Platform?\n\nHelpful\n\nFamiliarity with the Pelican Project\n\nWhat is the OSDF?\n\nHelpful\n\nFamiliarity with the Open Science Data Federation\n\nTime to learn: Approximately 35 minutes. More if you’re new to the concepts or extra curious!\n\n\n\n","type":"content","url":"/notebooks/sonarai-foundations#prerequisites","position":5},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"What are water column sonar data?"},"type":"lvl2","url":"/notebooks/sonarai-foundations#what-are-water-column-sonar-data","position":6},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"What are water column sonar data?"},"content":"\n\nThe health of our ocean ecosystems are vital to global economies. One way to learn more about our ocean is to examine the water column, which is the volume of water from the ocean surface to the ocean floor.\n\nfrom IPython.display import Image, display\ndisplay(Image(filename='../images/water column zones.png'))\n\n\n\nThe water column of the open ocean is divided into five zones from the surface to the seafloor. Each zone varies in pressure, light, temperature, oxygen, nutrients, and biological diversity. Credit: Barbara Ambrose, NOAA, from the \n\nUnderstanding Our Ocean with Water-Column Sonar Data story map\n\nScientists collect data on the water column using sonars. These instruments emit sound (“a ping”) at set frequencies that travels down through the water to the seafloor, bouncing or “scattering” off whatever is in it’s path. The instruments “listen” for the time and angle of return of the ping to capture information on the pings journey. If you’ve ever been on a boat that had a fish finder beeping away, these instruments are very similar - just more complex and more expensive. They’re scientific fish finders!\n\ndisplay(Image(filename='../images/sonar graphic.png'))\n\n\n\nSounds waves transmitted from ships using sonar instruments reflect back to the ship when they have hit an object(s), such as a school of fish. Credit: Barbara Ambrose, NOAA, from the \n\nUnderstanding Our Ocean with Water-Column Sonar Data story map\n\n","type":"content","url":"/notebooks/sonarai-foundations#what-are-water-column-sonar-data","position":7},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl3":"Supporting NOAA’s mission","lvl2":"What are water column sonar data?"},"type":"lvl3","url":"/notebooks/sonarai-foundations#supporting-noaas-mission","position":8},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl3":"Supporting NOAA’s mission","lvl2":"What are water column sonar data?"},"content":"\n\nNOAA Fisheries routinely use this technology to identify fish schools and estimate biomass for fisheries stock assessments.\n\ndisplay(Image(filename='../images/Fisheries_Science-to-Mangaement.png'))\n\n\n\nVisualization of the NOAA Fisheries science to management workflow. Credit \n\nNOAA Fisheries website\n\nWater column sonar systems are integrated on NOAA Fisheries Survey Vessels that traverse the waters of every coast.\n\ndisplay(Image(filename='../images/Schematics_ActiveAcoustics.jpg'))\n\n\n\nSchematic of NOAA Fisheries survey vessel using water column sonar to map fish. Credit \n\nNOAA Fisheries\n\n","type":"content","url":"/notebooks/sonarai-foundations#supporting-noaas-mission","position":9},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl3":"Centralized repositories and cloud access","lvl2":"What are water column sonar data?"},"type":"lvl3","url":"/notebooks/sonarai-foundations#centralized-repositories-and-cloud-access","position":10},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl3":"Centralized repositories and cloud access","lvl2":"What are water column sonar data?"},"content":"Because of their value to our nation, NOAA and other agencies water column sonar data are stewarded at the \n\nNOAA Water Column Sonar Data Archive. This archive currently holds over 350 TB of data collected over 20 years in all areas of the U.S. Exclusive Economic Zone. A copy of these archived data are accessible on Amazon Web Services (AWS) through the NOAA Open Data Dissemination Program.\n\n","type":"content","url":"/notebooks/sonarai-foundations#centralized-repositories-and-cloud-access","position":11},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Why including oceanographic data is important"},"type":"lvl2","url":"/notebooks/sonarai-foundations#why-including-oceanographic-data-is-important","position":12},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Why including oceanographic data is important"},"content":"\n\nWater column sonars provide information about fish and zooplankton inhabiting our waters. These marine organisms are heavily influenced by their surrounding ocenaographic conditions.\n\nKey drivers include\n\ntemperature\n\nsalinity\n\ndissoloved oxygen\n\nproductivity\n\ncurrents\n\nFish are also influenced by daily and monthly changes, specifically transitions from day to night and night to day, and the lunar cycle\n\n","type":"content","url":"/notebooks/sonarai-foundations#why-including-oceanographic-data-is-important","position":13},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"About the sonar dataset"},"type":"lvl2","url":"/notebooks/sonarai-foundations#about-the-sonar-dataset","position":14},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"About the sonar dataset"},"content":"\n\nRaw sonar files are complex and binary. To make them more accessible, analysis-ready and cloud-optimized, a subset of the data have been converted into \n\nZarr stores. The team is currently focusing on \n\nEK60 sonar systems run on the \n\nNOAA Ship Henry B. Bigelow by the \n\nNOAA Northeast Fisheries Science Center (NEFSC). NEFSC’s main objective for collecting these data is to determine the biomass of Atlantic herring Clupea harengus, which contributes to the commercial lobster industry.\n\nYou can explore the Zarr translated files using \n\nEchoFish, the team’s AWS-hosted interactive portal for data exploration.\n\nWe have selected a subset of data from NEFSC’s HB1906 survey, specifically from October 16, 2019.\n\ndisplay(Image(filename='../images/HB1906.png'))\n\n\n\nGeographic location for the NEFSC HB1906 cruise off the northeast coast of the U.S. Credit \n\nNOAA Water Column Sonar Data Archive\n\ndisplay(Image(filename='../images/HB1906_16Oct2019_afternoon_labeled.png'))\n\n\n\nVisualization of the HB1906 38 kHz sonar data used in the analysis.\n\n","type":"content","url":"/notebooks/sonarai-foundations#about-the-sonar-dataset","position":15},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"About the buoy dataset"},"type":"lvl2","url":"/notebooks/sonarai-foundations#about-the-buoy-dataset","position":16},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"About the buoy dataset"},"content":"\n\nWe will pull data from the \n\nNDBC stion 44005 located on George’s Bank near Nantuck Shoals. This buoy records multiple variables continuously from it’s moored location.\n\nSome of the data available on this buoy recorded at the ocean surface include\n\nwater temperature\n\nwind speed, direction, and gust\n\nwave height and direction\n\nair temperature\n\n","type":"content","url":"/notebooks/sonarai-foundations#about-the-buoy-dataset","position":17},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"About the solar dataset"},"type":"lvl2","url":"/notebooks/sonarai-foundations#about-the-solar-dataset","position":18},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"About the solar dataset"},"content":"\n\nWe use an API to call the solar position based on the defined date and location (using latitude and longitude). The solar elevation in degrees and solar azimuth in degrees are outputted. We use the solar elevation in our plots but solar azimuth could be used instead.\n\n","type":"content","url":"/notebooks/sonarai-foundations#about-the-solar-dataset","position":19},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Access from PelicanFS"},"type":"lvl2","url":"/notebooks/sonarai-foundations#access-from-pelicanfs","position":20},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Access from PelicanFS"},"content":"\n\nWe will access it directly in the next notebook using PelicanFS, a high-performance file system interface developed by the \n\nOSG and Pathfinders community.\n\nAdditional details of PelicanFS can be found in Chapter 1 - PelicanFS\n\n","type":"content","url":"/notebooks/sonarai-foundations#access-from-pelicanfs","position":21},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"What are our science drivers?"},"type":"lvl2","url":"/notebooks/sonarai-foundations#what-are-our-science-drivers","position":22},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"What are our science drivers?"},"content":"\n\nWe aim to explore the correlation between patterns extracted from the sonar data and the associated ocean temperature.\n\nAs we expand our workflow to longer time periods, larger areas, and multiple years, we will be able to further examine\n\nSeasonal and interannual variability in biological assemblages (i.e., fish and zooplankton)\n\nSpatial variability in biological assemblages\n\nThe influence of water temperature on marine organisms\n\n\n\n","type":"content","url":"/notebooks/sonarai-foundations#what-are-our-science-drivers","position":23},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/sonarai-foundations#summary","position":24},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Summary"},"content":"Throughout this notebook, we introduced the objectives, approach, and value of integrating water column sonar with oceanographic data. We reviewed the collection methods and availability of both sonar and buoy datasets, as well as their accessibility through PelicanFS. Readers should now understand what water column sonar data are and why they are important to NOAA, resource management, and supporting healthy oceans. They will also be familiar with oceanographic datasets that can provide additional context for interpreting patterns of marine organisms observed in the sonar data.\n\n","type":"content","url":"/notebooks/sonarai-foundations#summary","position":25},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/sonarai-foundations#resources-and-references","position":26},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Resources and references"},"content":"NOAA Water Column Sonar Data\n\nNortheast Ecosystem Surveys\n\n","type":"content","url":"/notebooks/sonarai-foundations#resources-and-references","position":27},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"What’s Next?"},"type":"lvl2","url":"/notebooks/sonarai-foundations#whats-next","position":28},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"What’s Next?"},"content":"In the next notebook, we will\n\nAccess and read zarr-translated sonar data for a subset of data archived at NCEI and accessible through the NOAA AWS bucket.\n\nPull temperature data from a NDBC buoy located in close proximity to where the sonar data were collected in the Gulf of Maine.\n\nCalculate the solar elevation based on the corresponding time and space of the sonar data.\n\nImport a map of sonar clusters previously calculated using a U-Net.\n\nCalculate hourly acoustic backscattering values (Sv) from the zarr files, including on the values associated with the cluster regions and above the detected seafloor\n\nVisualize the results by co-plotting the time series of water temperature, solar elevation, and Sv with the cluster map","type":"content","url":"/notebooks/sonarai-foundations#whats-next","position":29},{"hierarchy":{"lvl1":"Contextual Integration of NCAR/NOAA Environmental Data with the NOAA Water Column Sonar Archive"},"type":"lvl1","url":"/notebooks/sonarai-technical","position":0},{"hierarchy":{"lvl1":"Contextual Integration of NCAR/NOAA Environmental Data with the NOAA Water Column Sonar Archive"},"content":"\n\n","type":"content","url":"/notebooks/sonarai-technical","position":1},{"hierarchy":{"lvl1":"Contextual Integration of NCAR/NOAA Environmental Data with the NOAA Water Column Sonar Archive"},"type":"lvl1","url":"/notebooks/sonarai-technical#contextual-integration-of-ncar-noaa-environmental-data-with-the-noaa-water-column-sonar-archive","position":2},{"hierarchy":{"lvl1":"Contextual Integration of NCAR/NOAA Environmental Data with the NOAA Water Column Sonar Archive"},"content":"This notebook walks through an end-to-end workflow to relate shipboard sonar backscatter (Sv) to local environmental conditions. We (1) open EK60 data from a public NOAA S3 Zarr, (2) gather co-located environmental variables from OISST and IOOS ERDDAP, (3) compute hourly mean Sv, (4) assemble a depth×time error map for reference, and (5) synchronize timestamps to produce an interactive line-plus-heatmap visualization. All selections (time/depth/frequency) and conversions are kept explicit for reproducibility.\n\nImports\nLoad core libraries for data access (xarray, s3fs), analysis (numpy, pandas), plotting (plotly), and I/O.\n\nInitializing the datasets\nAccess HB1906 EK60 Zarr data from public S3; subset by time/depth, select 38 kHz, and mask bins below bottom.\n\nAccess buoy data\nDefine Georges Bank buoy coordinates, sample daily OISST SST at the nearest grid cell (±1 day), and download the model error map (.npy).\n\nCalculate the temperature anomaly, sun elevation in degree and azimuth\n\nDownloading external error map for the specific location\nDownloading the error map comes from a fixed file\n\nHelper Function: Mean Sv (dB)\nConvert Sv from dB→linear, compute mean, convert back to dB.\n\nGroup Cruise Data into Hourly Chunks\nAdd an hourly label and split the EK60 dataset into per-hour xarray.Dataset chunks.\n\nCompute Hourly Mean Sv & Attach to env_df\nAggregate Sv per hour and append results as a new column in the environmental dataframe.\n\nBuild Depth×Time Error-Map DataFrame & Align Timestamps\nConstruct a depth-by-time matrix from the error map, guard for size mismatches, and align env_df endpoints to the heatmap timestamps.\n\nData Visualization: Synchronized Lines + Heatmap\nPlot environmental time series above a depth×time heatmap with shared x-axis; save interactive HTML output.\n\n\n\n","type":"content","url":"/notebooks/sonarai-technical#contextual-integration-of-ncar-noaa-environmental-data-with-the-noaa-water-column-sonar-archive","position":3},{"hierarchy":{"lvl1":"Prerequisites"},"type":"lvl1","url":"/notebooks/sonarai-technical#prerequisites","position":4},{"hierarchy":{"lvl1":"Prerequisites"},"content":"This section was inspired by \n\nthis template of the wonderful \n\nThe Turing Way Jupyter Book.\n\nThis notebook opens public NOAA EK60 Zarr data from S3, subsets by time/depth, reads daily OISST SST near a buoy, fetches ERDDAP environmental variables, computes hourly mean Sv (dB), aligns with a depth×time error map, and renders synchronized line/heatmap plots.\n\nLabel the importance of each concept explicitly as helpful/necessary.\n\nConcepts\n\nImportance\n\nNotes\n\nXarray + Zarr basics\n\nNecessary\n\nOpening Zarr stores, selecting by coords/dims, .compute() semantics\n\ns3fs & public S3 access\n\nNecessary\n\nAnonymous reads from AWS S3 (anon=True)\n\nPandas time series\n\nNecessary\n\nDatetimeIndex, sorting, filtering, timezone-naive vs. aware\n\nNumPy fundamentals\n\nNecessary\n\nArray slicing, stats, type conversion\n\nAcoustic backscatter (Sv) & dB averaging\n\nNecessary\n\nConvert dB→linear, mean, then linear→dB, Understanding results\n\nERDDAP tabledap & info endpoints\n\nHelpful\n\nReading CSV responses; unit metadata lookup\n\nPlotly fundamentals\n\nHelpful\n\nSubplots, heatmaps, interactive HTML export\n\nUnderstanding of NetCDF/CF\n\nHelpful\n\nVariable metadata and geospatial conventions\n\nDask awareness\n\nHelpful\n\nLazy arrays; when/why to call .compute()\n\nGeographic coordinates\n\nHelpful\n\n0–360 vs. −180–180 longitude handling\n\nHTTP/IO with requests\n\nHelpful\n\nDownloading .npy assets for local use\n\nTime to learn: ~75 minutes\n\nSystem requirements:\n\nPython 3.9+ with Jupyter Notebook/Lab\n\nRequired packages: xarray, s3fs, numpy, pandas, plotly, requests, netCDF4 (optional but helpful: dask)\n\nNote: Run the cell below only in a local environment to install the required packages. If you’re using Binder, skip this step.\n\nimport os\nimport sys\nimport yaml\nimport shutil\nimport subprocess\n\nENV_PATH = \"../environment.yml\"\n\nwith open(ENV_PATH, \"r\", encoding=\"utf-8\") as f:\n    env = yaml.safe_load(f)\n\nchannels = env.get(\"channels\", [])\ndeps = env.get(\"dependencies\", [])\n\nconda_pkgs = []\npip_pkgs = []\n\nfor dep in deps:\n    if isinstance(dep, str):\n        # Skip python pin and the literal 'pip' meta-package entry\n        name = dep.split(\"=\")[0].strip().lower()\n        if name in {\"python\", \"pip\"}:\n            continue\n        conda_pkgs.append(dep)\n    elif isinstance(dep, dict) and \"pip\" in dep:\n        pip_pkgs.extend(dep[\"pip\"])\n\n# Prefer mamba if present; fallback to conda\nconda_exe = shutil.which(\"mamba\") or shutil.which(\"conda\")\n\n# Install Conda packages\nif conda_pkgs:\n    if not conda_exe:\n        raise RuntimeError(\"Conda/mamba not found in PATH. Run this inside a Conda environment.\")\n    # Install into the current environment prefix\n    env_prefix = os.environ.get(\"CONDA_PREFIX\", sys.prefix)\n    cmd = [conda_exe, \"install\", \"-y\", \"-p\", env_prefix]\n    for ch in channels:\n        cmd += [\"-c\", ch]\n    cmd += conda_pkgs\n    print(\"Running:\", \" \".join(cmd))\n    subprocess.check_call(cmd)\nelse:\n    print(\"No Conda packages to install.\")\n\n# Install pip packages into the current kernel's Python\nif pip_pkgs:\n    cmd = [sys.executable, \"-m\", \"pip\", \"install\", *pip_pkgs]\n    print(\"Running:\", \" \".join(cmd))\n    subprocess.check_call(cmd)\nelse:\n    print(\"No pip packages to install.\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/notebooks/sonarai-technical#prerequisites","position":5},{"hierarchy":{"lvl1":"1) Imports"},"type":"lvl1","url":"/notebooks/sonarai-technical#id-1-imports","position":6},{"hierarchy":{"lvl1":"1) Imports"},"content":"Core libraries used throughout the notebook.\n\nKey roles:xarray/s3fs for reading NOAA Zarr data from S3\nnumpy/pandas for arrays & tables\nplotly for interactive plotting\nrequests/io/os for file I/O and downloads\ndatetime for time calculations\n\nimport xarray as xr\nimport s3fs\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport pandas as pd\nimport requests\nimport io\nimport os\nimport rioxarray\nfrom datetime import datetime, timedelta\nimport pvlib\n\n\n\n","type":"content","url":"/notebooks/sonarai-technical#id-1-imports","position":7},{"hierarchy":{"lvl1":"2) Initializing the datasets"},"type":"lvl1","url":"/notebooks/sonarai-technical#id-2-initializing-the-datasets","position":8},{"hierarchy":{"lvl1":"2) Initializing the datasets"},"content":"\n\nBuilds the S3 path to the HB1906 EK60 Zarr dataset and opens it anonymously.\nSubsets by time window and depth range, selects the 38 kHz channel, and masks samples below the estimated bottom.\n.compute() materializes the selection; hm_timestamps will be reused for time alignment later. All datasets are accessed\nusing the OSDF infrastructure\n\nbucket_name = 'noaa-wcsd-zarr-pds'\nship_name = \"Henry_B._Bigelow\"\ncruise_name = \"HB1906\"\nsensor_name = \"EK60\"\n\n# Accessing the NOAA HB1906 dataset using OSDF (anonymous S3)\ns3_file_system = s3fs.S3FileSystem(anon=True)\nzarr_store = f'{cruise_name}.zarr'\ns3_zarr_store_path = f\"{bucket_name}/level_2/{ship_name}/{cruise_name}/{sensor_name}/{zarr_store}\"\n\n# Map S3 path to a zarr store and open (consolidated=None to let xarray infer metadata)\nstore = s3fs.S3Map(root=s3_zarr_store_path, s3=s3_file_system, check=False)\ncruise = xr.open_zarr(store=store, consolidated=None)\n\n# Time/depth subset and single-frequency selection\nstart_time = \"2019-10-16T15:00:00\"\nend_time = \"2019-10-16T23:11:09\"\ntimeslice = slice(start_time, end_time)\ndepths = slice(10, 250)\ncruise = cruise.sel(time=timeslice, depth=depths, drop=False)\ncruise = cruise.sel(frequency=38000, method='nearest').compute()  # materialize after selection\ncruise = cruise.where(cruise.depth < cruise.bottom + 2, drop=True)  # remove bins below bottom\n\n# Timestamps for later alignment\nhm_timestamps = cruise.time.values.tolist()\n\n\n\n","type":"content","url":"/notebooks/sonarai-technical#id-2-initializing-the-datasets","position":9},{"hierarchy":{"lvl1":"3) Accessing buoy data"},"type":"lvl1","url":"/notebooks/sonarai-technical#id-3-accessing-buoy-data","position":10},{"hierarchy":{"lvl1":"3) Accessing buoy data"},"content":"Defines a buoy location on Georges Bank (longitude converted to 0–360).\n\nOptional: Loads three daily OISST files and samples SST at the nearest grid point (day before, day of, day after).\n\nERDDAP buoy environmental data.\n\nSets ERDDAP dataset parameters and enforces a max_days cap by adjusting end_date_time if needed.\nReads station metadata to extract lon/lat and wind-speed units; prepares a conversion to knots.\nPulls a table of time, wind_speed, SST, significant wave height, converts wind speed to knots, indexes by time.\nFilters to the requested window and keeps the first nine rows (intentional truncation for later alignment).\n\n# Location of one specific buoy located on Georges Bank\ntarget_lon = 360 - 66.546  # convert from -180..180 to 0..360\ntarget_lat = 41.088\n\n# ______________OPTIONAL BUOY DATA FROM NCAR/UCAR______________\n# print(f\"Target coordinates: Longitude: {target_lon}, Latitude: {target_lat}\")\n#\n# # Accessing stationary buoy data (daily OISST files); select nearest grid cell\n# buoy_data_day_before = xr.open_dataset(\n#     'https://data.gdex.ucar.edu/d277007/avhrr_v2.1/2019/oisst-avhrr-v02r01.20191015.nc#mode=bytes', engine='netcdf4')\n# buoy_data_actual_day = xr.open_dataset(\n#     'https://data.gdex.ucar.edu/d277007/avhrr_v2.1/2019/oisst-avhrr-v02r01.20191016.nc#mode=bytes',\n#     engine='netcdf4')\n# buoy_data_day_after = xr.open_dataset(\n#     'https://data.gdex.ucar.edu/d277007/avhrr_v2.1/2019/oisst-avhrr-v02r01.20191017.nc#mode=bytes',\n#     engine='netcdf4')\n#\n# sst_day_before = buoy_data_day_before['sst'].sel(lon=target_lon, lat=target_lat, method='nearest').values[0][0]\n# sst_actual_day = buoy_data_actual_day['sst'].sel(lon=target_lon, lat=target_lat, method='nearest').values[0][0]\n# sst_day_after = buoy_data_day_after['sst'].sel(lon=target_lon, lat=target_lat, method='nearest').values[0][0]\n# ______________________________________________________________\n\n# The following code has been copied from https://github.com/ioos/soundcoop/blob/main/3_HMD_environmental_data/plot_sound_environmental_and_climatology_data.ipynb in order to access buoy environmental data.\n\nerddap_dataset = 'gov-ndbc-44005'\nsound_dataset = 'Monh'\nmax_days = 25\nstart_date_time = '2019-10-16T14:00:00.000'\nend_date_time = '2021-10-16T23:30:00.000'\nmin_frequency = 21\nmax_frequency = 24000\n\nerddap_base_url = 'https://erddap.sensors.ioos.us/erddap'\n\n# Cap the end date if requested range exceeds max_days\ntime_delta = datetime.fromisoformat(end_date_time) - datetime.fromisoformat(start_date_time)\nif time_delta.days > max_days:\n    end_date_time = str(datetime.fromisoformat(start_date_time) + timedelta(days=max_days))\n    print(f'end_date_time updated to {end_date_time}')\n\n# Get station lon/lat and units from ERDDAP metadata (CSV)\nerddap_metadata_url = f'{erddap_base_url}/info/{erddap_dataset}/index.csv'\nenv_metadata_df = pd.read_csv(erddap_metadata_url)\n\nenv_station_x = env_metadata_df.loc[env_metadata_df['Attribute Name'] == 'geospatial_lon_min']['Value'].item()\nenv_station_y = env_metadata_df.loc[env_metadata_df['Attribute Name'] == 'geospatial_lat_min']['Value'].item()\nswt_var = 'sea_surface_temperature'\n\n# __________OPTIONAL: Add Wind__________\n# Determine wind_speed units to convert to knots\n# wind_speed_units_row = env_metadata_df[\n#     (env_metadata_df['Row Type'] == 'attribute') &\n#     (env_metadata_df['Attribute Name'] == 'units') &\n#     # (env_metadata_df['Variable Name'] == 'wind_speed')\n#     ]\n# wind_speed_units = wind_speed_units_row.iloc[0]['Value']\n# print(wind_speed_units)\n\n# wind_speed_to_kts_factors = {\n#     \"m.s-1\": 1.94384,\n#     \"mph\": 0.86897423357831,\n#     \"kmh\": 0.53995555554212126825,\n#     \"ft.s-1\": 0.59248243198521155506\n# }\n\n# if wind_speed_units in wind_speed_to_kts_factors:\n#     print(\"Success! Units can be converted from\", wind_speed_units, 'to', 'kts')\n# else:\n#     print(\"Error! Wind speed cannot be converted from\", wind_speed_units, 'to', 'kts')\n\n# wind_var = 'wind_speed'\n# wave_var = 'sea_surface_wave_significant_height'\n# anomaly_var = 'swt_anomaly'\n# wind_var_kts = 'wind_speed_kts'\n# ________________________________________\n\n# Build ERDDAP tabledap query URL\nerddap_dataset_url = (\n    f'{erddap_base_url}/tabledap/{erddap_dataset}.csv'\n    f'?time,{swt_var}'\n)\n\n# Read dataset (skip the second row of units)\nenv_df = pd.read_csv(\n    erddap_dataset_url,\n    skiprows=[1]  # The second row (index 1) are the column units, which we don't need\n)\n\n# Format time, convert wind speed to knots, index by time\nenv_df['time'] = pd.to_datetime(env_df['time'])\n# env_df['wind_speed_kts'] = env_df['wind_speed'].apply(lambda x: x * wind_speed_to_kts_factors[wind_speed_units])\n# del env_df['wind_speed']\nenv_df = env_df.set_index('time').sort_index()\n\n# Filter by requested time window and keep first 9 rows (drops the rest)\nenv_df = env_df[(env_df.index > start_date_time) & (env_df.index < end_date_time)]\nenv_df.drop(env_df.tail(-9).index, inplace=True)\n# env_df\n\n\n\n","type":"content","url":"/notebooks/sonarai-technical#id-3-accessing-buoy-data","position":11},{"hierarchy":{"lvl1":"3) Accessing buoy data","lvl2":"4) Calculate the temperature anomaly, sun elevation in degree and azimuth"},"type":"lvl2","url":"/notebooks/sonarai-technical#id-4-calculate-the-temperature-anomaly-sun-elevation-in-degree-and-azimuth","position":12},{"hierarchy":{"lvl1":"3) Accessing buoy data","lvl2":"4) Calculate the temperature anomaly, sun elevation in degree and azimuth"},"content":"Extracts World Ocean Atlas 2023 temperature data for a specific location and month and calculates temperature anomaly (optional), sun elevation in degree and azimuth (optional).\n\ndef get_woa23_temp_at_xy(x, y, month, var='t_mn', depth=0):\n    \"\"\"\n    Get 1-degree WOA 2023 temperature values for a given point and month.\n\n    Args:\n        x: A longitude value given in decimal degrees\n        y: A latitude value given in decimal degrees\n        month: The month asn integer from which to extract the value\n        var (optional): The temperature variable to use. Defaults to the statistical mean.\n        depth (optional): The depth at which to extract the value. Defaults to the surface.\n    \"\"\"\n    url = (\n        'https://www.ncei.noaa.gov/thredds-ocean/dodsC/woa23/DATA/'\n        f'temperature/netcdf/decav/1.00/woa23_decav_t{month:02}_01.nc'\n    )\n    ds = xr.open_dataset(\n        url,\n        decode_times=False  # xarray can't handle times defined as \"months since ...\"\n    )\n\n    da = ds.isel(depth=depth)[var]  # Pull out just the variable we're interested in\n\n    # Because nearshore locations are often NaN due to the grid's low resolution\n    # we need to interpolate the NaNs to the nearest non-NaN before extracting our value.\n    # We use rioxarray to do the interpolations in two dimensions because plain vanilla xarray\n    # can only interpolate in one dimension.\n    da = da.rio.write_crs(4326)\n    da = da.rio.interpolate_na(method='nearest')\n\n    # Then we extract the value, also using the nearest neighbor method because the given\n    # x and y values are unlikely to fall exactly on one of the grid's lat/lon coordinate pairs\n    val = da.sel(lon=x, lat=y, method='nearest').item()\n\n    return val\n\n\n# Define the location of our selected ERDDAP dataset\n# Override here if needed\nx = env_station_x\ny = env_station_y\n\nurl = (\n    'https://www.ncei.noaa.gov/thredds-ocean/dodsC/woa23/DATA/'\n    f'temperature/netcdf/decav/1.00/woa23_decav_t07_01.nc'\n)\nda = xr.open_dataset(\n    url,\n    decode_times=False  # xarray can't handle times defined as \"months since ...\"\n).isel(depth=0)['t_mn']  # Pull out just the variable we're interested in\n\n# Because nearshore locations are often NaN due to the grid's low resolution\n# we need to interpolate the NaNs to the nearest non-NaN before extracting our value.\n# We use rioxarray to do the interpolations in two dimensions because plain vanilla xarray\n# can only interpolate in one dimension.\nda = da.rio.write_crs(4326)\nda = da.rio.interpolate_na(method='nearest')\n\n# Then we extract the value, also using the nearest neighbor method because the given\n# x and y values are unlikely to fall exactly on one of the grid's lat/lon coordinate pairs\nval = da.sel(lon=x, lat=y, method='nearest').item()\n\n# Assemble a mapping between months and WOA 2023 temperature values\nmonths = list(range(1, 13))\ntemps = [get_woa23_temp_at_xy(x, y, m) for m in months]\nclim_dict = {m: t for m, t in zip(months, temps)}\n\n# Calculate the sea water temperature anomaly by subtracting the monthly WOA 2023 temperature value\n# from each measured sea water temperature value and store it as a new variable\nanomaly_var = env_df[swt_var] - [clim_dict[10]]\n# We are not adding the temperature_anomaly variable to our dataset, because we were able to see that it follows the sea surface temperature.\n# env_df[\"temperature_anomaly\"] = anomaly_var\n\n# ---- Time range in UTC ----\ntimes_utc = pd.date_range(\n    start=start_date_time,\n    end=end_date_time,\n    freq=\"1h\",\n    tz=\"UTC\"  # <-- key: set timezone to UTC\n)\n\n# ---- Calculate solar position ----\nsolpos = pvlib.solarposition.get_solarposition(times_utc, target_lat, target_lon)\n\n# ---- Extract elevation ----\ndf = pd.DataFrame({\n    \"time_utc\": times_utc,\n    \"elevation_deg\": solpos[\"elevation\"],\n    \"azimuth_deg\": solpos[\"azimuth\"]\n})\n\nenv_df[\"elevation_deg\"] = solpos[\"elevation\"].tolist()[1:10]\n# env_df[\"azimuth_deg\"] = solpos[\"azimuth\"].tolist()[:9]\nenv_df\n\n\n\n","type":"content","url":"/notebooks/sonarai-technical#id-4-calculate-the-temperature-anomaly-sun-elevation-in-degree-and-azimuth","position":13},{"hierarchy":{"lvl1":"5) Downloading external error map for the specific location."},"type":"lvl1","url":"/notebooks/sonarai-technical#id-5-downloading-external-error-map-for-the-specific-location","position":14},{"hierarchy":{"lvl1":"5) Downloading external error map for the specific location."},"content":"Currently the error map comes from a fixed file; our plan is to switch to a dynamic AWS download that accepts location parameters.\n\n# Downloading anomaly detection model error map from NCAR via OSDF\nresponse = requests.get('https://osdf-data.gdex.ucar.edu/special_projects/pythia_2025/osdf-cookbook/mae_error_map.npy')\nresponse.raise_for_status()\nsonar_clusters = np.load(io.BytesIO(response.content))\n\n\n\n\n\n","type":"content","url":"/notebooks/sonarai-technical#id-5-downloading-external-error-map-for-the-specific-location","position":15},{"hierarchy":{"lvl1":"6) Helper: mean Sv in dB"},"type":"lvl1","url":"/notebooks/sonarai-technical#id-6-helper-mean-sv-in-db","position":16},{"hierarchy":{"lvl1":"6) Helper: mean Sv in dB"},"content":"Computes the mean of Sv correctly by converting dB → linear, averaging, then linear → dB.\nAccepts array-like input (NumPy/xarray/dask); returns a scalar in dB.\n\ndef calculate_sv_mean(input_sv):\n    # Convert dB to linear, mean in linear space, convert back to dB\n    sv = 10. ** (input_sv / 10.)\n    return 10 * np.log10(np.mean(sv))\n\n\n\n","type":"content","url":"/notebooks/sonarai-technical#id-6-helper-mean-sv-in-db","position":17},{"hierarchy":{"lvl1":"7) Group cruise data into hourly chunks"},"type":"lvl1","url":"/notebooks/sonarai-technical#id-7-group-cruise-data-into-hourly-chunks","position":18},{"hierarchy":{"lvl1":"7) Group cruise data into hourly chunks"},"content":"Adds an hourly label and groups the cruise data by hour.\nProduces a list of per-hour xarray.Dataset chunks for downstream aggregation.\n\ncruise['time_hour'] = cruise['time'].dt.floor('1h')  # hourly bin label\n\n# Group by each hour\ngrouped = cruise.groupby('time_hour')\n\n# Extract each 1-hour Dataset as a chunk (drop helper label)\nchunks = [group.drop_vars('time_hour') for _, group in grouped]\n\n\n\n","type":"content","url":"/notebooks/sonarai-technical#id-7-group-cruise-data-into-hourly-chunks","position":19},{"hierarchy":{"lvl1":"8) Compute hourly mean Sv and attach to env_df"},"type":"lvl1","url":"/notebooks/sonarai-technical#id-8-compute-hourly-mean-sv-and-attach-to-env-df","position":20},{"hierarchy":{"lvl1":"8) Compute hourly mean Sv and attach to env_df"},"content":"Iterates over hourly chunks, computes mean Sv per hour using calculate_sv_mean.\nConverts dask→NumPy→Python float and appends to a list.\nAssigns the resulting hourly series to env_df[“sv_hourly”].\nAssumes the number/order of hours matches rows retained in env_df.\n\nsv_hourly = []\ntimestamps = []\n\nfor i in range(0, len(chunks)):\n    sv_data = chunks[i]['Sv']\n    result = calculate_sv_mean(sv_data)\n\n    # Use first time in hour as representative timestamp\n    ts = pd.to_datetime(chunks[i]['time'].values[0])\n    result = result.compute()  # dask -> numpy\n    result = float(result.values)  # numpy -> Python float\n\n    sv_hourly.append(result)\n\nenv_df[\"sv_hourly\"] = sv_hourly\n\n\n\n","type":"content","url":"/notebooks/sonarai-technical#id-8-compute-hourly-mean-sv-and-attach-to-env-df","position":21},{"hierarchy":{"lvl1":"9) Build (depth × time) error-map DataFrame and align timestamps"},"type":"lvl1","url":"/notebooks/sonarai-technical#id-9-build-depth-time-error-map-dataframe-and-align-timestamps","position":22},{"hierarchy":{"lvl1":"9) Build (depth × time) error-map DataFrame and align timestamps"},"content":"Extracts one channel from sonar_clusters and pairs it with cruise depths and timestamps to form a DataFrame.\nUses min(...) to guard against size mismatches in depth/time dimensions.\nAligns only the first and last timestamps in env_df to the heatmap’s time range (keeps interior indices unchanged, sets UTC).\n\n# Prepare axes\ndepths = np.asarray(cruise.depth.values)\ntimes = pd.to_datetime(hm_timestamps)\n\n# Select channel/slice from sonar_clusters\nvals = sonar_clusters[:, :, 1]  # (1088, 28096)\n\n# Guard against mismatched sizes\nn_depth = min(len(depths), vals.shape[0])\nn_time = min(len(times), vals.shape[1])\n\n# DataFrame: rows=depths, cols=timestamps\ndf = pd.DataFrame(\n    data=vals[:n_depth, :n_time],\n    index=depths[:n_depth],\n    columns=times[:n_time]\n)\n\n# Align env_df index endpoints to heatmap timestamps (keeps interior unchanged)\nidx = env_df.index.tolist()\ndf_timestamps = pd.to_datetime(df.columns).tz_localize(None)\nidx[0] = pd.Timestamp(df_timestamps.values[0], tz='UTC').floor(\"s\")\nidx[-1] = pd.Timestamp(df_timestamps.values[-1], tz='UTC').floor(\"s\")\nenv_df.index = idx\n\n\n\n","type":"content","url":"/notebooks/sonarai-technical#id-9-build-depth-time-error-map-dataframe-and-align-timestamps","position":23},{"hierarchy":{"lvl1":"10) Data Visualization: Synchronized Lines + Heatmap"},"type":"lvl1","url":"/notebooks/sonarai-technical#id-10-data-visualization-synchronized-lines-heatmap","position":24},{"hierarchy":{"lvl1":"10) Data Visualization: Synchronized Lines + Heatmap"},"content":"\n\nPlots synchronized data: top = time series from line_df; bottom = depth×time heatmap from heatmap_df.\nExpects line_df to have a DatetimeIndex (timezone-naive or converted).\nDepth axis is reversed (surface at top).\nSaves an interactive HTML file to the parent directory (correlations.html) and shows the figure if show=True.\n\ndef plot_synchronized_heatmaps_from_df(\n        heatmap_df: pd.DataFrame,\n        line_df: pd.DataFrame,\n        colorscale: str = \"Reds\",\n        show_markers: bool = False,\n        show: bool = False,\n):\n    if not isinstance(line_df.index, pd.DatetimeIndex):\n        raise TypeError(\"line_df must have a DatetimeIndex\")\n    line_df = line_df.copy()\n    if line_df.index.tz is not None:\n        line_df.index = line_df.index.tz_convert(None)\n\n    depths = np.asarray(heatmap_df.index)\n    heatmap_timestamps = pd.to_datetime(heatmap_df.columns)\n    z = heatmap_df.to_numpy()\n\n    n = len(line_df.columns)\n    fig = make_subplots(\n        rows=n + 1,\n        cols=1,\n        shared_xaxes=True,\n        vertical_spacing=0.02,\n        row_heights=[0.1] * n + [0.7],  # n small rows + large heatmap row\n    )\n\n    mode = \"lines+markers\" if show_markers else \"lines\"\n\n    # Define units for each variable\n    units = {\n        'sea_surface_temperature': '°C',\n        'elevation_deg': 'degrees',\n        'sv_hourly': 'dB'\n    }\n\n    for i, col in enumerate(line_df.columns, start=1):\n        fig.add_trace(\n            go.Scatter(x=line_df.index, y=line_df[col], name=str(col), mode=mode, showlegend=True),\n            row=i, col=1\n        )\n\n        # Add units as y-axis titles\n        unit = units.get(col, '')  # Default to empty string if variable not found\n        fig.update_yaxes(\n            title_text=unit,\n            row=i, col=1\n        )\n\n        if i < len(line_df.columns):\n            fig.update_xaxes(showticklabels=False, row=i, col=1)\n\n    fig.add_trace(\n        go.Heatmap(\n            z=z, x=heatmap_timestamps, y=depths, colorscale=colorscale,\n            zmin=np.nanmin(z), zmax=np.nanmax(z),\n            hovertemplate=\"t=%{x}<br>depth=%{y}<br>value=%{z}<extra></extra>\",\n        ),\n        row=n + 1, col=1\n    )\n    fig.update_yaxes(autorange=\"reversed\", row=n + 1, col=1, title_text=\"Depth\")\n\n    fig.update_layout(\n        margin=dict(l=40, r=40, t=60, b=40),  # Reduced left margin, increased top for legend\n        hovermode=\"x unified\",\n        template=\"plotly_white\",\n        # height=10 * n + 500,  # scale height with number of signals\n    )\n\n    # Enhanced horizontal legend positioning\n    fig.update_layout(\n        legend=dict(\n            orientation='h',\n            x=0,\n            y=1.02,\n            xanchor='left',\n            yanchor='bottom',\n            bgcolor='rgba(255,255,255,0.8)',\n            bordercolor='rgba(0,0,0,0.1)',\n            borderwidth=1\n        )\n    )\n\n    save_path = os.path.join(os.path.dirname(os.getcwd()), \"correlations.html\")\n    fig.write_html(save_path)\n    print(f\"Plot saved to: {save_path}\")\n    if show:\n        fig.show()\n    return fig\n# Uncomment this line in order to create the plot.\n# fig = plot_synchronized_heatmaps_from_df(heatmap_df=df, line_df=env_df)\n# fig.show()\n\n\n\nBecause rendering the plot is computationally intensive and involves downloading approximately 1 GB of data, we present a static image of the result instead.\n\nfrom IPython.display import Image, display\ndisplay(Image(filename='../images/sonarai_example.png'))\n\n","type":"content","url":"/notebooks/sonarai-technical#id-10-data-visualization-synchronized-lines-heatmap","position":25},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks"},"type":"lvl1","url":"/notebooks/pycogss-foundations","position":0},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks"},"content":"The PyCoGSS software and resource environment make it easier for scientists who study landscapes to document Arctic change using large datasets. We are creating software to search for, identify, and study topographic patterns and change in landscapes underlain by permafrost, step-by-step guides on how to use it, and educational materials to teach the next generation of geoscientists. We focus on the Arctic because while there are now many new datasets of this remote environment, their size and complexity can make them daunting to use for asking science questions. Our goal is to empower undergraduate researchers to collect input data for teaching computers how to find landscape disturbances and to use those data for their own research questions.\n\n\n\n\n\n","type":"content","url":"/notebooks/pycogss-foundations","position":1},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Overview"},"type":"lvl3","url":"/notebooks/pycogss-foundations#overview","position":2},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Overview"},"content":"In this notebook you will learn about spectral data and how it can be used to track vegetation change. You will learn why it is important to track vegetation change across the Arctic, and how a scalable and reproducible approach to this problem will answer important questions about changing permafrost landscapes.\n\n","type":"content","url":"/notebooks/pycogss-foundations#overview","position":3},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Prerequisites"},"type":"lvl3","url":"/notebooks/pycogss-foundations#prerequisites","position":4},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Prerequisites"},"content":"To fully understand the context and content of this notebook, readers should be familiar with a few foundational concepts and tools. While this notebook itself is explanatory and doesn’t run any code, it introduces ideas and data structures that are implemented in the accompanying technical notebook.\n\nBelow is a table of key concepts and how important they are for this material:\n\nConcepts\n\nImportance\n\nNotes\n\nIntro to STAC: an Overview of the Specification\n\nNecessary\n\nRead about the SpatioTemporal Asset Catalog (STAC) specification\n\nWhat is NDVI?\n\nHelpful\n\nShort page describing NDVI from satellites\n\nOSDF and \n\nPelicanFS Overview\n\nNecessary\n\nExplains how the curated data is accessed from OSDF through PelicanFS\n\nTime to learn: 45-60 minutes.\n\n\n\n","type":"content","url":"/notebooks/pycogss-foundations#prerequisites","position":5},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl2":"Motivation"},"type":"lvl2","url":"/notebooks/pycogss-foundations#motivation","position":6},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl2":"Motivation"},"content":"\n\n","type":"content","url":"/notebooks/pycogss-foundations#motivation","position":7},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"What is NDVI?","lvl2":"Motivation"},"type":"lvl3","url":"/notebooks/pycogss-foundations#what-is-ndvi","position":8},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"What is NDVI?","lvl2":"Motivation"},"content":"NDVI (Normalized Difference Vegetation Index) is a simple way to measure how green and healthy vegetation is using satellite imagery. It compares how much red light and near-infrared light (NIR) is reflected from the Earth’s surface:\n\nHealthy vegetation absorbs most of the red light for photosynthesis and reflects a lot of near-infrared light.\n\nUnhealthy or sparse vegetation reflects more red light and less near-infrared light.\n\nNDVI is calculated using the following formula:\n\nNDVI = (NIR - RED) / (NIR + RED)\n\nWhere:\n\nNIR = reflectance in the near-infrared band\n\nRED = reflectance in the red band\n\n","type":"content","url":"/notebooks/pycogss-foundations#what-is-ndvi","position":9},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"How do satellites help track NDVI in the Arctic?","lvl2":"Motivation"},"type":"lvl3","url":"/notebooks/pycogss-foundations#how-do-satellites-help-track-ndvi-in-the-arctic","position":10},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"How do satellites help track NDVI in the Arctic?","lvl2":"Motivation"},"content":"Satellite data can track vegetation change in the Arctic by repeatedly measuring how much red and near-infrared light is reflected from the land surface over time. By comparing NDVI values across seasons and years, scientists can detect trends such as Arctic greening or browning, helping to monitor the effects of climate change on tundra ecosystems at large scales. This is especially important in places like the Arctic, where remote characterization is the rule rather than the exception, and where local effects of climate, fire, and other disturbance can make vegetation change \n\ncomplex.\n\n","type":"content","url":"/notebooks/pycogss-foundations#how-do-satellites-help-track-ndvi-in-the-arctic","position":11},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Why is vegetation changing in the Arctic, and why does it matter?","lvl2":"Motivation"},"type":"lvl3","url":"/notebooks/pycogss-foundations#why-is-vegetation-changing-in-the-arctic-and-why-does-it-matter","position":12},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Why is vegetation changing in the Arctic, and why does it matter?","lvl2":"Motivation"},"content":"Vegetation in the Arctic is changing due to climate warming, which enhances plant growth, alters nutrient availability, and shifts species distributions. These changes affect permafrost integrity by modifying soil thermal regimes—vegetation can insulate soils in winter and cool them in summer, influencing thaw depth. These changes promote the growth of shrubs and graminoids, contributing to a trend known as Arctic greening. However, this pattern is not uniform—spatial heterogeneity plays a major role, as local factors like soil moisture, topography, permafrost ice content, and disturbance events (e.g., wildfires, flooding, abrupt thaw) can lead to browning or shifts in plant community composition.The transformation of tundra ecosystems has global implications, including greenhouse gas release from thawing permafrost, contributing to climate feedback loops.\n\n ![vegetation change trajectories](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs43017-021-00233-0/MediaObjects/43017_2021_233_Fig1_HTML.png \"vegetation change trajectories\") \n\nFigure 1 from \n\nHeijmans et al., 2022\n\n","type":"content","url":"/notebooks/pycogss-foundations#why-is-vegetation-changing-in-the-arctic-and-why-does-it-matter","position":13},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Research questions","lvl2":"Motivation"},"type":"lvl3","url":"/notebooks/pycogss-foundations#research-questions","position":14},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Research questions","lvl2":"Motivation"},"content":"Even though the hydrology and thermal state of permafrost is important in predicting how much it will thaw and the fate of the carbon and sediment subjected to thaw, we generally can’t see water or heat in the subsurface from space. But wouldn’t it be great if we could use vegetation greening and browning to “see” what’s under the permafrost? To get to a place where we can do that, one outstanding research question we would need to solve is how topographic and hydrological variations influence plant growth and change across the Arctic landscape. Our research team is using specific Arctic landforms known as \n\nwater tracks, which we think are particularly sensitive to climate change, as a sort of “litmus test” for vegetation change to see if these features are getting warmer, colder, wetter, or drier, and how vegetation in versus outisde of these water tracks are changing. That’s why we need a scalable and reproducible workflow to track vegetation change across many temporal scales - tracking green-up (when plants first leaf out and get green early in the growing season) as well as overall greenness (which might go up or down over time depending on climate or landscape change).\n\n\n\n","type":"content","url":"/notebooks/pycogss-foundations#research-questions","position":15},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Summary","lvl2":"Motivation"},"type":"lvl3","url":"/notebooks/pycogss-foundations#summary","position":16},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Summary","lvl2":"Motivation"},"content":"We learned how satellites can track vegetation change, and why it’s particularly useful to use satellites to track vegetation change in the Arctic. We learned why it is important, but perhaps computationally challenging, to develop ways to calculate vegetation change over specific areas over specific times.\n\n","type":"content","url":"/notebooks/pycogss-foundations#summary","position":17},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"What’s Next?","lvl2":"Motivation"},"type":"lvl3","url":"/notebooks/pycogss-foundations#whats-next","position":18},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"What’s Next?","lvl2":"Motivation"},"content":"In the next notebook we will:\n\nEstablish an area of interest (AOI) in which to track NDVI\n\nFor our AOI and time of interest, build a SpatioTemporal Asset Catalog (STAC) of Sentinel-2 L2A data from an AWS archive of the collection.\n\nSearch our STAC catalog items in OSDF caches through PelicanFS or default to the AWS archive\n\nBuild a lazy spatio-temporal raster dataset of xarray objects from of items in our catalog using OSDF caches or from AWS\n\nUse our spatio-temporal dataset to calculate a time series of NDVI values to monitor spectral characteristics of water tracks in the AOI","type":"content","url":"/notebooks/pycogss-foundations#whats-next","position":19},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks"},"type":"lvl1","url":"/notebooks/pycogss-spectralchange-pfs","position":0},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks"},"content":"\n\n","type":"content","url":"/notebooks/pycogss-spectralchange-pfs","position":1},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl2":"Section 1: Data access through PelicanFS and OSDF"},"type":"lvl2","url":"/notebooks/pycogss-spectralchange-pfs#section-1-data-access-through-pelicanfs-and-osdf","position":2},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl2":"Section 1: Data access through PelicanFS and OSDF"},"content":"\n\n","type":"content","url":"/notebooks/pycogss-spectralchange-pfs#section-1-data-access-through-pelicanfs-and-osdf","position":3},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Overview","lvl2":"Section 1: Data access through PelicanFS and OSDF"},"type":"lvl3","url":"/notebooks/pycogss-spectralchange-pfs#overview","position":4},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Overview","lvl2":"Section 1: Data access through PelicanFS and OSDF"},"content":"This notebook provides an example of a scientific use-case of the PelicanFS framework by accessing data to be analyzed through federated OSDF caches. We use this data to analyze spectral changes of water tracks in a small area of interest in the Arctic. In this analysis we calculate NDVI index which we use to infer the greening and browning trends of the water tracks in the summer months as permafrost thaws.\n\nWe build a dataset from a catalog of open-access Sentinel-2 data from the Amazon Web Service (AWS). Normally, we would access this open data from AWS each time we need to run or re-run our analysis, and each time we would be making requests to the AWS servers. Given the vast spatial, temporal and spectral scale of the satellite data  we use for our analysis, retrieving data can be time and resource costly depending on the infrastructure or network context from which we are performing our computation.  With PelicanFS, we can reduce data acquisition times by performing our catalog search on OSDF federated caches. Additionally, when possible PelicanFS caches data that was previously unavailable in the OSDF cache so that next time we would be able to retrieve it from the cache. Ultimately, we expect that accessing data through PelicanFS will improve the overall time complexity of our analysis workflows.\n\n","type":"content","url":"/notebooks/pycogss-spectralchange-pfs#overview","position":5},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Prerequisites","lvl2":"Section 1: Data access through PelicanFS and OSDF"},"type":"lvl3","url":"/notebooks/pycogss-spectralchange-pfs#prerequisites","position":6},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Prerequisites","lvl2":"Section 1: Data access through PelicanFS and OSDF"},"content":"To better understand this notebook, please familiarize yourself with the following concepts:\n\nConcepts\n\nImportance\n\nNotes\n\nIntro to OSDF\n\nRecommended\n\nOverview of OSDF\n\nOverview of FSSpec\n\nNecessary\n\nTo better understand the FSSpec library\n\nOverview of Python xarrays\n\nNecessary\n\nAn introduction to data manipulation using Xarray DataArrays and Datasets\n\nWorking with STAC catalogs\n\nNecessary\n\nAn overview of SpatioTemporal Asset Catalog (STAC) catalogs for spatial data\n\nSpatial STAC catalogs as xarray data structures; \n\nWorking with STAC catalogs in Python\n\nNecessary\n\nEfficient computation of spatial raster data as STAC catalogs and xarray data structure in Python\n\nTime to learn: 30-45 minutes\n\n","type":"content","url":"/notebooks/pycogss-spectralchange-pfs#prerequisites","position":7},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Imports","lvl2":"Section 1: Data access through PelicanFS and OSDF"},"type":"lvl3","url":"/notebooks/pycogss-spectralchange-pfs#imports","position":8},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Imports","lvl2":"Section 1: Data access through PelicanFS and OSDF"},"content":"\n\nfrom pelicanfs.core import PelicanFileSystem, PelicanMap,OSDFFileSystem \n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport xarray as xr\nimport numpy as np\nimport urllib\n\nimport geopandas as gpd\nimport pystac_client \nimport stackstac\nimport rasterio\nimport shapely\n\n\n\n","type":"content","url":"/notebooks/pycogss-spectralchange-pfs#imports","position":9},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Build a STAC catalog of Sentinel-2 data from AWS","lvl2":"Section 1: Data access through PelicanFS and OSDF"},"type":"lvl3","url":"/notebooks/pycogss-spectralchange-pfs#build-a-stac-catalog-of-sentinel-2-data-from-aws","position":10},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Build a STAC catalog of Sentinel-2 data from AWS","lvl2":"Section 1: Data access through PelicanFS and OSDF"},"content":"\n\nHere, we begin by building a catalog of Sentinel-2 data that we will access from AWS. The data will be in form of  When we query our catalog, we obtain SPEC metadata objects which we will process further through the PySTAC library. As we see below, our catalog search returned 104 items, where item is a Sentinel-2 scene and take [cite, provide more info for S2 scenes, takes?] matching our AOI for June-October 2020.\n\n\n# build a geometry of AOI from these coordinate bounds\naoiBounds = (134.66615966473387,  66.82737559988661, 134.72162967387277, 66.85380494758718)\naoiGeom = shapely.geometry.box(*aoiBounds)\nlon, lat = 134.70071475239416, 66.84143426792251\n\nstartDate      = '2020-06'\nendDate       = '2020-08'\n# cloudCovMaxPct = 5\n\ncatalogURL = 'https://earth-search.aws.element84.com/v1'\nsearch = pystac_client.Client.open(catalogURL).search(collections=['sentinel-2-l2a'],\n                                                      # bbox=aoiGeom.bounds,\n                                                      datetime=f'{startDate}/{endDate}',\n                                                      intersects=dict(type=\"Point\", coordinates=(lon, lat)),\n                                                      # query={'eo:cloud_cover': {'lt': cloudCovMaxPct}}\n                                                        )\n\n\n# Get all matching items\nitems = list(search.items())\nprint(f'Found {len(items)} matching items.')\n\n\n\n","type":"content","url":"/notebooks/pycogss-spectralchange-pfs#build-a-stac-catalog-of-sentinel-2-data-from-aws","position":11},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Pointing STAC catalog to OSDF caches","lvl2":"Section 1: Data access through PelicanFS and OSDF"},"type":"lvl3","url":"/notebooks/pycogss-spectralchange-pfs#pointing-stac-catalog-to-osdf-caches","position":12},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Pointing STAC catalog to OSDF caches","lvl2":"Section 1: Data access through PelicanFS and OSDF"},"content":"\n\nNow is the time to utilize the capabilities of Pelican File System (PelicanFS) and OSDF caches. Remember that our STAC catalog contains metadata of our data. As mentioned above, each of the 104 items in our catalog is a Sentinel-2 scene for a particular timestamp matching our AOI. We can look up one of the items from the catalog to see its metadata.\n\nprint(items[0].properties)\n\n\n\nWe can peek further to see where the “assets” of this particular item is store by looking for its URL. As expected the URL points to a AWS bucket somewhere. This checks out, because out STAC catalog is build from Sentinel-2 data store in AWS.\n\nitems[0].assets['nir'].href\n\n\n\nRemember we are trying to access our data through PelicanFS, which will hopefully point us to an OSDF cache instead of AWS. If PelicanFS were able to do that, then the URL above would point to an OSDF network of cache resource in some non-AWS server. Now we will prepare to access our data through PelicanFS by first telling PelicanFS where to find Sentinel-2 data in AWS, i.e. pointing it to Sentinel-2 data namespace in AWS. This creates a kind of file path to where OSDF caches data. We will have to do this for each asset URL as seen in the example above. Here is an example of how a new constructed OSDF path from an Asset URL looks like:\n\ndef getOSDFPath(url,AWSRegion='us-west-2'):\n    \"\"\"\n    Constructs an OSDF path from an asset's original URL.\n\n    Parameters:\n    - url: URL to convert.\n\n    Returns:\n    - OSDF path.\n    \"\"\"\n    \n    return f'/aws-opendata/{AWSRegion}/sentinel-cogs{urllib.parse.urlparse(url).path}'\n\n\n\n\ngetOSDFPath(items[0].assets['nir'].href)\n\n\n\nNow let’s try accessing this asset through PelicanFS, and opening the retrieved GeoTIFF file using Rasterio.\n\n%%time\npelFS = PelicanFileSystem('pelican://osg-htc.org')\n\nAWSRegionLst = ['us-east-1','us-east-2','us-central-1','us-central-2','us-west-1','us-west-2']\nbandUrl = getOSDFPath(items[1].assets['nir'].href, 'us-west-2')\nbandDS = rasterio.open(bandUrl, opener=pelFS)\nbandDS.close()\n\n\n\n\n\nSo we are able to read the raster through the PelicanFS using Rasterio. Let’s probe further to see which OSDF caches PelicanFS is routing us to!\n\npelFS._access_stats.get_responses(bandUrl)[0][-1].access_path\n\n\n\nNow we’ll pull a small trick and try to find out which OSDF resource PelicanFS found the cache, i.e. our raster file. This is helpful for us, because now we can easily point PelicanFS to this OSDF resource for all items in our catalog. Why are we doing this? We see from the two cells above that reading a single item’s asset takes ~8 second. Since we have 104 items each with 3 assets (the Red, NIR and SCL bands), that means at least ~24 seconds for each item, and 104 \\times 24 = 2496 for all items in our catalog. About 42 minutes in total, with potential additional network overhead time costs!\n\nNow we will point all the URLs in our cache to  https://osdf1.newy32aoa.nrp.internet2.edu:8443. We will also make a HTTP request to the new OSDF Cache URL and only point update our catalog with this new URL if the HTTP request is successful.\n\n%%time\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npelFS = PelicanFileSystem('pelican://osg-htc.org')\n\nitemsOSDFCache = items\nfor idx, item in enumerate(items): # start=1):\n    # print(f'Processing dataset #{idx}')\n\n    cacheCount = 0\n    for band in ['red','nir','scl']:\n        bandUrl = getOSDFPath(items[idx].assets[band].href)\n        \n        # takes too long to access cache through PelicanFS unfortunately\n        # we can do a sneak peak of one of the items to see the OSDF location \n        # from which PelicanFS finds its cache then point the rest of our catalog to it!\n        '''\n        bandDS = rasterio.open(bandUrl, opener=pelFS)\n        if pelFS._access_stats.get_responses(bandUrl)[1]:\n            osdfCachePath = pelFS._access_stats.get_responses(bandUrl)[0][-1].access_path\n            # print(f'cache for dataset #{idx} found in {osdfCachePath} \\n')\n            items[idx].assets[band].href = pelFS._access_stats.get_responses(bandUrl)[0][-1].access_path\n            cacheCount += 1\n        # close dataset!\n        bandDS.close()\n        '''\n        cacheOSDF = 'https://osdf1.newy32aoa.nrp.internet2.edu:8443'+bandUrl\n        with urllib.request.urlopen(cacheOSDF) as res:\n            if res.status == 200:\n                itemsOSDFCache[idx].assets[band].href = cacheOSDF\n            \n    \n    # print(f'{cacheCount} caches found for dataset #{idx} \\n')\n    \n\n\n\nInspecting one of the items shows the that its assets point to the new OSDF URL!\n\nitems[23].assets['nir'].href\n\n\n\n","type":"content","url":"/notebooks/pycogss-spectralchange-pfs#pointing-stac-catalog-to-osdf-caches","position":13},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Stacking STAC into lazy xarray objects","lvl2":"Section 1: Data access through PelicanFS and OSDF"},"type":"lvl3","url":"/notebooks/pycogss-spectralchange-pfs#stacking-stac-into-lazy-xarray-objects","position":14},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Stacking STAC into lazy xarray objects","lvl2":"Section 1: Data access through PelicanFS and OSDF"},"content":"\n\nNow is the time to turn our “OSDF catalog” into data that we can analyze. We’ll do so by using the stackstac library to convert the STAC catalog to xarray data structures, and makes it possible to perform distributed computing with the help of dask. In any case, the resulting xarray data structures will be lazy, meaning that data is not loaded upfront, but only when needed. Because we have a large dataset [cite: compute size], lazy loading will be really helpful to optimize our resource usage.\n\n%%time\ns2Stack = stackstac.stack(itemsOSDFCache, assets=['red', 'nir', 'scl'],\n                         bounds = aoiGeom.bounds,\n                          gdal_env=stackstac.DEFAULT_GDAL_ENV.updated(\n                               {'GDAL_HTTP_MAX_RETRY': 3,\n                                'GDAL_HTTP_RETRY_DELAY': 5,\n                               }),\n                          epsg=4326,\n                              #    chunksize=(1, 1, 50, 50) # Original - many small chunks bad for plotting\n                                 chunksize=(1, -1, 100, 100)\n                                ).rename(\n       {'x': 'lon', 'y': 'lat'}).to_dataset(dim='band')\ns2Stack\n\n\n\n\n\n","type":"content","url":"/notebooks/pycogss-spectralchange-pfs#stacking-stac-into-lazy-xarray-objects","position":15},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl2":"Section 2: NDVI analysis of water tracks and inter-tracks"},"type":"lvl2","url":"/notebooks/pycogss-spectralchange-pfs#section-2-ndvi-analysis-of-water-tracks-and-inter-tracks","position":16},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl2":"Section 2: NDVI analysis of water tracks and inter-tracks"},"content":"\n\nNow we can continue our analysis!\n\n","type":"content","url":"/notebooks/pycogss-spectralchange-pfs#section-2-ndvi-analysis-of-water-tracks-and-inter-tracks","position":17},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Calculate NDVI","lvl2":"Section 2: NDVI analysis of water tracks and inter-tracks"},"type":"lvl3","url":"/notebooks/pycogss-spectralchange-pfs#calculate-ndvi","position":18},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Calculate NDVI","lvl2":"Section 2: NDVI analysis of water tracks and inter-tracks"},"content":"\n\nzeroMask = s2Stack['nir'] + s2Stack['red']\n\ns2Stack['ndvi'] = (s2Stack.where(zeroMask != 0, np.nan)['nir'] - s2Stack.where(zeroMask != 0, np.nan)['red'])/\\\n                        (s2Stack.where(zeroMask != 0, np.nan)['nir'] + s2Stack.where(zeroMask != 0, np.nan)['red'])\n\n# # Only keep ndvi and classification, but know you can save things like 'visible' or other fun rasters!\ns2Stack = s2Stack[['ndvi', 'scl']]\ns2Stack = s2Stack.drop_vars([c for c in s2Stack.coords if not (c in ['time', 'lat', 'lon'])])\ns2Stack\n\n\n\n","type":"content","url":"/notebooks/pycogss-spectralchange-pfs#calculate-ndvi","position":19},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Test on the centroid","lvl2":"Section 2: NDVI analysis of water tracks and inter-tracks"},"type":"lvl3","url":"/notebooks/pycogss-spectralchange-pfs#test-on-the-centroid","position":20},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Test on the centroid","lvl2":"Section 2: NDVI analysis of water tracks and inter-tracks"},"content":"\n\n%%time\ns2Point = s2Stack.interp(lat=lat, lon=lon,method='nearest')\n\ns2Df = s2Point.to_dataframe()\ns2DfFilt = s2Df[(s2Df['scl'] == 4) | (s2Df['scl'] == 5)]\n\nfig, ax = plt.subplots()\ns2DfFilt['ndvi'].plot(label='unfiltered',\n                      marker='o', \n                      # linestyle='--',\n                      markersize=2, ax=ax)\nax.set_ylim(-1.0,5.0)\nplt.show()\n\n\n\n\n\nFascinating. What happened in late July, early August 2020? Let’s take a look at an asset from the collection during that time period.\n\npics = {}\nfrom datetime import datetime\nfor item in items:\n    item_dict = {}\n    item_dict['date'] = item.properties['datetime']\n    item_dict['pic'] = item.assets['visual'].href\n    item_dict['thumb'] = item.assets['thumbnail'].href\n    pics[item.id] = item_dict\n\n\n\n%%time\nfrom rasterio.plot import show\nwith rasterio.open(pics['S2A_53WMQ_20200804_1_L2A']['pic']) as dataset:\n    show(dataset)\n\n\n\n\n\nThat’ll do it! (this is a fire burning the larch forests of the eastern Siberia taiga)\n\n","type":"content","url":"/notebooks/pycogss-spectralchange-pfs#test-on-the-centroid","position":21},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Plot the whole stack?","lvl2":"Section 2: NDVI analysis of water tracks and inter-tracks"},"type":"lvl3","url":"/notebooks/pycogss-spectralchange-pfs#plot-the-whole-stack","position":22},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Plot the whole stack?","lvl2":"Section 2: NDVI analysis of water tracks and inter-tracks"},"content":"\n\n","type":"content","url":"/notebooks/pycogss-spectralchange-pfs#plot-the-whole-stack","position":23},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl4":"Mosaic by date","lvl3":"Plot the whole stack?","lvl2":"Section 2: NDVI analysis of water tracks and inter-tracks"},"type":"lvl4","url":"/notebooks/pycogss-spectralchange-pfs#mosaic-by-date","position":24},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl4":"Mosaic by date","lvl3":"Plot the whole stack?","lvl2":"Section 2: NDVI analysis of water tracks and inter-tracks"},"content":"\n\ns2StackMosaic = s2Stack.groupby('time.date').median(dim='time')\n# s2StackMosaic = sentinel_stack_mosaicked.rename({'date': 'time'})\n# sentinel_stack_mosaicked['time'] = sentinel_stack_mosaicked['time'].astype('datetime64[ns]')\n\n\n\nDay of year makes it easier for a linear trend analysis!\n\nday_of_year = xr.DataArray(\n    s2StackMosaic['date'].astype('datetime64[ns]').dt.dayofyear,\n    coords={'date': s2StackMosaic['date']},\n    dims='date',\n    name='day_of_year'\n)\n\ns2StackMosaic = s2StackMosaic.assign_coords({'day_of_year': day_of_year})\ns2StackMosaic\n\n\n\nNow we can look at cool pictures by date\n\ns2StackMosaic['ndvi'][1].plot.imshow(vmin=-1.0, vmax=1.0)\n\n\n\n","type":"content","url":"/notebooks/pycogss-spectralchange-pfs#mosaic-by-date","position":25},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl4":"Mask out undesireable pixels in the stack according to the scene classification layer","lvl3":"Plot the whole stack?","lvl2":"Section 2: NDVI analysis of water tracks and inter-tracks"},"type":"lvl4","url":"/notebooks/pycogss-spectralchange-pfs#mask-out-undesireable-pixels-in-the-stack-according-to-the-scene-classification-layer","position":26},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl4":"Mask out undesireable pixels in the stack according to the scene classification layer","lvl3":"Plot the whole stack?","lvl2":"Section 2: NDVI analysis of water tracks and inter-tracks"},"content":"\n\ns2StackMosaic['scl'][0].plot.imshow(cmap='Set1')\n\n\n\nGoogle ‘sentinel 2 scene classification layer’. You’ll see that 4 and 5 are coded for vegetated and not vegetated. Conservatively, everything else is trash for interpreting NDVI.\n\nndviMasked = s2StackMosaic['ndvi'].where(s2StackMosaic['scl'].isin([4, 5]))\n\nndviMasked = ndviMasked.where(ndviMasked >= -1.0)\n\nndviMasked = ndviMasked.where(ndviMasked <= 1.0)\n\nndviMasked[0].plot.imshow(\n    # vmin=-1.0, vmax=1.0\n)\n\n\n\nAh yes, that’s better.\n\n%%time\nmeanNDVIDoY = ndviMasked.mean(dim=['lat', 'lon'], skipna=True).to_dataframe(name='mean_ndvi').reset_index()\nmeanNDVIDoY.head()\n\n\n\n\n\nmeanNDVIDoY.plot.scatter(x='day_of_year', y='mean_ndvi')\n\n\n\n\n\nOk, so now we’re able to detect this somehow automatically. It would be helpful to cross-referencing this against some climate (in this case it’s burned, not climate, which are related but also maybe checking against some fire database or product too).\n\n","type":"content","url":"/notebooks/pycogss-spectralchange-pfs#mask-out-undesireable-pixels-in-the-stack-according-to-the-scene-classification-layer","position":27},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Do a trendline fit","lvl2":"Section 2: NDVI analysis of water tracks and inter-tracks"},"type":"lvl3","url":"/notebooks/pycogss-spectralchange-pfs#do-a-trendline-fit","position":28},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"Do a trendline fit","lvl2":"Section 2: NDVI analysis of water tracks and inter-tracks"},"content":"\n\nWhat if we could visualize the pixelwise trend in greening and browning over a season? Let’s take a linear trend for each pixel’s NDVI across the year (hence why day of year is a useful variable here).\n\n%%time\nfit = ndviMasked.polyfit(dim='day_of_year', deg=1)\nslopes = fit['polyfit_coefficients'].sel(degree=1)\n\nslopes.plot.imshow(\n                cmap='coolwarm_r',\n                vmin=-.01,\n                vmax=.01)\n\n\n\n\n\n\nThis is now a great raster to play around with in GIS if you choose. A lot of those red streaks are the flowpaths we’re intersted in (water tracks) so it’s interesting that they got browner over the growing season compared to the intertrack areas which are generally positive (got greener over the growing season).\n\n","type":"content","url":"/notebooks/pycogss-spectralchange-pfs#do-a-trendline-fit","position":29},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"What’s Next?","lvl2":"Section 2: NDVI analysis of water tracks and inter-tracks"},"type":"lvl3","url":"/notebooks/pycogss-spectralchange-pfs#whats-next","position":30},{"hierarchy":{"lvl1":"PyCoGSS: Spectral Change Analysis of Arctic Water Tracks","lvl3":"What’s Next?","lvl2":"Section 2: NDVI analysis of water tracks and inter-tracks"},"content":"In the near future, this notebook will:\n\nAddress PelicanFS bottlenecks during reading of cache metadata.\n\nUse a larger AOI and longer time period to see this anylysis over multiple years.\n\nWith a larger AOI and longer time series, we’ll have more data, wo we’ll experiment with distributed computing with Dask.","type":"content","url":"/notebooks/pycogss-spectralchange-pfs#whats-next","position":31},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook"},"type":"lvl1","url":"/notebooks/atmosphere-llc2160-visualization","position":0},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook"},"content":"\n\nThe DYnamics of the Atmospheric general circulation Modeled On Non-hydrostatic Domains (DYAMOND) data provides high resolution ocean circulation models, offering unprecedented detail. This dataset comprises a C1440 configuration of the Goddard Earth Observing System (GEOS) atmospheric model, with 7-km horizontal grid spacing and 72 vertical layers, coupled to a LLC2160 configuration of the Massachusetts Institute of Technology general circulation model (MITgcm) with 2–4-km grid spacing and 90 vertical levels. The C1440-LLC2160 simulation has been integrated for 14 months, starting from prescribed initial conditions on January 20, 2020.\n\nThis notebook is the second part of the DYAMOND LLC2160 Ocean Dataset Cookbook.\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization","position":1},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Overview"},"type":"lvl3","url":"/notebooks/atmosphere-llc2160-visualization#overview","position":2},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Overview"},"content":"This notebook demonstrates how to access and visualize high-resolution atmospheric data from the DYAMOND dataset using OpenVisus. The data is hosted in OSDF and served using Pelican Platform and OpenVisus. You’ll learn how to read metadata from the cloud, interactively select variables, and explore regional and depth-based slices of the data.\n\nRead the metadata file from cloud\n\nData Subset\n\nVisualize the data\n\nExplore multi-resolution data for a specific region and depth\n\nBy the end of this notebook, you will understand how to:\n\nStream and query oceanographic data using PelicanFS\n\nUse metadata to inform data exploration\n\nVisualize regional and depth-specific ocean data using Panel and Bokeh\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#overview","position":3},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Prerequisites"},"type":"lvl3","url":"/notebooks/atmosphere-llc2160-visualization#prerequisites","position":4},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Prerequisites"},"content":"This section was inspired by \n\nthis template of the wonderful \n\nThe Turing Way Jupyter Book.\n\nConcepts\n\nImportance\n\nNotes\n\nOpenVisus\n\nHelpful\n\nRequired for multiresolution data access and streaming\n\nOceanographic data formats and interpretation\n\nHelpful\n\nUnderstanding of gridded climate/ocean data such as LLC2160\n\nPelicanFS\n\nHelpful\n\nUsed for high-throughput data access from cloud storage\n\nTime to learn: 30 minutes\n\nSystem requirements:\n\nPython packages: panel, bokeh, xmltodict, colorcet, boto3, basemap, pelicanfs, OpenVisus, openvisuspy\n\nRecommended: Python ≥ 3.8, internet access for cloud-hosted data\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#prerequisites","position":5},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl2":"Step 1: Importing the libraries"},"type":"lvl2","url":"/notebooks/atmosphere-llc2160-visualization#step-1-importing-the-libraries","position":6},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl2":"Step 1: Importing the libraries"},"content":"\n\nimport numpy as np\nimport openvisuspy as ovp\nimport matplotlib.pyplot as plt\n\n\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#step-1-importing-the-libraries","position":7},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl4":"The section below shows different LLC2160 fields we have available in cloud. Each field is >200TB.","lvl2":"Step 1: Importing the libraries"},"type":"lvl4","url":"/notebooks/atmosphere-llc2160-visualization#the-section-below-shows-different-llc2160-fields-we-have-available-in-cloud-each-field-is-200tb","position":8},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl4":"The section below shows different LLC2160 fields we have available in cloud. Each field is >200TB.","lvl2":"Step 1: Importing the libraries"},"content":"\n\nvariable = 'u'\nface=0 # 6 variables are available\ntimestep=1 # There are 10000 timesteps available\n\n\n\nbase_url= \"pelican://osg-htc.org/nasa/nsdf/climate3/dyamond/GEOS/\"\nvar_dir=f\"GEOS_{variable.upper()}/{variable.lower()}_face_{face}_depth_52_time_0_10269.idx\"\nvar_url=base_url+var_dir\n\n\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#the-section-below-shows-different-llc2160-fields-we-have-available-in-cloud-each-field-is-200tb","position":9},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Step 2: Reading the metadata file from cloud","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/atmosphere-llc2160-visualization#step-2-reading-the-metadata-file-from-cloud","position":10},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Step 2: Reading the metadata file from cloud","lvl2":"Step 1: Importing the libraries"},"content":"In this section, you can select any variables that you can declared in the cells above and replace it inside LoadDataset. We are just reading the metadata for the dataset here.\n\ndb=ovp.LoadDataset(var_url)\nprint(f'Dimensions: {db.getLogicBox()[1][0]}*{db.getLogicBox()[1][1]}*{db.getLogicBox()[1][2]}')\nprint(f'Total Timesteps: {len(db.getTimesteps())}')\nprint(f'Field: {db.getField().name}')\nprint('Data Type: float32')\n\n\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#step-2-reading-the-metadata-file-from-cloud","position":11},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Step 3:  Data Selection","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/atmosphere-llc2160-visualization#step-3-data-selection","position":12},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Step 3:  Data Selection","lvl2":"Step 1: Importing the libraries"},"content":"This section shows you how to load the data you want. You can select any timestep, region (x,y,z) you want. You can set the quality or resolution of the data as well. Higher quality means the finer(more) data. Not setting any time means first timestep available. Not setting quality means full data which takes a while to load because of the higher filesize.  Since each timestep is >30GB, I am only selecting 1 level out of 90.\n\ndata=db.db.read(time=0,z=[0,1],quality=-4) #Since each timestep is >30GB, I am only selecting 1 level out of 90.\ndata.shape\n\n\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#step-3-data-selection","position":13},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/atmosphere-llc2160-visualization#step-4-visualize-the-data","position":14},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"content":"We are using a simple matplotlib here, but since the data is in numpy array, it can loaded with any python modules that support numpy. Feel free to set the vmin,vmax appropriately.\n\nfig,axes=plt.subplots(1,1,figsize=(12,8))\nim= axes.imshow(data[0,:,:], aspect='auto',origin='lower',cmap='coolwarm')\ncbar = plt.colorbar(im, ax=axes)\ncbar.set_label('Temperature (deg. C)')\nplt.show()\n\n\n\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#step-4-visualize-the-data","position":15},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"type":"lvl4","url":"/notebooks/atmosphere-llc2160-visualization#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":16},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"content":"Aashish Panta (\n\naashishpanta0@gmail​.com)\n\nGiorgio Scorzelli (\n\nscrgiorgio@gmail​.com)\n\nValerio Pascucci (\n\npascucci​.valerio@gmail​.com)","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":17},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS"},"type":"lvl1","url":"/notebooks/introduction-to-nsdf-openvisus","position":0},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS"},"content":"\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus","position":1},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl2","url":"/notebooks/introduction-to-nsdf-openvisus#openvisus-high-performance-big-data-analysis-and-visualization","position":2},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#openvisus-high-performance-big-data-analysis-and-visualization","position":3},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Overview","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#overview","position":4},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Overview","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"OpenViSUS is an open-source framework designed for efficient management, analysis, and visualization of large-scale scientific datasets. It enables interactive exploration of petabyte-scale data on a wide range of devices, from supercomputers to commodity laptops, making big data accessible to all users.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#overview","position":5},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Key Features","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#key-features","position":6},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Key Features","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Efficient Data Storage: Uses the IDX format, which stores data in a hierarchical Z (HZ) order for cache-oblivious, progressive access.\n\nScalable Visualization: Enables interactive visualization of terabyte and petabyte datasets without requiring high-end hardware.\n\nProgressive Streaming: Optimizes network utilization with state-of-the-art compression algorithms for fast data delivery and streaming.\n\nWeb-Based Dashboards: Provides customizable dashboards for data analysis accessible from any device with an internet connection.\n\nOpen Source: Distributed under the permissive BSD license.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#key-features","position":7},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Data Storage: The IDX Format","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#data-storage-the-idx-format","position":8},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Data Storage: The IDX Format","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Hierarchical Z (HZ) Order: Data is organized to allow efficient, multi-resolution access and visualization.\n\nProgressive Access: Users can interactively explore data at different resolutions, starting with coarse overviews and refining to full detail as needed.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#data-storage-the-idx-format","position":9},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Data Delivery","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#data-delivery","position":10},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Data Delivery","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Interactive Streaming: Share and stream large datasets using simple server modules (e.g., Apache), enabling teravoxel imagery delivery.\n\nCloud and Local Access: Data can be accessed from local storage or cloud repositories, with optimized streaming for remote analysis.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#data-delivery","position":11},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Interactive Analysis and Visualization","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#interactive-analysis-and-visualization","position":12},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Interactive Analysis and Visualization","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Cross-Platform: Works on supercomputers, desktops, and laptops.\n\nScripting Support: Experiment with interactive scripting for rapid data insights.\n\nUser-Friendly Querying: Abstracts complexities of file systems and cloud services, allowing scientists to focus on analysis.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#interactive-analysis-and-visualization","position":13},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"2026 IEEE SciVis  Contest","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#id-2026-ieee-scivis-contest","position":14},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"2026 IEEE SciVis  Contest","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"NSDF-OpenVISUS is directly supporting the 2026 IEEE SciVis Contest. This contest is held annually as part of the IEEE VIS Conference. In 2026, this contest will focus on the visualization of petascale oceanic and atmospheric climate data provided by NASA. This year’s challenge emphasizes advanced visualization methods for exploring vast climate datasets, encouraging innovative solutions that address real-world issues such as climate prediction, weather simulation, and environmental impact analysis.\n\nThe best submission wins $1000 cash prize. Find more information on the official \n\nSciVis website.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#id-2026-ieee-scivis-contest","position":15},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Example Use Cases","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#example-use-cases","position":16},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Example Use Cases","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"NASA LLC2160 Data Interactive Dashboard\n\nNEX-GDDP-CMIP6 Dashboard\n\nClassroom deployment for minority-serving institutions\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#example-use-cases","position":17},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Other Deplyments","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#other-deplyments","position":18},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Other Deplyments","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Cornell High Energy Synchrotron Source (CHESS)\n\nNational Center for Atmospheric Research (NCAR)\n\nNEON\n\nSOMOSPIE\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#other-deplyments","position":19},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Getting Started","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#getting-started","position":20},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Getting Started","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#getting-started","position":21},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl4":"Installation","lvl3":"Getting Started","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl4","url":"/notebooks/introduction-to-nsdf-openvisus#installation","position":22},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl4":"Installation","lvl3":"Getting Started","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Install Using pip\n\npip install OpenVisus\n\nBuild from Source:\n\nClone the repository: git clone https://github.com/sci-visus/OpenVisus.git\n\nFollow instructions in the README.md and docs/compilation.md for building on your platform.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#installation","position":23},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"References","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#references","position":24},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"References","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"National Science Data Fabric\n\nOpenVisus\n\nOpenVisuspy\n\nPlease consult these papers for technical details and use cases:\n\nWeb-based Visualization and Analytics of Petascale data: Equity as a Tide that Lifts All Boats\n\nInteractive Visualization of Terascale Data in the Browser: Fact or Fiction?\n\nFast Multiresolution Reads of Massive Simulation Datasets\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#references","position":25},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl3":"References","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl4","url":"/notebooks/introduction-to-nsdf-openvisus#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":26},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl3":"References","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Aashish Panta (\n\naashishpanta0@gmail​.com)\n\nGiorgio Scorzelli (\n\nscrgiorgio@gmail​.com)\n\nValerio Pascucci (\n\npascucci​.valerio@gmail​.com)\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":27},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook"},"type":"lvl1","url":"/notebooks/ocean-llc2160-visualization","position":0},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook"},"content":"\n\nThe DYnamics of the Atmospheric general circulation Modeled On Non-hydrostatic Domains (DYAMOND) data provides high resolution ocean circulation models, offering unprecedented detail. This dataset comprises a C1440 configuration of the Goddard Earth Observing System (GEOS) atmospheric model, with 7-km horizontal grid spacing and 72 vertical layers, coupled to a LLC2160 configuration of the Massachusetts Institute of Technology general circulation model (MITgcm) with 2–4-km grid spacing and 90 vertical levels. The C1440-LLC2160 simulation has been integrated for 14 months, starting from prescribed initial conditions on January 20, 2020.\n\n\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization","position":1},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Overview"},"type":"lvl3","url":"/notebooks/ocean-llc2160-visualization#overview","position":2},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Overview"},"content":"This notebook demonstrates how to access and visualize high-resolution ocean data from the LLC2160 dataset using OpenVisus. The data is hosted in OSDF and served using Pelican Platform and OpenVisus. You’ll learn how to read metadata from the cloud, interactively select variables, and explore regional and depth-based slices of the data.\n\nRead the metadata file from cloud\n\nData Subset\n\nVisualize the data\n\nExplore multi-resolution data for a specific region and depth\n\nBy the end of this notebook, you will understand how to:\n\nStream and query oceanographic data using PelicanFS\n\nUse metadata to inform data exploration\n\nVisualize regional and depth-specific ocean data using Panel and Bokeh\n\n\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#overview","position":3},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Prerequisites"},"type":"lvl3","url":"/notebooks/ocean-llc2160-visualization#prerequisites","position":4},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Prerequisites"},"content":"This section was inspired by \n\nthis template of the wonderful \n\nThe Turing Way Jupyter Book.\n\nConcepts\n\nImportance\n\nNotes\n\nOpenVisus\n\nHelpful\n\nRequired for multiresolution data access and streaming\n\nOceanographic data formats and interpretation\n\nHelpful\n\nUnderstanding of gridded climate/ocean data such as LLC2160\n\nPelicanFS\n\nHelpful\n\nUsed for high-throughput data access from cloud storage\n\nTime to learn: 30 minutes\n\nSystem requirements:\n\nPython packages: panel, bokeh, xmltodict, colorcet, boto3, basemap, pelicanfs, OpenVisus, openvisuspy\n\nRecommended: Python ≥ 3.8, internet access for cloud-hosted data\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#prerequisites","position":5},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 1: Importing the libraries"},"type":"lvl2","url":"/notebooks/ocean-llc2160-visualization#step-1-importing-the-libraries","position":6},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 1: Importing the libraries"},"content":"\n\nimport numpy as np\nimport openvisuspy as ovp\nimport matplotlib.pyplot as plt\n\n\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-1-importing-the-libraries","position":7},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl4":"The section below shows different LLC2160 fields we have available in cloud. Each field is >200TB.","lvl2":"Step 1: Importing the libraries"},"type":"lvl4","url":"/notebooks/ocean-llc2160-visualization#the-section-below-shows-different-llc2160-fields-we-have-available-in-cloud-each-field-is-200tb","position":8},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl4":"The section below shows different LLC2160 fields we have available in cloud. Each field is >200TB.","lvl2":"Step 1: Importing the libraries"},"content":"\n\nvariable='salt' # options are: u,v,w,salt,theta\n\n\n\n\nbase_url= \"pelican://osg-htc.org/nasa/nsdf/climate3/dyamond/\"\nif variable==\"theta\" or variable==\"w\":\n    base_dir=f\"mit_output/llc2160_{variable}/llc2160_{variable}.idx\"\nelif variable==\"u\":\n    base_dir= \"mit_output/llc2160_arco/visus.idx\"\nelse:\n    base_dir=f\"mit_output/llc2160_{variable}/{variable}_llc2160_x_y_depth.idx\"\nvar_url=base_url+base_dir\n\n\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#the-section-below-shows-different-llc2160-fields-we-have-available-in-cloud-each-field-is-200tb","position":9},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Step 2: Reading the metadata file from cloud","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc2160-visualization#step-2-reading-the-metadata-file-from-cloud","position":10},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Step 2: Reading the metadata file from cloud","lvl2":"Step 1: Importing the libraries"},"content":"In this section, you can select any variables that you can declared in the cells above and replace it inside LoadDataset. We are just reading the metadata for the dataset here.\n\ndb=ovp.LoadDataset(var_url)\nprint(f'Dimensions: {db.getLogicBox()[1][0]}*{db.getLogicBox()[1][1]}*{db.getLogicBox()[1][2]}')\nprint(f'Total Timesteps: {len(db.getTimesteps())}')\nprint(f'Field: {db.getField().name}')\nprint('Data Type: float32')\n\n\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-2-reading-the-metadata-file-from-cloud","position":11},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Step 3:  Data Selection","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc2160-visualization#step-3-data-selection","position":12},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Step 3:  Data Selection","lvl2":"Step 1: Importing the libraries"},"content":"This section shows you how to load the data you want. You can select any timestep, region (x,y,z) you want. You can set the quality or resolution of the data as well. Higher quality means the finer(more) data. Not setting any time means first timestep available. Not setting quality means full data which takes a while to load because of the higher filesize.  Since each timestep is >30GB, I am only selecting 1 level out of 90.\n\ndata=db.db.read(time=0,z=[0,1],quality=-4) #Since each timestep is >30GB, I am only selecting 1 level out of 90.\ndata.shape\n\n\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-3-data-selection","position":13},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc2160-visualization#step-4-visualize-the-data","position":14},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"content":"We are using a simple matplotlib here, but since the data is in numpy array, it can loaded with any python modules that support numpy. Feel free to set the vmin,vmax appropriately.\n\nfig,axes=plt.subplots(1,1,figsize=(12,8))\nim= axes.imshow(data[0,:,:], aspect='auto',origin='lower',cmap='turbo')\ncbar = plt.colorbar(im, ax=axes)\ncbar.set_label('Temperature (deg. C)')\nplt.show()\n\n\n\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-4-visualize-the-data","position":15},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"But, what if you want to see the full data for a certain region at a certain depth?","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc2160-visualization#but-what-if-you-want-to-see-the-full-data-for-a-certain-region-at-a-certain-depth","position":16},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"But, what if you want to see the full data for a certain region at a certain depth?","lvl2":"Step 1: Importing the libraries"},"content":"Just set the right x,y,z while reading the data. x and y are the bounding box, z is the depth/layer.\n\ndata1=db.db.read(time=1,z=[0,1],quality=-6,x=[500,2500],y=[2500,5000])\nplt.imshow(data1[0,:,:], origin='lower',cmap='turbo')\nplt.colorbar()\n\n\n\ndata1.shape #\n\n\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#but-what-if-you-want-to-see-the-full-data-for-a-certain-region-at-a-certain-depth","position":17},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 5: Save the data for the region locally"},"type":"lvl2","url":"/notebooks/ocean-llc2160-visualization#step-5-save-the-data-for-the-region-locally","position":18},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 5: Save the data for the region locally"},"content":"You can save the data locally as you want. For example, here we are only saving the region shown above as a numpy array.\n\nnp.save('test_region2.npy', data1)\n\n\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-5-save-the-data-for-the-region-locally","position":19},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 6: Load the locally saved region and visualize using matplotlib"},"type":"lvl2","url":"/notebooks/ocean-llc2160-visualization#step-6-load-the-locally-saved-region-and-visualize-using-matplotlib","position":20},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 6: Load the locally saved region and visualize using matplotlib"},"content":"\n\nlocal_data=np.load('test_region2.npy')\nplt.imshow(local_data[0,:,:], origin='lower',cmap='turbo')\nplt.colorbar()\n\n\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-6-load-the-locally-saved-region-and-visualize-using-matplotlib","position":21},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 7: Horizontal Slicing"},"type":"lvl2","url":"/notebooks/ocean-llc2160-visualization#step-7-horizontal-slicing","position":22},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 7: Horizontal Slicing"},"content":"\n\ndata1=db.db.read(time=1,x=[500,2500],y=[5100,5101])\ndata1.shape\n\n\n\n\nplt.figure(figsize=(14,8))\nplt.imshow(data1[:,0,:],cmap='turbo')\n# plt.colorbar()\n\n\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-7-horizontal-slicing","position":23},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl2":"Step 7: Horizontal Slicing"},"type":"lvl4","url":"/notebooks/ocean-llc2160-visualization#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":24},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl2":"Step 7: Horizontal Slicing"},"content":"Aashish Panta (\n\naashishpanta0@gmail​.com)\n\nGiorgio Scorzelli (\n\nscrgiorgio@gmail​.com)\n\nValerio Pascucci (\n\npascucci​.valerio@gmail​.com)","type":"content","url":"/notebooks/ocean-llc2160-visualization#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":25},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook"},"type":"lvl1","url":"/notebooks/ocean-llc4320-visualization","position":0},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook"},"content":"\n\nLLC4320, a product of \n\nEstimating the Circulation and Climate of the Ocean (ECCO) project, is the product of a 14-month simulation of ocean circulation and dynamics using MITgcm model. This simulation is similar to the ocean portion of the DYAMOND coupled simulation but was run with half the horizontal grid spacing (4\\times the cell count) and with ocean surface boundary values derived from observations and physical models. The model output has five 3D and thirteen 2D fields, including temperature, salinity, three velocity components, sea ice, and radiation. This massive dataset is 2.8 PB.\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization","position":1},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Overview"},"type":"lvl3","url":"/notebooks/ocean-llc4320-visualization#overview","position":2},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Overview"},"content":"This notebook demonstrates how to access and visualize high-resolution ocean data from the LLC4320 dataset using OpenVisus. The data is hosted in OSDF and served using Pelican Platform and OpenVisus. You’ll learn how to read metadata from the cloud, interactively select variables, and explore regional and depth-based slices of the data.\n\nRead the metadata file from cloud\n\nData Subset\n\nVisualize the data\n\nExplore multi-resolution data for a specific region and depth\n\nBy the end of this notebook, you will understand how to:\n\nStream and query oceanographic data using PelicanFS\n\nUse metadata to inform data exploration\n\nVisualize regional and depth-specific ocean data using Panel and Bokeh\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#overview","position":3},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Prerequisites"},"type":"lvl3","url":"/notebooks/ocean-llc4320-visualization#prerequisites","position":4},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Prerequisites"},"content":"This section was inspired by \n\nthis template of the wonderful \n\nThe Turing Way Jupyter Book.\n\nConcepts\n\nImportance\n\nNotes\n\nOpenVisus\n\nHelpful\n\nRequired for multiresolution data access and streaming\n\nOceanographic data formats and interpretation\n\nHelpful\n\nUnderstanding of gridded climate/ocean data such as LLC2160\n\nPelicanFS\n\nHelpful\n\nUsed for high-throughput data access from cloud storage\n\nTime to learn: 30 minutes\n\nSystem requirements:\n\nPython packages: panel, bokeh, xmltodict, colorcet, boto3, basemap, pelicanfs, OpenVisus, openvisuspy\n\nRecommended: Python ≥ 3.8, internet access for cloud-hosted data\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#prerequisites","position":5},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 1: Importing the libraries"},"type":"lvl2","url":"/notebooks/ocean-llc4320-visualization#step-1-importing-the-libraries","position":6},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 1: Importing the libraries"},"content":"\n\nimport numpy as np\nimport openvisuspy as ovp\nimport matplotlib.pyplot as plt\n\n\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-1-importing-the-libraries","position":7},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl4":"The section below shows different LLC4320 fields we have available in cloud. Each field is >400TB.","lvl2":"Step 1: Importing the libraries"},"type":"lvl4","url":"/notebooks/ocean-llc4320-visualization#the-section-below-shows-different-llc4320-fields-we-have-available-in-cloud-each-field-is-400tb","position":8},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl4":"The section below shows different LLC4320 fields we have available in cloud. Each field is >400TB.","lvl2":"Step 1: Importing the libraries"},"content":"\n\ntemperature=\"pelican://osg-htc.org/nasa/nsdf/climate1/llc4320/idx/theta/theta_llc4320_x_y_depth.idx\"\n\nsalinity=\"pelican://osg-htc.org/nasa/nsdf/climate1/llc4320/idx/salt/salt_llc4320_x_y_depth.idx\"\n\nvertical_velocity=\"pelican://osg-htc.org/nasa/nsdf/climate2/llc4320/idx/w/w_llc4320_x_y_depth.idx\"\n\n\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#the-section-below-shows-different-llc4320-fields-we-have-available-in-cloud-each-field-is-400tb","position":9},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Step 2: Reading the metadata file from cloud","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc4320-visualization#step-2-reading-the-metadata-file-from-cloud","position":10},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Step 2: Reading the metadata file from cloud","lvl2":"Step 1: Importing the libraries"},"content":"In this section, you can select any variables that you can declared in the cells above and replace it inside LoadDataset. We are just reading the metadata for the dataset here.\n\ndb=ovp.LoadDataset(temperature)\nprint(f'Dimensions: {db.getLogicBox()[1][0]}*{db.getLogicBox()[1][1]}*{db.getLogicBox()[1][2]}')\nprint(f'Total Timesteps: {len(db.getTimesteps())}')\nprint(f'Field: {db.getField().name}')\nprint('Data Type: float32')\n\n\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-2-reading-the-metadata-file-from-cloud","position":11},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Step 3:  Data Selection","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc4320-visualization#step-3-data-selection","position":12},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Step 3:  Data Selection","lvl2":"Step 1: Importing the libraries"},"content":"This section shows you how to load the data you want. You can select any timestep, region (x,y,z) you want. You can set the quality or resolution of the data as well. Higher quality means the finer(more) data. Not setting any time means first timestep available. Not setting quality means full data which takes a while to load because of the higher filesize.  Since each timestep is >30GB, I am only selecting 1 level out of 90.\n\ndata=db.db.read(time=0,z=[0,1],quality=-4) #Since each timestep is >30GB, I am only selecting 1 level out of 90.\ndata.shape\n\n\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-3-data-selection","position":13},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc4320-visualization#step-4-visualize-the-data","position":14},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"content":"We are using a simple matplotlib here, but since the data is in numpy array, it can loaded with any python modules that support numpy. Feel free to set the vmin,vmax appropriately.\n\nfig,axes=plt.subplots(1,1,figsize=(12,8))\nim= axes.imshow(data[0,:,:], aspect='auto',origin='lower',cmap='turbo')\ncbar = plt.colorbar(im, ax=axes)\ncbar.set_label('Temperature (deg. C)')\nplt.show()\n\n\n\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-4-visualize-the-data","position":15},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"But, what if you want to see the full data for a certain region at a certain depth?","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc4320-visualization#but-what-if-you-want-to-see-the-full-data-for-a-certain-region-at-a-certain-depth","position":16},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"But, what if you want to see the full data for a certain region at a certain depth?","lvl2":"Step 1: Importing the libraries"},"content":"Just set the right x,y,z while reading the data. x and y are the bounding box, z is the depth/layer.\n\ndata1=db.db.read(time=1,z=[0,1],quality=-6,x=[500,2500],y=[8500,11000])\nplt.imshow(data1[0,:,:], origin='lower',cmap='turbo')\nplt.colorbar()\n\n\n\ndata1.shape #\n\n\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#but-what-if-you-want-to-see-the-full-data-for-a-certain-region-at-a-certain-depth","position":17},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 5: Save the data for the region locally"},"type":"lvl2","url":"/notebooks/ocean-llc4320-visualization#step-5-save-the-data-for-the-region-locally","position":18},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 5: Save the data for the region locally"},"content":"You can save the data locally as you want. For example, here we are only saving the region shown above as a numpy array.\n\nnp.save('test_region.npy', data1)\n\n\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-5-save-the-data-for-the-region-locally","position":19},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 6: Load the locally saved region and visualize using matplotlib"},"type":"lvl2","url":"/notebooks/ocean-llc4320-visualization#step-6-load-the-locally-saved-region-and-visualize-using-matplotlib","position":20},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 6: Load the locally saved region and visualize using matplotlib"},"content":"\n\nlocal_data=np.load('test_region.npy')\nplt.imshow(local_data[0,:,:], origin='lower',cmap='turbo')\nplt.colorbar()\n\n\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-6-load-the-locally-saved-region-and-visualize-using-matplotlib","position":21},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 7: Horizontal Slicing"},"type":"lvl2","url":"/notebooks/ocean-llc4320-visualization#step-7-horizontal-slicing","position":22},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 7: Horizontal Slicing"},"content":"\n\ndata1=db.db.read(time=1,x=[500,2500],y=[10500,10501])\ndata1.shape\n\n\n\n\nplt.figure(figsize=(14,8))\nplt.imshow(data1[:,0,:],cmap='turbo')\n# plt.colorbar()\n\n\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-7-horizontal-slicing","position":23},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl2":"Step 7: Horizontal Slicing"},"type":"lvl4","url":"/notebooks/ocean-llc4320-visualization#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":24},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl2":"Step 7: Horizontal Slicing"},"content":"Aashish Panta (\n\naashishpanta0@gmail​.com)\n\nGiorgio Scorzelli (\n\nscrgiorgio@gmail​.com)\n\nValerio Pascucci (\n\npascucci​.valerio@gmail​.com)","type":"content","url":"/notebooks/ocean-llc4320-visualization#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":25},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in this Project Pythia Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1}]}