{"version":"1","records":[{"hierarchy":{"lvl1":"OSDF Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"OSDF Cookbook"},"content":"\n\n\n\n\n\n\n\n\n\nThis Project Pythia Cookbook covers using the Open Science Data Federation (OSDF), a service for streaming scientific data across the globe.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":2},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Motivation"},"content":"Have you ever been frustrated by the complications of accessing scientific data?  Why can’t it “just work”, like watching a Netflix movie?\n\nThe OSDF is a service that simplifies the streaming of a wide range of scientific datasets with a goal that data access “just works”.  It\nis meant to improve data availability for researchers working at any scale from individual laptops to distributed computing services\nsuch as the OSG’s \n\nOSPool.\n\nThis cookbook gives motivating use cases from the geoscience community, including using datasets from NSF NCAR’s \n\nResearch Data Archive (RDA) and the datasets of AWS \n\nOpenData.","type":"content","url":"/#motivation","position":3},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":4},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Authors"},"content":"Harsha R. Hampapura\n\n\nBrian Bockelman\n\n\nAlexander Hoelzeman\n\n\nCarrie Wall\n\n\nEmma Turetsky\n\n\nAmandha Wingert Barok\n\n\nAashish Panta\n\n\nJoanmarie Del Vecchio\n\n\nJustin Hiemstra\n\n\nDouglas Schuster\n\n\nRiley Conroy\n\n\nKibiwott Koech","type":"content","url":"/#authors","position":5},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":6},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":7},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":8},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Structure"},"content":"This cookbook is broken up into two pieces - some background knowledge on the OSDF service itself\nand then a series of motivating examples from different repositories accessible via the OSDF.","type":"content","url":"/#structure","position":9},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"OSDF Fundamentals","lvl2":"Structure"},"type":"lvl3","url":"/#osdf-fundamentals","position":10},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"OSDF Fundamentals","lvl2":"Structure"},"content":"What is the OSDF?  Who supports it? How can it benefit from my science?  A dive into the infrastructure itself.","type":"content","url":"/#osdf-fundamentals","position":11},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using Datasets from NCAR’s Research Data Archive","lvl2":"Structure"},"type":"lvl3","url":"/#using-datasets-from-ncars-research-data-archive","position":12},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using Datasets from NCAR’s Research Data Archive","lvl2":"Structure"},"content":"NSF NCAR’s \n\nResearch Data Archive (RDA) contains a large collection of meteorological, atmospheric composition, and oceanographic observations, and operational and reanalysis model outputs, integrated with NSF NCAR High Performance Compute services to support atmospheric and geosciences research. This chapter demonstrates how to use common data science tools when streaming from the RDA.","type":"content","url":"/#using-datasets-from-ncars-research-data-archive","position":13},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using Datasets from FIU’s Envistor","lvl2":"Structure"},"type":"lvl3","url":"/#using-datasets-from-fius-envistor","position":14},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using Datasets from FIU’s Envistor","lvl2":"Structure"},"content":"Florida International University (FIU) runs the \n\nEnvistor project, aggregating climate datasets from the south Florida region.","type":"content","url":"/#using-datasets-from-fius-envistor","position":15},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using NOAA’s SONAR Fisheries Datasets","lvl2":"Structure"},"type":"lvl3","url":"/#using-noaas-sonar-fisheries-datasets","position":16},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using NOAA’s SONAR Fisheries Datasets","lvl2":"Structure"},"content":"NOAA maintains a copy of its SONAR-based datasets of Atlanta fisheries data in the popular Zarr format.  This chapter shows how to load and use the datasets and fuse it with other products.","type":"content","url":"/#using-noaas-sonar-fisheries-datasets","position":17},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using Sentinel Data from AWS","lvl2":"Structure"},"type":"lvl3","url":"/#using-sentinel-data-from-aws","position":18},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using Sentinel Data from AWS","lvl2":"Structure"},"content":"All of AWS OpenData is connected to the OSDF!  This chapter includes examples of streaming Sentinel-2 data, stored in AWS’s OpenData program, to your notebook.","type":"content","url":"/#using-sentinel-data-from-aws","position":19},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":20},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\nBinder or on your local machine.","type":"content","url":"/#running-the-notebooks","position":21},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":22},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you’ll be able to execute\nand even change the example programs. You’ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.\n\nNote, not all Cookbook chapters are executable. If you do not see\nthe rocket ship icon, such as on this page, you are not viewing an\nexecutable book chapter.","type":"content","url":"/#running-on-binder","position":23},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":24},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will need to follow this workflow:\n\nClone the https://github.com/ProjectPythia/osdf-cookbook repository: git clone https://github.com/ProjectPythia/osdf-cookbook.git\n\nMove into the osdf-cookbook directorycd osdf-cookbook\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate osdf-cookbook\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":25},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)"},"type":"lvl1","url":"/notebooks/osdf-intro","position":0},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)"},"content":"Have you ever pondered why accessing large-scale scientific data is so complicated, while accessing large-scale volumes of movies is so simple on Netflix?\n\nEach data repository has its own website or a set of unique tools for accessing data.  Users are often encouraged to download datasets locally and then do local computations, as repositories prioritize long-term storage and preservation rather than fast or distributed access.\n\nHow does Netflix do it without making you download the whole movie ahead of time?  They leverage a content distribution network (CDN), which caches copies of the most popular movies at opportune locations on the Internet closer to users. They also let you stream your favorite shows so you can start watching while later sections of the show are still downloading.\n\nThe \n\nOSDF, an NSF-funded infrastructure providing a CDN for science, makes this kind of streaming possible for scientific data.  It is connected to popular open science repositories and has hardware embedded across US and international networks and at large computing sites.\n\nThis cookbook provides examples of using the OSDF’s streaming to power science use cases in earth sciences.","type":"content","url":"/notebooks/osdf-intro","position":1},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl2":"Do First, Understand Later"},"type":"lvl2","url":"/notebooks/osdf-intro#do-first-understand-later","position":2},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl2":"Do First, Understand Later"},"content":"How Do You Use the OSDF?\n\nThe service is powered by the same protocol as the web, HTTPS.  Thus, the simplest use case is to download an object by using the browser.\n\nClick on this link:\n\nhttps://​osdf​-director​.osg​-htc​.org​/ospool​/uc​-shared​/public​/OSG​-Staff​/validation​/test​.txt\n\nIf a new tab opened with the text “Hello, World” – congratulations, you used the OSDF!\n\nOSDF is often used in conjunction with computing workflows and downloads occur as part of a script.  For this, a command line client - \n\npelican is utilized.  Try running the following:\n\npelican object get osdf:///routeviews/chicago/route-views.chicago/bgpdata/2025.03/RIBS/rib.20250319.0400.bz2 ./\n\nDepending on the speed of your Internet connection, you may see a progress bar as the download proceeds.\n\nCongratulations, you’re now the proud owner of 72MB of Internet routing data!\n\nFor both of these cases, we downloaded the entire object.  What happens if the dataset contains data for the entire planet but you are only interested in the state of Nebraska?  It’s more effective to stream the subset.  For that, we will use the \n\nPelican Python library; this library will be used throughout the remaining chapters of this cookbook.","type":"content","url":"/notebooks/osdf-intro#do-first-understand-later","position":3},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl2":"About the OSDF"},"type":"lvl2","url":"/notebooks/osdf-intro#about-the-osdf","position":4},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl2":"About the OSDF"},"content":"You don’t need to know how Netflix is built to press “play”.  Similarly, you don’t need to understand the guts of the OSDF to use it in your science.  However, a few key concepts are useful!\n\nOSDF Infrastructure: The map below shows the distributed pieces of the OSDF:\n\nEach “O” on the map is an origin; the origin service connects an existing repository to the OSDF service, making some datasets available and protecting the repository from overload.  Origins are typically placed nearby where the data lives; the origin for the NCAR Research Data Archive (RDA) is in the same datacenter as the RDA.\n\nEach “C” is a cache.  The cache makes temporary copies of objects upon access so, on subsequent accesses, the object comes from the cache and not from the repository.  This reduces the load on the repository and, ideally, increases scalability.\n\nUnified Namespace: The OSDF provides a unified namespace for all available objects.  Each repository receives a unique prefix (the IceCube experiment’s data is available from /icecube; NCAR’s RDA is available from /ncar-rda) and the object can be referenced from within the prefix.\n\nFrom our RouteViews example above, we were interested in accessing the object named chicago/route-views.chicago/bgpdata/2025.03/RIBS/rib.20250319.0400.bz2.  Since the prefix for RouteViews is /routeviews, the entire OSDF name is:osdf://routeviews/chicago/route-views.chicago/bgpdata/2025.03/RIBS/rib.20250319.0400.bz2\n\n“Objects” vs “Files”: You may have noticed that this notebook refers to downloading/streaming “objects” instead of “files”. What’s the difference, and why does OSDF bother making this distinction?\n\nBoth objects and files are ways for computers to store data, and in practice, the earth science calculations in this cookbook use them the same way—regardless of where the data comes from.\n\nThe key difference is the way we typically think about accessing or retrieving that data: When you open files like Word documents or images on your computer, you probably click through folders or directories to find them. But when you’re working with data over the internet, that folder-based structure doesn’t always apply.\n\nIn the OSDF, an object is simply a piece of data that can be shared, like a file—but without needing to think about where it’s stored or how it’s organized on someone else’s computer. “Object” is a more flexible term that works better when data is stored in large systems across many locations.\n\nWarning\n\nImmutable Objects: The OSDF assumes that objects are immutable; once created, they aren’t permitted to be changed.  This allows OSDF to effectively make copies of the objects in the caches.\n\nThis typically works well with scientific datasets: you rarely change your data after you record it!  However, if you start using OSDF more heavily, this is an important requirement to be aware of.","type":"content","url":"/notebooks/osdf-intro#about-the-osdf","position":5},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl2":"Finding My Objects"},"type":"lvl2","url":"/notebooks/osdf-intro#finding-my-objects","position":6},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl2":"Finding My Objects"},"content":"How do you find the object you’re interested in?\n\nTypically, dataset providers connected to the OSDF provide a search, data catalog, or STAC catalog publishing OSDF-style URLs.  You can determine this from the provider’s website; additionally, OSDF maintains a \n\nlist of known links you can peruse.\n\nExplore this cookbook: This cookbook provides examples for how to use OSDF to access:\n\nNCAR’s Research Data Archive.\n\nAWS’s OpenData Program.\n\nThe Envistor platform at Florida International University.","type":"content","url":"/notebooks/osdf-intro#finding-my-objects","position":7},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF"},"type":"lvl1","url":"/notebooks/pelicanfs","position":0},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF"},"content":"\n\n\n\n","type":"content","url":"/notebooks/pelicanfs","position":1},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/pelicanfs#overview","position":2},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Overview"},"content":"Now that you’ve learned about the OSDF, you may be wondering how you can easily access that data from within a notebook using python.\n\nYou can do this using pelicanfs, which is an FSSPec implementation of the Pelican client","type":"content","url":"/notebooks/pelicanfs#overview","position":3},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"This notebook will contain:","lvl2":"Overview"},"type":"lvl3","url":"/notebooks/pelicanfs#this-notebook-will-contain","position":4},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"This notebook will contain:","lvl2":"Overview"},"content":"A brief explanation of FSSPec and PelicanFS\n\nA real-world example using FSSPec, Pelican, XArray, and Zarr\n\nOther common access patterns\n\nFAQs\n\n","type":"content","url":"/notebooks/pelicanfs#this-notebook-will-contain","position":5},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/pelicanfs#prerequisites","position":6},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Prerequisites"},"content":"To better understand this notebook, please familiarize yourself with the following concepts:\n\nConcepts\n\nImportance\n\nNotes\n\nIntro to OSDF\n\nNecessary\n\n\n\nUnderstanding of XArray\n\nHelpful\n\nTo better understand the example workflow\n\nOverview of FSSpec\n\nHelpful\n\nTo better understand the FSSpec library\n\nTime to learn: 20-30 minutes\n\n\n\n","type":"content","url":"/notebooks/pelicanfs#prerequisites","position":7},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/pelicanfs#imports","position":8},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Imports"},"content":"\n\nimport xarray as xr\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport metpy.calc as mpcalc\nfrom metpy.units import units\nimport fsspec\n\n","type":"content","url":"/notebooks/pelicanfs#imports","position":9},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"What are PelicanFS and FSSPec?"},"type":"lvl2","url":"/notebooks/pelicanfs#what-are-pelicanfs-and-fsspec","position":10},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"What are PelicanFS and FSSPec?"},"content":"First, let’s understand PelicanFS and how it integrates with FSSpec\n\n","type":"content","url":"/notebooks/pelicanfs#what-are-pelicanfs-and-fsspec","position":11},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"FSSPec","lvl2":"What are PelicanFS and FSSPec?"},"type":"lvl3","url":"/notebooks/pelicanfs#fsspec","position":12},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"FSSPec","lvl2":"What are PelicanFS and FSSPec?"},"content":"FileSystem Spec (fsspec) is a python library which endeavors to provide a unified interface to many different storage backends. This includes, but is not limited to, posix, https, and s3. It’s used by various data processing libraries such as xarray, pandas, and intake, just to name a few.\n\nTo learn more about FSSPec, visit its \n\ninformation page","type":"content","url":"/notebooks/pelicanfs#fsspec","position":13},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl4":"Protocols","lvl3":"FSSPec","lvl2":"What are PelicanFS and FSSPec?"},"type":"lvl4","url":"/notebooks/pelicanfs#protocols","position":14},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl4":"Protocols","lvl3":"FSSPec","lvl2":"What are PelicanFS and FSSPec?"},"content":"FSSpec differentiates between different interfaces by using a protocol. When you give a path to FSSPec, it determine the which interface to use by the ‘protocol’ that’s included in the path.  For example, a path which starts with https: indicates to FSSpec to use its http interface. This allows users to just user the top level FSSPec library without needing to know the specifics of the underlying interface.","type":"content","url":"/notebooks/pelicanfs#protocols","position":15},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"PelicanFS","lvl2":"What are PelicanFS and FSSPec?"},"type":"lvl3","url":"/notebooks/pelicanfs#pelicanfs","position":16},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"PelicanFS","lvl2":"What are PelicanFS and FSSPec?"},"content":"PelicanFS is an implementation of FSSpec which introduces two new protocols to FSSpec: pelican and osdf. The pelican protocol specifies for FSSpec to the use the PelicanFS implementation access data via a Pelican Federation. To use it, you must specify the federation root. A pelican fsspec path would look like:\n\npelican://<federation-root>/<namespace-path>\n\nThe osdf protocol is a specific instance of the pelican protocol that knows how to access the OSDF. A path using the osdf protocol should not provide the federation root. An osdf fsspec path would look like:\n\nosdf:///<namespace-path>\n\nNote\n\nNotice the three ‘/’ after “osdf:”, this is required for a properly formed osdf path\n\nIf you’d like to understand more about how pelican works, check out the documentation \n\nhere","type":"content","url":"/notebooks/pelicanfs#pelicanfs","position":17},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Putting it all together","lvl2":"What are PelicanFS and FSSPec?"},"type":"lvl3","url":"/notebooks/pelicanfs#putting-it-all-together","position":18},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Putting it all together","lvl2":"What are PelicanFS and FSSPec?"},"content":"What does this mean in practice?\n\nIn an environment where pelicanfs is installed, then if you want to access data from the OSDF using fsspec or any library which uses fsspec, all that you need to do is give it the proper path with the osdf protocol to fsspec, and fsspec and pelicanfs will do all the work to resolve it behind the scenes.\n\n\n\n","type":"content","url":"/notebooks/pelicanfs#putting-it-all-together","position":19},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"A PelicanFS Example using Real Data"},"type":"lvl2","url":"/notebooks/pelicanfs#a-pelicanfs-example-using-real-data","position":20},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"A PelicanFS Example using Real Data"},"content":"The following is an example that shows how pelicanfs works on real world data using FSSPec and XArray to access Zarr data from AWS.\n\nThis portion of the notebook is based off of the \n\nProject Pythia HRRR AWS Cookbook\n\n","type":"content","url":"/notebooks/pelicanfs#a-pelicanfs-example-using-real-data","position":21},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Setting the Proper Path","lvl2":"A PelicanFS Example using Real Data"},"type":"lvl3","url":"/notebooks/pelicanfs#setting-the-proper-path","position":22},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Setting the Proper Path","lvl2":"A PelicanFS Example using Real Data"},"content":"The data for this tutorial is part of AWS Open Data, hosted in the us-west-2 region. The OSDF provides access to that region using the aws-opendata/us-west-1 namespace.\n\nLet’s first create a path which uses the osdf protocol.\n\n# Set the date, hour, variable, and level for the HRRR data\ndate = '20211016'\nhour = '21'\nvar = 'TMP'\nlevel = '2m_above_ground'\n\n# Construct file paths for the Zarr datasets using the osdf protocol\nnamespace_file1 = f'osdf:///aws-opendata/us-west-1/hrrrzarr/sfc/{date}/{date}_{hour}z_anl.zarr/{level}/{var}/{level}/'\nnamespace_file2 = f'osdf:///aws-opendata/us-west-1/hrrrzarr/sfc/{date}/{date}_{hour}z_anl.zarr/{level}/{var}/'\n\n","type":"content","url":"/notebooks/pelicanfs#setting-the-proper-path","position":23},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Using FSSpec to access the data","lvl2":"A PelicanFS Example using Real Data"},"type":"lvl3","url":"/notebooks/pelicanfs#using-fsspec-to-access-the-data","position":24},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Using FSSpec to access the data","lvl2":"A PelicanFS Example using Real Data"},"content":"Now we can access the data using XArray like normal. The two files will accessed using fsspec’s get_mapper function, which knows to use PelicanFS because we created the path using the osdf protocol.\n\n# Get mappers for the Zarr datasets\n\nfile1 = fsspec.get_mapper(namespace_file1)\nfile2 = fsspec.get_mapper(namespace_file2)\n\n# Open the datasets\nds = xr.open_mfdataset([file1, file2], engine='zarr', decode_timedelta=True)\n\n# Display the dataset\nds\n\n","type":"content","url":"/notebooks/pelicanfs#using-fsspec-to-access-the-data","position":25},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Continue the workflow","lvl2":"A PelicanFS Example using Real Data"},"type":"lvl3","url":"/notebooks/pelicanfs#continue-the-workflow","position":26},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Continue the workflow","lvl2":"A PelicanFS Example using Real Data"},"content":"As you can see, xarray streamed the data correctly into the datasets. To prove the workflow works, the next cell continues the computation and generates two plots. This tutorial will not go in depth as to what this code is accomplishing.\n\nIf you’d like to know more about the following workflow, please refer to the \n\nProject Pythia HRRR AWS Cookbook\n\n# Define coordinates for projection\nlon1 = -97.5\nlat1 = 38.5\nslat = 38.5\n\n# Define the Lambert Conformal projection\nprojData = ccrs.LambertConformal(\n    central_longitude=lon1,\n    central_latitude=lat1,\n    standard_parallels=[slat, slat],\n    globe=ccrs.Globe(\n        semimajor_axis=6371229,\n        semiminor_axis=6371229\n    )\n)\n\n# Display dataset coordinates\nds.coords\n\n# Extract temperature data\nairTemp = ds.TMP\n\n# Display the temperature data\nairTemp\n\n# Convert temperature units to Celsius\nairTemp = airTemp.metpy.convert_units('degC')\n\n# Display the converted temperature data\nairTemp\n\n# Extract projection coordinates\nx = airTemp.projection_x_coordinate\ny = airTemp.projection_y_coordinate\n\n# Plot temperature data\nairTemp.plot(figsize=(11, 8.5))\n\n# Compute minimum and maximum temperatures\nminTemp = airTemp.min().compute()\nmaxTemp = airTemp.max().compute()\n\n# Display minimum and maximum temperature values\nminTemp.values, maxTemp.values\n\n# Define contour levels\nfint = np.arange(np.floor(minTemp.values), np.ceil(maxTemp.values) + 2, 2)\n\n# Define plot bounds and resolution\nlatN = 50.4\nlatS = 24.25\nlonW = -123.8\nlonE = -71.2\nres = '50m'\n\n# Create a figure and axis with projection\nfig = plt.figure(figsize=(18, 12))\nax = plt.subplot(1, 1, 1, projection=projData)\nax.set_extent([lonW, lonE, latS, latN], crs=ccrs.PlateCarree())\nax.add_feature(cfeature.COASTLINE.with_scale(res))\nax.add_feature(cfeature.STATES.with_scale(res))\n\n# Add the title\ntl1 = 'HRRR 2m temperature ($^\\\\circ$C)'\ntl2 = f'Analysis valid at: {hour}00 UTC {date}'\nplt.title(f'{tl1}\\n{tl2}', fontsize=16)\n\n# Contour fill\nCF = ax.contourf(x, y, airTemp, levels=fint, cmap=plt.get_cmap('coolwarm'))\n\n# Make a colorbar for the ContourSet returned by the contourf call\ncbar = fig.colorbar(CF, shrink=0.5)\ncbar.set_label(r'2m Temperature ($^\\circ$C)', size='large')\n\n# Show the plot\nplt.show()","type":"content","url":"/notebooks/pelicanfs#continue-the-workflow","position":27},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data"},"type":"lvl1","url":"/notebooks/cmip6-gmst","position":0},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data"},"content":"\n\n","type":"content","url":"/notebooks/cmip6-gmst","position":1},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/cmip6-gmst#overview","position":2},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Overview"},"content":"In this notebook we will compute the Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data and compare it with observations. This notebook is heavily inspired by the \n\nGMST example in the CMIP6 cookbook and we thank the authors for their workflow.\n\nWe will get the CMIP6 temperature data from the AWS open data program via the us-west-2 origin\n\nIn order to do this, we will use an intake-ESM catalog (hosted on NCAR’s RDA) that uses pelicanFS backed links instead of https or s3 links\n\nWe will grab observational data hosted on NCAR’s RDA, which is accessible via the NCAR origin\n\nPlease refer to the first chapter of this cookbook to learn more about OSDF, pelican or pelicanFS\n\nThis notebook demonstrates that you can seamlessly stream data from multiple OSDF origins in your workflow\n\n","type":"content","url":"/notebooks/cmip6-gmst#overview","position":3},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/cmip6-gmst#prerequisites","position":4},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Intake-ESM\n\nNecessary\n\nUsed for searching CMIP6 data\n\nUnderstanding of Zarr\n\nHelpful\n\nFamiliarity with metadata structure\n\nSeaborn\n\nHelpful\n\nUsed for plotting\n\nPelicanFS\n\nNecessary\n\nThe python package used to stream data in this notebook\n\nOSDF\n\nHelpful\n\nOSDF is used to stream data in this notebook\n\nTime to learn: 20 mins\n\n","type":"content","url":"/notebooks/cmip6-gmst#prerequisites","position":5},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Table of Contents"},"type":"lvl2","url":"/notebooks/cmip6-gmst#table-of-contents","position":6},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Table of Contents"},"content":"Set up local dask cluster\n\nData Loading\n\nGMST computation\n\n","type":"content","url":"/notebooks/cmip6-gmst#table-of-contents","position":7},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/cmip6-gmst#imports","position":8},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Imports"},"content":"\n\nfrom matplotlib import pyplot as plt\nimport xarray as xr\nimport numpy as np\nfrom dask.diagnostics import progress\nfrom tqdm.autonotebook import tqdm\nimport intake\nimport fsspec\nimport seaborn as sns\nimport aiohttp\nimport dask\nfrom dask.distributed import LocalCluster\nimport pelicanfs \n\nWe will use an intake-ESM catalog hosted on NCAR’s Research Data Archive. This is nothing but the AWS cmip6 catalog modified to use OSDF\n\n# Load catalog URL\nrda_url     =  'https://data.rda.ucar.edu/'\ncat_url     = rda_url +  'd850001/catalogs/osdf/cmip6-aws/cmip6-osdf-zarr.json'\nprint(cat_url)\n\n","type":"content","url":"/notebooks/cmip6-gmst#imports","position":9},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Set up local dask cluster"},"type":"lvl2","url":"/notebooks/cmip6-gmst#set-up-local-dask-cluster","position":10},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Set up local dask cluster"},"content":"\n\nBefore we do any computation let us first set up a local cluster using dask\n\ncluster = LocalCluster()          \nclient = cluster.get_client()\n\n# Scale the cluster\nn_workers = 4\ncluster.scale(n_workers)\ncluster\n\n","type":"content","url":"/notebooks/cmip6-gmst#set-up-local-dask-cluster","position":11},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Data Loading"},"type":"lvl2","url":"/notebooks/cmip6-gmst#data-loading","position":12},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Data Loading"},"content":"","type":"content","url":"/notebooks/cmip6-gmst#data-loading","position":13},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Load CMIP6 data from AWS","lvl2":"Data Loading"},"type":"lvl3","url":"/notebooks/cmip6-gmst#load-cmip6-data-from-aws","position":14},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Load CMIP6 data from AWS","lvl2":"Data Loading"},"content":"\n\ncol = intake.open_esm_datastore(cat_url)\ncol\n\n# there is currently a significant amount of data for these runs\nexpts = ['historical', 'ssp245', 'ssp370']\n\nquery = dict(\n    experiment_id=expts,\n    table_id='Amon',\n    variable_id=['tas'],\n    member_id = 'r1i1p1f1',\n    #activity_id = 'CMIP',\n)\n\ncol_subset = col.search(require_all_on=[\"source_id\"], **query)\ncol_subset\n\nLet us inspect the zarr store paths to see if we are using the pelican protocol.\n\nWe see that zstore column has paths that start with ‘osdf:///’ instead of ‘https://’ which tells us that we are not using a simple ‘https’ GET request to fetch the data.\n\nIn order to know more about the pelican protocol, please refer to the first chapter of this cookbook.\n\ncol_subset.df\n\n","type":"content","url":"/notebooks/cmip6-gmst#load-cmip6-data-from-aws","position":15},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Grab some Observational time series data for comparison with ensemble spread","lvl2":"Data Loading"},"type":"lvl3","url":"/notebooks/cmip6-gmst#grab-some-observational-time-series-data-for-comparison-with-ensemble-spread","position":16},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Grab some Observational time series data for comparison with ensemble spread","lvl2":"Data Loading"},"content":"The observational data we will use is the HadCRUT5 dataset from the UK Met Office\n\nThe data has been downloaded to NCAR’s Research Data Archive (RDA) from \n\nhttps://​www​.metoffice​.gov​.uk​/hadobs​/hadcrut5/\n\nWe will use an OSDF to access this copy from the RDA. Again the links will start with ‘osdf:///’\n\n%%time\nobs_url    = 'osdf:///ncar/rda/d850001/HadCRUT.5.0.2.0.analysis.summary_series.global.monthly.nc'\n#\nobs_ds = xr.open_dataset(obs_url, engine='h5netcdf').tas_mean\nobs_ds\n\n","type":"content","url":"/notebooks/cmip6-gmst#grab-some-observational-time-series-data-for-comparison-with-ensemble-spread","position":17},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Some helpful functions","lvl2":"Data Loading"},"type":"lvl3","url":"/notebooks/cmip6-gmst#some-helpful-functions","position":18},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Some helpful functions","lvl2":"Data Loading"},"content":"\n\ndef drop_all_bounds(ds):\n    drop_vars = [vname for vname in ds.coords\n                 if (('_bounds') in vname ) or ('_bnds') in vname]\n    return ds.drop_vars(drop_vars)\n\ndef open_dset(df):\n    assert len(df) == 1\n    mapper = fsspec.get_mapper(df.zstore.values[0])\n    #path = df.zstore.values[0][7:]+\".zmetadata\"\n    ds = xr.open_zarr(mapper, consolidated=True)\n    return drop_all_bounds(ds)\n\ndef open_delayed(df):\n    return dask.delayed(open_dset)(df)\n\nfrom collections import defaultdict\ndsets = defaultdict(dict)\n\nfor group, df in col_subset.df.groupby(by=['source_id', 'experiment_id']):\n    dsets[group[0]][group[1]] = open_delayed(df)\n\ndsets_ = dask.compute(dict(dsets))[0]\n\n#calculate global means\ndef get_lat_name(ds):\n    for lat_name in ['lat', 'latitude']:\n        if lat_name in ds.coords:\n            return lat_name\n    raise RuntimeError(\"Couldn't find a latitude coordinate\")\n\ndef global_mean(ds):\n    lat = ds[get_lat_name(ds)]\n    weight = np.cos(np.deg2rad(lat))\n    weight /= weight.mean()\n    other_dims = set(ds.dims) - {'time'}\n    return (ds * weight).mean(other_dims)\n\n#calculate global means\ndef get_lat_name(ds):\n    for lat_name in ['lat', 'latitude']:\n        if lat_name in ds.coords:\n            return lat_name\n    raise RuntimeError(\"Couldn't find a latitude coordinate\")\n\ndef global_mean(ds):\n    lat = ds[get_lat_name(ds)]\n    weight = np.cos(np.deg2rad(lat))\n    weight /= weight.mean()\n    other_dims = set(ds.dims) - {'time'}\n    return (ds * weight).mean(other_dims)\n\n","type":"content","url":"/notebooks/cmip6-gmst#some-helpful-functions","position":19},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"GMST computation"},"type":"lvl2","url":"/notebooks/cmip6-gmst#gmst-computation","position":20},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"GMST computation"},"content":"\n\nexpt_da = xr.DataArray(expts, dims='experiment_id', name='experiment_id',\n                       coords={'experiment_id': expts})\n\ndsets_aligned = {}\n\nfor k, v in tqdm(dsets_.items()):\n    expt_dsets = v.values()\n    if any([d is None for d in expt_dsets]):\n        print(f\"Missing experiment for {k}\")\n        continue\n\n    for ds in expt_dsets:\n        ds.coords['year'] = ds.time.dt.year\n\n    # workaround for\n    # https://github.com/pydata/xarray/issues/2237#issuecomment-620961663\n    dsets_ann_mean = [v[expt].pipe(global_mean).swap_dims({'time': 'year'})\n                             .drop_vars('time').coarsen(year=12).mean()\n                      for expt in expts]\n\n    # align everything with the 4xCO2 experiment\n    dsets_aligned[k] = xr.concat(dsets_ann_mean, join='outer',dim=expt_da)\n\n%%time\nwith progress.ProgressBar():\n    dsets_aligned_ = dask.compute(dsets_aligned)[0]\n\nsource_ids = list(dsets_aligned_.keys())\nsource_da = xr.DataArray(source_ids, dims='source_id', name='source_id',\n                         coords={'source_id': source_ids})\n\nbig_ds = xr.concat([ds.reset_coords(drop=True)\n                    for ds in dsets_aligned_.values()],\n                    dim=source_da)\n\nbig_ds\n\nHint\n\nNote that even though the variable is called tas, the DataArray big_ds actually has the global and annual mean of surface temperatures! If you are wondering why this is the case, take a look at all the functions that were applied to obtain dsets_ann_mean!\n\n# Compute annual mean temperatures anomalies of observational data\nobs_gmsta = obs_ds.resample(time='YS').mean(dim='time')\n# obs_gmsta\n\n","type":"content","url":"/notebooks/cmip6-gmst#gmst-computation","position":21},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Compute anomlaies and plot","lvl2":"GMST computation"},"type":"lvl3","url":"/notebooks/cmip6-gmst#compute-anomlaies-and-plot","position":22},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Compute anomlaies and plot","lvl2":"GMST computation"},"content":"We will compute the temperature anomalies w.r.t 1960-1990 baseline period\n\nConvert xarray datasets to pandas dataframes\n\nUse Seaborn to plot GMSTA\n\ndf_all = big_ds.to_dataframe().reset_index()\ndf_all.head()\n\n# Define the baseline period\nbaseline_df = df_all[(df_all[\"year\"] >= 1960) & (df_all[\"year\"] <= 1990)]\n\n# Compute the baseline mean\nbaseline_mean = baseline_df[\"tas\"].mean()\n\n# Compute anomalies\ndf_all[\"tas_anomaly\"] = df_all[\"tas\"] - baseline_mean\ndf_all\n\nobs_df = obs_gmsta.to_dataframe(name='tas_anomaly').reset_index()\n\n# Convert 'time' to 'year' (keeping only the year)\nobs_df['year'] = obs_df['time'].dt.year\n\n# Drop the original 'time' column since we extracted 'year'\nobs_df = obs_df[['year', 'tas_anomaly']]\nobs_df\n\nAlmost there! Let us now use seaborn to plot all the anomalies\n\ng = sns.relplot(data=df_all, x=\"year\", y=\"tas_anomaly\",\n                hue='experiment_id', kind=\"line\", errorbar=\"sd\", aspect=2, palette=\"Set2\")  # Adjust the color palette)\n\n# Get the current axis from the FacetGrid\nax = g.ax\n\n# Overlay the observational data in red\nsns.lineplot(data=obs_df, x=\"year\", y=\"tas_anomaly\",color=\"red\", \n             linestyle=\"dashed\", linewidth=2,label=\"Observations\", ax=ax)\n\n# Adjust the legend to include observations\nax.legend(title=\"Experiment ID + Observations\")\n\n# Show the plot\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/cmip6-gmst#compute-anomlaies-and-plot","position":23},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/cmip6-gmst#summary","position":24},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Summary"},"content":"In this notebook, we used surface air temperature data from several CMIP6 models for the ‘historical’, ‘SSP245’ and ‘SSP370’ runs to compute Global Mean Surface Temperature Anomaly (GMSTA) relative to the 1960-1990 baseline period and compare it with anomalies computed from the HadCRUT monthly surface temperature dataset. We used a modified intake-ESM catalog and pelicanFS to ‘stream/download’ temperature data from two different OSDF origins. The CMIP6 model data was streamed from the AWS OpenData origin in the us-west-2 region and the observational data was streamed from NCAR’s OSDF origin.\n\n","type":"content","url":"/notebooks/cmip6-gmst#summary","position":25},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/cmip6-gmst#resources-and-references","position":26},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Resources and references"},"content":"Original notebook in the Pangeo Gallery by Henri Drake and Ryan Abernathey\n\nCMIP6 cookbook by Ryan Abernathey, Henri Drake, Robert Ford and Max Grover\n\nCoupled Model Intercomparison Project 6 was accessed from \n\nhttps://​registry​.opendata​.aws​/cmip6 using a modified intake-ESM catalog hosted on NCAR’s RDA\n\nWe thank the UK Met Office Hadley Center for providing the observational data","type":"content","url":"/notebooks/cmip6-gmst#resources-and-references","position":27},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive"},"type":"lvl1","url":"/notebooks/ncar-intro","position":0},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive"},"content":"\n\n\n\n","type":"content","url":"/notebooks/ncar-intro","position":1},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/ncar-intro#overview","position":2},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Overview"},"content":"The Research Data Archive (RDA) is a large, publicly accessible collection of atmospheric, oceanic, and related geophysical data managed at the National Center for Atmposheric Research (NCAR) sponsored by the National Science Foundation (NSF).\n\nCurrently the Research Data Archive can be visited using the link \n\nhttps://​rda​.ucar​.edu","type":"content","url":"/notebooks/ncar-intro#overview","position":3},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl3":"In this tutorial you will learn:","lvl2":"Overview"},"type":"lvl3","url":"/notebooks/ncar-intro#in-this-tutorial-you-will-learn","position":4},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl3":"In this tutorial you will learn:","lvl2":"Overview"},"content":"Purpose of the RDA\n\nTypes of data\n\nFeatures and tools\n\nAccess and connection to OSDF\n\n\n\n","type":"content","url":"/notebooks/ncar-intro#in-this-tutorial-you-will-learn","position":5},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Purpose of the RDA"},"type":"lvl2","url":"/notebooks/ncar-intro#purpose-of-the-rda","position":6},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Purpose of the RDA"},"content":"The RDA provides free access to curated data for research, with an emphasis on datasets which provide high value to NCAR and member university researchers. Additionally, the RDA provides value added services and tools to help scientists discover, access, and manipulate data. There is also an increasing empahsis on enabling users to stream data to their local computational environment or apply computations directly to data.\n\n","type":"content","url":"/notebooks/ncar-intro#purpose-of-the-rda","position":7},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Types of Data"},"type":"lvl2","url":"/notebooks/ncar-intro#types-of-data","position":8},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Types of Data"},"content":"The RDA data has a wide variety of datasets that cover many different domains of geosciences including:\n\nGlobal and regional reanalysis datasets (e.g., \n\nERA5, \n\nNCEP/NCAR, \n\nJRA-3Q)\n\nNumerical weather prediction model output (e.g. \n\nGFS)\n\nObservational data (e.g., surface, radiosonde, satellite)\n\nClimate model simulations (e.g., \n\nCESM)\n\nOceanographic datasets (e.g. \n\nICOADS)","type":"content","url":"/notebooks/ncar-intro#types-of-data","position":9},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl3":"File Formats","lvl2":"Types of Data"},"type":"lvl3","url":"/notebooks/ncar-intro#file-formats","position":10},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl3":"File Formats","lvl2":"Types of Data"},"content":"NetCDF4\n\nGrib\n\nProprietary binary\n\nBUFR\n\nZarr\n\nKerchunk\n\n","type":"content","url":"/notebooks/ncar-intro#file-formats","position":11},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Features and Tools"},"type":"lvl2","url":"/notebooks/ncar-intro#features-and-tools","position":12},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Features and Tools"},"content":"\n\nSearch and filtering tools for finding datasets\n\nData subsetting and format conversion\n\nDocumentation and metadata for reproducibility\n\nAPIs and scripts for automated access\n\n","type":"content","url":"/notebooks/ncar-intro#features-and-tools","position":13},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Access and OSDF"},"type":"lvl2","url":"/notebooks/ncar-intro#access-and-osdf","position":14},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Access and OSDF"},"content":"All datasets are open and free to download, however registration (via ORCID) is required for subset requests.\n\nIf you made it this far, you might wonder what the RDA has to do with the Open Science Data Federation (OSDF)?\n\nA:\nThe RDA is a member of the OSDF and its data holdings are served via an origin.\nThe implications are that subsequent requests of the same data will result in lower latency as data will be stored at a geographically close cache.\n\n\n\n","type":"content","url":"/notebooks/ncar-intro#access-and-osdf","position":15},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/ncar-intro#summary","position":16},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Summary"},"content":"In essence, the NCAR RDA is a vital resource for the climate and weather research community, providing long-term, reliable access to high-quality Earth system data.","type":"content","url":"/notebooks/ncar-intro#summary","position":17},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/ncar-intro#whats-next","position":18},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl3":"What’s next?","lvl2":"Summary"},"content":"RDA is currently undergoing a rebranding effort and will soon be renamed GDEX, which stands for Geoscience Data Exchange. Essentially, all references to RDA can be replaced with GDEX as of Sep 9, 2025.\n\n","type":"content","url":"/notebooks/ncar-intro#whats-next","position":19},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/ncar-intro#resources-and-references","position":20},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Resources and references"},"content":"https://​rda​.ucar​.edu/\n\nContact \n\nrdahelp@ucar.edu if you want to learn more about the RDA.","type":"content","url":"/notebooks/ncar-intro#resources-and-references","position":21},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters"},"type":"lvl1","url":"/notebooks/envistor-foundations","position":0},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters"},"content":"This notebook introduces a user-facing science workflow that explores salinity patterns in South Florida using curated buoy data published through the EnviStor smart data pipeline and made accessible via PelicanFS as part of the Open Science Data Federation (OSDF).\n\nEnviStor is an AI-assisted, modular data pipeline developed at Florida International University (FIU) to process and publish environmental datasets. It automates tasks like file type detection, metadata generation, geospatial transformations, and dataset publication. For this use case, EnviStor ingested buoy data, processed and cleaned it, and published it through the OSDF federation, making it available to science users via PelicanFS.\n\n\n\n","type":"content","url":"/notebooks/envistor-foundations","position":1},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/envistor-foundations#overview","position":2},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Overview"},"content":"This notebook introduces a real-world scientific workflow built around the question: “What are the salinity patterns in South Florida?” Using curated data from FIU’s buoy network—processed by the EnviStor smart pipeline and accessed through OSDF using PelicanFS, we explore how environmental datasets can be made reproducible, discoverable, and usable by domain scientists.\n\nBelow is a breakdown of what this notebook covers:\n\n1. What is Salinity?\nA brief introduction to salinity, its units, and its role in coastal science.\n\n2. Why South Florida?\nContext around why this region is important for salinity monitoring, including environmental and societal implications.\n\n3. About the Dataset\nDescription of the FIU CREST buoy network, the variables collected, and how the data is prepared through EnviStor.\n\n4. Data Access with PelicanFS\nExplanation of how the dataset is accessed directly from OSDF using PelicanFS, and how that supports open science.\n\n5. Research Question\nA clear framing of the central question that will be explored in the analysis notebook.\n\n6. What’s Next?\nA preview of what readers will do in the second notebook: loading, analyzing, and visualizing the data.\n\nBy the end of this notebook, the reader should understand the scientific motivation, the data context, and how curated environmental data made available via OSDF can be leveraged in reproducible Jupyter-based workflows.\n\n","type":"content","url":"/notebooks/envistor-foundations#overview","position":3},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/envistor-foundations#prerequisites","position":4},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Prerequisites"},"content":"To fully understand the context and content of this notebook, readers should be familiar with a few foundational concepts and tools. While this notebook itself is explanatory and doesn’t run any code, it introduces ideas and data structures that are implemented in the accompanying technical notebook.\n\nBelow is a table of key concepts and how important they are for this material:\n\nConcepts\n\nImportance\n\nNotes\n\nWhat is Salinity?\n\nNecessary\n\nCore environmental concept used in the notebook\n\nIntro to Buoy-based monitoring\n\nHelpful\n\nUnderstanding how buoy data is collected\n\nPelicanFS Overview\n\nNecessary\n\nExplains how the curated data is accessed from OSDF\n\nWhat is OSDF\n\nHelpful\n\nHigh-level context for how EnviStor data is shared\n\nTime to learn: 20-30 minutes. (~5–10 minutes per external concept; extra time optional for those new to Pelican/OSDF)\n\n\n\n","type":"content","url":"/notebooks/envistor-foundations#prerequisites","position":5},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"What is Salinity?"},"type":"lvl2","url":"/notebooks/envistor-foundations#what-is-salinity","position":6},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"What is Salinity?"},"content":"Salinity measures how much salt is dissolved in water, typically in Practical Salinity Units (PSU).\n\nSalinity influences ocean circulation, marine life, and freshwater availability. In coastal regions like South Florida, it’s a key indicator of environmental change, particularly for detecting saltwater intrusion into freshwater systems.\n\n","type":"content","url":"/notebooks/envistor-foundations#what-is-salinity","position":7},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Why South Florida?"},"type":"lvl2","url":"/notebooks/envistor-foundations#why-south-florida","position":8},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Why South Florida?"},"content":"South Florida is particularly sensitive to salinity changes due to its mix of freshwater inflows, tidal dynamics, sea level rise, and storm surge events. Monitoring salinity helps scientists detect:\n\nSaltwater intrusion into aquifers\n\nSeasonal or storm-related shifts in water quality\n\nLong-term climate-driven changes\n\n","type":"content","url":"/notebooks/envistor-foundations#why-south-florida","position":9},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"About the Dataset"},"type":"lvl2","url":"/notebooks/envistor-foundations#about-the-dataset","position":10},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"About the Dataset"},"content":"This dataset comes from FIU’s CREST buoy network, which continuously measures water quality parameters including salinity, temperature, turbidity, and more.\n\nFor this analysis, we selected three stations:\n\nBuoy 2: NW Biscayne Bay\n\nBuoy 3: Haulover Inlet\n\nBuoy 3-2: Little River\n\nThese datasets were processed through the EnviStor smart pipeline, which cleaned, standardized, and published them into the Open Science Data Federation (OSDF). The data is now directly accessible using PelicanFS, a high-performance data access layer.\n\n","type":"content","url":"/notebooks/envistor-foundations#about-the-dataset","position":11},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Data Access with PelicanFS"},"type":"lvl2","url":"/notebooks/envistor-foundations#data-access-with-pelicanfs","position":12},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Data Access with PelicanFS"},"content":"The curated data is available in the /envistor namespace on OSDF. We will access it directly in the next notebook using PelicanFS, a high-performance file system interface developed by the OSG and Pathfinders community.\n\nNo pre-downloaded files are needed — all data will be loaded live from OSDF via PelicanFS.\n\nNote\n\nIf you’re running this notebook as part of the Pythia Cook-off, PelicanFS is already set up in the environment — no extra steps are needed.\n\nHowever, if you’re adapting this workflow for use outside the Cook-off platform, you’ll need to install and mount PelicanFS on your system to access data from OSDF. For instructions, see the \n\nPelican documentation.\n\n","type":"content","url":"/notebooks/envistor-foundations#data-access-with-pelicanfs","position":13},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Research Question"},"type":"lvl2","url":"/notebooks/envistor-foundations#research-question","position":14},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Research Question"},"content":"We aim to explore:“How does water salinity vary across time and space in South Florida coastal waters?”\n\nThis question is central to understanding environmental patterns related to:\n\nSeasonality (e.g., wet vs. dry season)\n\nFreshwater discharge (e.g., from canals or rivers)\n\nSaltwater intrusion (e.g., due to sea level rise or storm surge)\n\nTo answer it, we will compare salinity data collected from three distinct coastal monitoring stations (NW Biscayne Bay, Haulover Inlet, and Little River) over a multi-month period. These stations span both urban and natural areas, helping us examine spatial variation.\n\nBy plotting and analyzing this data over time, we can begin to identify how salinity responds to both natural processes and human-driven impacts — providing insights useful for coastal management and long-term monitoring.\n\n\n\n","type":"content","url":"/notebooks/envistor-foundations#research-question","position":15},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/envistor-foundations#summary","position":16},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Summary"},"content":"In this notebook, we explored the motivation, context, and scientific importance of analyzing salinity patterns in South Florida. We introduced the curated buoy datasets processed through the EnviStor smart data pipeline and made accessible via PelicanFS, part of the Open Science Data Federation. The reader now understands why salinity matters, how the data was collected and prepared, and what question the upcoming technical notebook will answer. This context sets the stage for a reproducible, user-facing environmental analysis powered by FAIR data infrastructure.","type":"content","url":"/notebooks/envistor-foundations#summary","position":17},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl3":"What’s Next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/envistor-foundations#whats-next","position":18},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl3":"What’s Next?","lvl2":"Summary"},"content":"In the next notebook, we will:\n\nLoad salinity datasets for three buoy stations (NW Biscayne Bay, Haulover Inlet, and Little River) directly from OSDF via PelicanFS.\n\nCombine and clean the data: We’ll merge the datasets into a single time series, convert timestamps, label each station, and handle any missing or invalid salinity readings.\n\nResample to daily averages to reduce noise and highlight broader trends.\n\nVisualize the results using line plots to show salinity over time for each location, making it easy to compare patterns across stations.\n\nAnalyze temporal and spatial patterns, such as seasonal variations, sudden shifts, or location-specific behaviors that may suggest environmental changes like saltwater intrusion or storm impact.\n\nThis notebook will give users a complete, reproducible workflow that demonstrates how curated environmental data can be analyzed using standard Python tools — all made possible by the EnviStor pipeline and OSDF infrastructure.","type":"content","url":"/notebooks/envistor-foundations#whats-next","position":19},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida"},"type":"lvl1","url":"/notebooks/envistor-technical","position":0},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida"},"content":"This notebook demonstrates how to analyze salinity patterns in South Florida using curated buoy data processed by the EnviStor smart data pipeline and made available through PelicanFS, a high-performance file system interface for the Open Science Data Federation (OSDF).\n\nThe data used here comes from three monitoring stations managed by FIU, each located in a distinct part of South Florida’s coastal waters. These datasets were curated and made analysis-ready by the EnviStor pipeline.\n\nThe central question is:\n\nWhat are the salinity patterns in South Florida?\n\nWe’ll answer this by loading the data, cleaning and merging it, and visualizing salinity trends over time across multiple locations.\n\n\n\n","type":"content","url":"/notebooks/envistor-technical","position":1},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/envistor-technical#imports","position":2},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Imports"},"content":"\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pelicanfs.core import OSDFFileSystem\nfrom io import BytesIO\n\nsns.set(style=\"whitegrid\")\n\n","type":"content","url":"/notebooks/envistor-technical#imports","position":3},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Load the Curated Salinity Datasets"},"type":"lvl2","url":"/notebooks/envistor-technical#load-the-curated-salinity-datasets","position":4},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Load the Curated Salinity Datasets"},"content":"We’ll load data from three buoy stations directly from PelicanFS using the OSDFFileSystem() interface. These Excel files have already been curated and prepared by the EnviStor smart pipeline, and are stored in the /envistor namespace in OSDF.\n\nEach file corresponds to a different buoy location:\n\nBiscayne Bay\n\nHaulover Inlet\n\nLittle River\n\nWe use BytesIO to read the content as a stream before passing it to pandas.read_excel(). Each resulting DataFrame includes a \"Station\" column to identify its source location.\n\npelfs = OSDFFileSystem()\nfile_buoy1 = pelfs.cat('/envistor/CREST_Buoy_2_NW_Biscayne_Bay_-_S_of_Biscayne_Canal_082720-112221.xlsx')\nfile_buoy2 = pelfs.cat('/envistor/CREST_Buoy_3_Haulover_Inlet_100518_-_073020_updated.xlsx')\nfile_buoy3 = pelfs.cat('/envistor/CREST_Buoy_3-2_Little_River_042121-050624.xlsx')\n\nexcel_file1 = BytesIO(file_buoy1)\ndf_file_buoy1 = pd.read_excel(excel_file1)\ndf_file_buoy1['Station'] = 'Buoy - Biscayne Bay'\n\nexcel_file2 = BytesIO(file_buoy2)\ndf_file_buoy2 = pd.read_excel(excel_file2)\ndf_file_buoy2['Station'] = 'Buoy - Haulover Inlet'\n\nexcel_file3 = BytesIO(file_buoy3)\ndf_file_buoy3 = pd.read_excel(excel_file3)\ndf_file_buoy3['Station'] = 'Little River'\n\n\n","type":"content","url":"/notebooks/envistor-technical#load-the-curated-salinity-datasets","position":5},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Clean and Combine the Data"},"type":"lvl2","url":"/notebooks/envistor-technical#clean-and-combine-the-data","position":6},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Clean and Combine the Data"},"content":"Now that we’ve loaded the individual DataFrames, we’ll prepare them for analysis.\n\nHere’s what we do:\n\nCreate a timestamp: Combine the \"Date\" and \"Time\" columns into a single datetime column.\n\nStandardize salinity: Rename the \"Sal_psu\" column to \"Salinity\" and convert its values to numeric (in case of string or error-prone entries).\n\nMerge datasets: Concatenate the three cleaned DataFrames into one (df_all) so we can analyze salinity trends across all buoy stations together. We also drop any rows with missing salinity values and set the datetime column as the index to enable time-based operations later.\n\nfor df in [df_file_buoy1, df_file_buoy2, df_file_buoy3]:\n    df[\"datetime\"] = pd.to_datetime(df[\"Date\"].astype(str) + \" \" + df[\"Time\"].astype(str))\n\n    df.rename(columns={\"Sal_psu\": \"Salinity\"}, inplace=True)\n    df[\"Salinity\"] = pd.to_numeric(df[\"Salinity\"], errors=\"coerce\")\n\ndf_all = pd.concat([df_file_buoy1, df_file_buoy2, df_file_buoy3], ignore_index=True)\ndf_all.dropna(subset=[\"Salinity\"], inplace=True)\ndf_all.set_index(\"datetime\", inplace=True)\n\n\n","type":"content","url":"/notebooks/envistor-technical#clean-and-combine-the-data","position":7},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Resample and Aggregate"},"type":"lvl2","url":"/notebooks/envistor-technical#resample-and-aggregate","position":8},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Resample and Aggregate"},"content":"Salinity readings are taken multiple times per day. To reveal broader trends, we resample the data to daily averages.\n\nThis step:\n\nReduces noise\n\nMakes it easier to compare across time\n\nPrepares the data for visualization\n\nWe group by station and resample by day ('1D').\n\ndf_daily = (\n    df_all\n    .groupby(\"Station\")\n    .resample(\"1D\")\n    [\"Salinity\"]\n    .mean()\n    .reset_index()\n)\n\n\n","type":"content","url":"/notebooks/envistor-technical#resample-and-aggregate","position":9},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Visualize the Salinity Patterns"},"type":"lvl2","url":"/notebooks/envistor-technical#visualize-the-salinity-patterns","position":10},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Visualize the Salinity Patterns"},"content":"Now we can plot daily salinity patterns to compare how they evolve over time across the three locations.\n\nWe’ll use Seaborn for a clean, readable line chart.\n\nplt.figure(figsize=(14, 6))\nsns.lineplot(data=df_daily, x=\"datetime\", y=\"Salinity\", hue=\"Station\")\nplt.title(\"Daily Average Salinity in South Florida (by Buoy Station)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Salinity (psu)\")\nplt.legend(title=\"Station\")\nplt.tight_layout()\nplt.show()\n\n\nNote\n\n💡 Why this is possible\n\nThis visualization is only possible thanks to the EnviStor Smart Pipeline, which curated, cleaned, and enriched the buoy data, and published it to OSDF.\n\nAdditionally, the Pelican platform allowed us to access the data on-demand using PelicanFS — no local downloads or manual data wrangling required. This is a great example of how data infrastructure can directly support scientific insight.\n\n","type":"content","url":"/notebooks/envistor-technical#visualize-the-salinity-patterns","position":11},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Interpret the Results"},"type":"lvl2","url":"/notebooks/envistor-technical#interpret-the-results","position":12},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Interpret the Results"},"content":"With the visualization in hand, we can start identifying key salinity patterns in South Florida:\n\nBiscayne Bay (blue) shows a clear seasonal fluctuation, with higher salinity in dry months and noticeable drops likely linked to storm events or freshwater inflow.\n\nHaulover Inlet (orange) tends to have consistently higher salinity levels, suggesting stronger tidal mixing and less influence from freshwater discharge.\n\nLittle River (green) displays the most variability — sharp dips and spikes in salinity hint at frequent freshwater input, possibly from canals, rain events, or upstream runoff.\n\nThese differences illustrate how geographic location and local hydrology impact salinity levels. By comparing trends across stations, we gain insight into how dynamic and localized coastal salinity can be.\n\nNote\n\nThis type of analysis can support environmental monitoring, resource management, and research on saltwater intrusion and estuarine health.\n\n","type":"content","url":"/notebooks/envistor-technical#interpret-the-results","position":13},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Next Steps"},"type":"lvl2","url":"/notebooks/envistor-technical#next-steps","position":14},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Next Steps"},"content":"This notebook provided a foundation for analyzing coastal salinity patterns using curated data from the EnviStor pipeline. If you’re interested in extending this work, here are a few ideas:\n\nIncorporate other environmental variables: Analyze how temperature, turbidity, or dissolved oxygen vary alongside salinity to build a more holistic view of water quality.\n\nAdd spatial analysis: Use GIS tools or libraries (e.g., Cartopy or Folium) to visualize station locations and explore spatial gradients in salinity.\n\nCompare across years: Investigate long-term salinity trends and identify anomalies across different seasons or years.\n\nInclude other datasets from EnviStor: Expand the workflow by pulling additional datasets from OSDF, such as ocean currents, rainfall, or metadata-enriched observations.\n\nDevelop alert thresholds: Identify salinity levels that may signal ecological stress or risk, potentially integrating this with decision-making tools.\n\nHint\n\nCurious about how the data got so clean? Check out the \n\nEnviStor smart pipeline’s role in preparing these files — from metadata tagging to anomaly filtering — to better understand the power of backend automation.","type":"content","url":"/notebooks/envistor-technical#next-steps","position":15},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data"},"type":"lvl1","url":"/notebooks/foundations","position":0},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data"},"content":"Let’s start here! If you can directly link to an image relevant to your notebook, such as \n\ncanonical logos, do so here at the top of your notebook. You can do this with MyST Markdown syntax, outlined in \n\nthis MyST guide, or you edit this cell to see a demonstration. Be sure to include alt text for any embedded images to make your content more accessible.\n\nNext, title your notebook appropriately with a top-level Markdown header, # (see the very first cell above). Do not use this level header anywhere else in the notebook. Our book build process will use this title in the navbar, table of contents, etc. Keep it short, keep it descriptive.\n\nFollow this with a --- cell to visually distinguish the transition to the prerequisites section.\n\n\n\n","type":"content","url":"/notebooks/foundations","position":1},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/foundations#overview","position":2},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl2":"Overview"},"content":"Scientists collect data on the water column using sonars; NOAA Fisheries routinely use this technology to identify fish schools and estimate biomass for fisheries stock assessments. Because of their value to our nation, these data are stewarded at the NOAA National Centers for Environmental Information. \n\nNCEI’s Water Column Sonar Data Archive currently holds over 350 TB of data collected over 20 years in all areas of the U.S. Exclusive Economic Zone. A copy of these archived data are accessible on Amazon Web Services (AWS) through the NOAA Open Data Dissemination Program.\n\nRaw sonar files are complex and binary. To make them more accessible, analysis-ready and cloud-optimized, the Sonar AI team has converted a subset of those files into \n\nZarr stores. The team is currently focusing on \n\nEK60 sonar systems run on the \n\nNOAA Ship Henry B. Bigelow by the \n\nNOAA Northeast Fisheries Science Center (NEFSC). NEFSC’s main objective for collecting these data is to determine the biomass of Atlantic herring Clupea harengus, which contributes to the commercial lobster industry.\n\nYou can explore the Zarr translated files using \n\nEchoFish, the team’s AWS-hosted interactive portal for data exploration.\n\nThe goals of the \n\nNational Science Foundation funded Sonar AI project are to\n\nFuse multiple decades of water column sonar data\n\nDevelop novel self-supervised machine learning models to extract patterns from the sonar data across large spatio-temporal scales, and integrate with environmental and geographical information to classify clusters in the context of oceanographic and biologically significant features.\n\nEngage the community by sharing results through knowledge graphs and open source workflows (like this!)\n\nUltimately, the team will build cyberinfrastructure that supports a robust, community-driven analysis pipeline.\n\nBelow we demonstrate the steps to integrate clusters detected from sonar data with temperature data pulled from a nearby ocean buoy.\n\nRead in sonar data from Zarr stores on AWS. We are specifically pulling in a set of data collected on the HB1906 cruise between October 12 - 14, 2019.\n\nAggregate the \n\nSv values into biologically relevant depth bins and calculate the \n\nNautical Area Scattering Coefficient (NASC)\n\nPull temperature data from the \n\nNOAA National Buoy Data Center (NDBC) Station 44011, located on George’s Bank near Nantucket Shoals off the Northeast U.S.\n\nPlot the temperature data with the NASC values, calculate the correlation\n\n","type":"content","url":"/notebooks/foundations#overview","position":3},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/foundations#prerequisites","position":4},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nXarray\n\nNecessary\n\n\n\nIntroduction to Pandas\n\nNecessary\n\n\n\nNumpy Basics\n\nNecessary\n\n\n\nPython for Atmosphere and Ocean Scientists\n\nHelpful\n\nFamiliarity with ocean science-driven code\n\nPangeo Gallery\n\nHelpful\n\nConnection to a wide variety of open datasets\n\nTime to learn: estimate in minutes. For a rough idea, use 5 mins per subsection, 10 if longer; add these up for a total. Safer to round up and overestimate.\n\nSystem requirements:\n\nPopulate with any system, version, or non-Python software requirements if necessary\n\nOtherwise use the concepts table above and the Imports section below to describe required packages as necessary\n\nIf no extra requirements, remove the System requirements point altogether\n\n\n\n","type":"content","url":"/notebooks/foundations#prerequisites","position":5},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/foundations#imports","position":6},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl2":"Imports"},"content":"Begin your body of content with another --- divider before continuing into this section, then remove this body text and populate the following code cell with all necessary Python imports up-front:\n\nimport sys\n\n","type":"content","url":"/notebooks/foundations#imports","position":7},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl2":"Your first content section"},"type":"lvl2","url":"/notebooks/foundations#your-first-content-section","position":8},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl2":"Your first content section"},"content":"\n\nThis is where you begin your first section of material, loosely tied to your objectives stated up front. Tie together your notebook as a narrative, with interspersed Markdown text, images, and more as necessary,\n\n# as well as any and all of your code cells\nprint(\"Hello world!\")\n\n","type":"content","url":"/notebooks/foundations#your-first-content-section","position":9},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl3":"A content subsection","lvl2":"Your first content section"},"type":"lvl3","url":"/notebooks/foundations#a-content-subsection","position":10},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl3":"A content subsection","lvl2":"Your first content section"},"content":"Divide and conquer your objectives with Markdown subsections, which will populate the helpful navbar in Jupyter Lab and here on the Jupyter Book!\n\n# some subsection code\na = [1, 2, 3, 4, 5]\n[i + 2 for i in a]\n\n","type":"content","url":"/notebooks/foundations#a-content-subsection","position":11},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl3":"Another content subsection","lvl2":"Your first content section"},"type":"lvl3","url":"/notebooks/foundations#another-content-subsection","position":12},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl3":"Another content subsection","lvl2":"Your first content section"},"content":"Keep up the good work! A note, try to avoid using code comments as narrative, and instead let them only exist as brief clarifications where necessary.\n\n","type":"content","url":"/notebooks/foundations#another-content-subsection","position":13},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl2":"Your second content section"},"type":"lvl2","url":"/notebooks/foundations#your-second-content-section","position":14},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl2":"Your second content section"},"content":"Here we can move on to our second objective, and we can demonstrate...\n\n","type":"content","url":"/notebooks/foundations#your-second-content-section","position":15},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl3":"A subsection to the second section","lvl2":"Your second content section"},"type":"lvl3","url":"/notebooks/foundations#a-subsection-to-the-second-section","position":16},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl3":"A subsection to the second section","lvl2":"Your second content section"},"content":"","type":"content","url":"/notebooks/foundations#a-subsection-to-the-second-section","position":17},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl4":"a quick demonstration","lvl3":"A subsection to the second section","lvl2":"Your second content section"},"type":"lvl4","url":"/notebooks/foundations#a-quick-demonstration","position":18},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl4":"a quick demonstration","lvl3":"A subsection to the second section","lvl2":"Your second content section"},"content":"","type":"content","url":"/notebooks/foundations#a-quick-demonstration","position":19},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl5":"of further and further","lvl4":"a quick demonstration","lvl3":"A subsection to the second section","lvl2":"Your second content section"},"type":"lvl5","url":"/notebooks/foundations#of-further-and-further","position":20},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl5":"of further and further","lvl4":"a quick demonstration","lvl3":"A subsection to the second section","lvl2":"Your second content section"},"content":"","type":"content","url":"/notebooks/foundations#of-further-and-further","position":21},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl6":"header levels","lvl5":"of further and further","lvl4":"a quick demonstration","lvl3":"A subsection to the second section","lvl2":"Your second content section"},"type":"lvl6","url":"/notebooks/foundations#header-levels","position":22},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl6":"header levels","lvl5":"of further and further","lvl4":"a quick demonstration","lvl3":"A subsection to the second section","lvl2":"Your second content section"},"content":"\n\nas well as m = a * t / h text! Similarly, you have access to other \\LaTeX equation \n\nfunctionality via MathJax:\\begin{align}\n\\dot{x} & = \\sigma(y-x) \\\\\n\\dot{y} & = \\rho x - y - xz \\\\\n\\dot{z} & = -\\beta z + xy\n\\end{align}\n\nCheck out \n\nany number of helpful Markdown resources for further customizing your notebooks and the \n\nMyST Syntax Overview for MyST-specific formatting information. Don’t hesitate to ask questions if you have problems getting it to look just right.\n\n","type":"content","url":"/notebooks/foundations#header-levels","position":23},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl2":"Last Section"},"type":"lvl2","url":"/notebooks/foundations#last-section","position":24},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl2":"Last Section"},"content":"You can add \n\nadmonitions using MyST syntax:\n\nNote\n\nYour relevant information here!\n\nSome other admonitions you can put in (\n\nthere are 10 total):\n\nHint\n\nA helpful hint.\n\nWarning\n\nBe careful!\n\nDanger\n\nScary stuff be here.\n\nWe also suggest checking out Jupyter Book’s \n\nbrief demonstration on adding cell tags to your cells in Jupyter Notebook, Lab, or manually. Using these cell tags can allow you to \n\ncustomize how your code content is displayed and even \n\ndemonstrate errors without altogether crashing our loyal army of machines!\n\n\n\n","type":"content","url":"/notebooks/foundations#last-section","position":25},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/foundations#summary","position":26},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl2":"Summary"},"content":"Add one final --- marking the end of your body of content, and then conclude with a brief single paragraph summarizing at a high level the key pieces that were learned and how they tied to your objectives. Look to reiterate what the most important takeaways were.","type":"content","url":"/notebooks/foundations#summary","position":27},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/foundations#whats-next","position":28},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl3":"What’s next?","lvl2":"Summary"},"content":"Let Jupyter book tie this to the next (sequential) piece of content that people could move on to down below and in the sidebar. However, if this page uniquely enables your reader to tackle other nonsequential concepts throughout this book, or even external content, link to it here!\n\n","type":"content","url":"/notebooks/foundations#whats-next","position":29},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/foundations#resources-and-references","position":30},{"hierarchy":{"lvl1":"Project Pythia - Sonar AI ... Integrating sonar and buoy-based temperature data","lvl2":"Resources and references"},"content":"Finally, be rigorous in your citations and references as necessary. Give credit where credit is due. Also, feel free to link to relevant external material, further reading, documentation, etc. Then you’re done! Give yourself a quick review, a high five, and send us a pull request. A few final notes:\n\nKernel > Restart Kernel and Run All Cells... to confirm that your notebook will cleanly run from start to finish\n\nKernel > Restart Kernel and Clear All Outputs... before committing your notebook, our machines will do the heavy lifting\n\nTake credit! Provide author contact information if you’d like; if so, consider adding information here at the bottom of your notebook\n\nGive credit! Attribute appropriate authorship for referenced code, information, images, etc.\n\nOnly include what you’re legally allowed: no copyright infringement or plagiarism\n\nThank you for your contribution!","type":"content","url":"/notebooks/foundations#resources-and-references","position":31},{"hierarchy":{"lvl1":"Use the GeoJSON file to get the Area of Interest and query the STAC catalog for items"},"type":"lvl1","url":"/notebooks/pycogss-spectral-change","position":0},{"hierarchy":{"lvl1":"Use the GeoJSON file to get the Area of Interest and query the STAC catalog for items"},"content":"# Imports\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nimport re\nfrom pelicanfs.core import PelicanFileSystem, PelicanMap,OSDFFileSystem \nimport geopandas as gpd\nfrom pystac_client import Client\nfrom rasterio.mask import mask\nimport rasterio\nfrom shapely.geometry import box\nfrom urllib.parse import urlparse\n\n","type":"content","url":"/notebooks/pycogss-spectral-change","position":1},{"hierarchy":{"lvl1":"Use the GeoJSON file to get the Area of Interest and query the STAC catalog for items"},"type":"lvl1","url":"/notebooks/pycogss-spectral-change#use-the-geojson-file-to-get-the-area-of-interest-and-query-the-stac-catalog-for-items","position":2},{"hierarchy":{"lvl1":"Use the GeoJSON file to get the Area of Interest and query the STAC catalog for items"},"content":"\n\n# Load the GeoJSON file\ngeojson_path = '3100180240.geojson' \ngdf = gpd.read_file(geojson_path)\n\n# Display the loaded GeoDataFrame\nprint(gdf)\n\n# Extract AOI geometry\naoi_geometry = gdf.geometry.iloc[0]\naoi_bounds   = aoi_geometry.bounds  # (minx, miny, maxx, maxy)\n\n# Get AOI centroid for visualization\ncentroid  = aoi_geometry.centroid\nlong, lat = centroid.x, centroid.y\n\n# Print the bounding box to verify\nprint(\"Bounding Box:\", aoi_bounds)\n\n# Connect to the Earth Search STAC API (Sentinel-2 Level-2A COGs are available here)\ncatalog_url = \"https://earth-search.aws.element84.com/v1\"\ncatalog     = Client.open(catalog_url)\n\n# Define the date range as strings\nstart_date      = \"2019-01\"\nend_date        = \"2023-02\"\n\n# Define cloud cover threshold\ncloud_cover_max = 0.05  # 5% cloud cover threshold\n#cloud_cover_max = 0.20  # 20% cloud cover threshold\n\n# Perform the search\nsearch = catalog.search(\n                 collections=[\"sentinel-2-l2a\"],\n                 bbox=aoi_bounds,\n                 datetime=f\"{start_date}/{end_date}\",\n                 #datetime=\"2022-06-01/2022-09-30\",\n                 query={\"eo:cloud_cover\": {\"lt\": cloud_cover_max * 100}}\n                )\n\n# Get all matching items\nitems = list(search.items())\nprint(f\"Found {len(items)} matching items.\")\n\n# Confirm that the matching items are hosted in a aws-us-region by printing out the href links for the first n items\nn=1\n\nfor i, item in enumerate(items[:n]):\n    print(f\"\\nItem {i+1}: {item.id}\")\n    for asset_key, asset in item.assets.items():\n        print(f\"  {asset_key}: {asset.href}\")\n\nfrom shapely.geometry import mapping\n\n# Reproject AOI to match raster CRS\nfrom pyproj import CRS\nfrom geopandas import GeoSeries\n\ndef returnOSDFPath(url):\n    \"\"\"\n    Converts a URL to an OSDF path.\n\n    Parameters:\n    - url: URL to convert.\n\n    Returns:\n    - OSDF path.\n    \"\"\"\n    # Parse the URL\n    parsed_url = urlparse(url)\n    \n    # Construct the OSDF path\n    osdf_path = f\"/aws-opendata/us-west-2/sentinel-cogs{parsed_url.path}\"\n    \n    return osdf_path\n\n\n","type":"content","url":"/notebooks/pycogss-spectral-change#use-the-geojson-file-to-get-the-area-of-interest-and-query-the-stac-catalog-for-items","position":3},{"hierarchy":{"lvl1":"Apply various masks and calculate NDVI"},"type":"lvl1","url":"/notebooks/pycogss-spectral-change#apply-various-masks-and-calculate-ndvi","position":4},{"hierarchy":{"lvl1":"Apply various masks and calculate NDVI"},"content":"\n\ndef clp(image_src, aoi_geometry):\n    \"\"\"\n    Clip a raster image to the Area of Interest (AOI).\n\n    Parameters:\n    - image_src: Open rasterio dataset.\n    - aoi_geometry: AOI geometry as a GeoJSON-like object.\n\n    Returns:\n    - Clipped image array and updated metadata.\n    \"\"\"\n    # out_image, out_transform = mask(image_src, [aoi_geometry], crop=True)\n    out_image, out_transform = mask(image_src, aoi_geometry, crop=True)\n    out_meta = image_src.meta.copy()\n    out_meta.update({\n        \"driver\": \"GTiff\",\n        \"height\": out_image.shape[1],\n        \"width\": out_image.shape[2],\n        \"transform\": out_transform\n    })\n    return out_image, out_meta\n\ndef maskWater(image, water_mask):\n    \"\"\"\n    Masks out water pixels using the MODIS water mask.\n\n    Parameters:\n    - image: Raster image to mask.\n    - water_mask: Water mask raster.\n\n    Returns:\n    - Water-masked image.\n    \"\"\"\n    water = water_mask.read(1)  # Read the water mask\n    mask = water < 1  # Mask water pixels (water < 1)\n    image_masked = np.where(mask, image, np.nan)\n    return image_masked\n\ndef maskS2snow(image, snow_prob):\n    \"\"\"\n    Masks snow pixels using the MSK_SNWPRB (Snow Probability Mask).\n\n    Parameters:\n    - image: Raster image to mask.\n    - snow_prob: Snow probability raster.\n\n    Returns:\n    - Snow-masked image.\n    \"\"\"\n    snow = snow_prob.read(1)  # Read the snow probability mask\n    mask = snow < 0.009  # Mask snow pixels (snow probability < 0.9%)\n    image_masked = np.where(mask, image, np.nan)\n    return image_masked\n\ndef maskWhite(image, b2, b3, b4):\n    \"\"\"\n    Masks white pixels by converting RGB to grayscale and applying a threshold.\n\n    Parameters:\n    - image: Raster image to mask.\n    - b2, b3, b4: Blue, Green, and Red bands respectively.\n\n    Returns:\n    - Grayscale-masked image.\n    \"\"\"\n    # Convert RGB to grayscale\n    grayscale = (0.3 * b4.read(1) + 0.59 * b3.read(1) + 0.11 * b2.read(1)) * 1e4\n    mask = grayscale <= 2000  # Mask white pixels (grayscale > 2000)\n    image_masked = np.where(mask, image, np.nan)\n    return image_masked\n\n%%time\n# # Using pelicanfs\n# Loop through each item in the STAC query\nfor idx, item in enumerate(items, start=1):\n    print(f\"Processing dataset #{idx}\")\n    if idx == 6: # Otherwise \n        break\n\n    # Check required assets\n    required_assets = [\"red\", \"green\", \"blue\", \"scl\"]\n    if not all(asset in item.assets for asset in required_assets):\n        print(f\"Skipping dataset #{idx}: Missing required assets.\")\n        continue\n\n    # Get asset URLs\n    red_url = item.assets[\"red\"].href\n    green_url = item.assets[\"green\"].href\n    blue_url = item.assets[\"blue\"].href\n    scl_url = item.assets[\"scl\"].href\n\n    pel_red_url   = returnOSDFPath(red_url)\n    pel_green_url = returnOSDFPath(green_url)\n    pel_blue_url  = returnOSDFPath(blue_url)\n    pel_scl_url   = returnOSDFPath(scl_url)\n\n    pelfs = PelicanFileSystem(\"pelican://osg-htc.org\")\n\n    # Reproject AOI to match raster CRS\n    with rasterio.open(pel_red_url,opener=pelfs) as src:\n        raster_crs = CRS(src.crs)\n    aoi_geometry_reprojected = GeoSeries(aoi_geometry).set_crs(gdf.crs).to_crs(raster_crs)\n\n    # Reproject AOI to match raster CRS\n    with rasterio.open(pel_red_url, opener=pelfs) as red_src, \\\n         rasterio.open(pel_green_url, opener=pelfs) as green_src, \\\n         rasterio.open(pel_blue_url, opener=pelfs) as blue_src, \\\n         rasterio.open(pel_scl_url, opener=pelfs) as scl_src:\n\n        aoi_geometry_reprojected = GeoSeries(aoi_geometry).set_crs(gdf.crs).to_crs(raster_crs)\n        raster_bounds = box(*red_src.bounds)\n\n        if not aoi_geometry_reprojected[0].intersects(raster_bounds):\n            print(f\"Warning: AOI does not intersect the bounds of {red_url}. Skipping.\")\n            continue\n\n        # Clip all RGB and SCL bands to AOI\n        red_clipped, red_meta = clp(red_src, aoi_geometry_reprojected)\n        green_clipped, _      = clp(green_src, aoi_geometry_reprojected)\n        blue_clipped, _       = clp(blue_src, aoi_geometry_reprojected)\n        scl_clipped, _        = clp(scl_src, aoi_geometry_reprojected)\n\n        print(f\"Dataset #{idx} processed. Ready for visualization or further analysis.\")\n\nfrom sklearn.preprocessing import MinMaxScaler\n# Function to scale image bands properly\ndef scale_band(band):\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    return scaler.fit_transform(band)  # Normalize between 0-1\n\n# Assuming red_clipped, green_clipped, and blue_clipped are 2D NumPy arrays\nrgb = np.dstack((scale_band(red_clipped.squeeze()),\n    scale_band(green_clipped.squeeze()),\n    scale_band(blue_clipped.squeeze()),\n))\n\n# Plot\nplt.figure(figsize=(10, 8))\nplt.imshow(rgb)\nplt.axis(\"off\")\nplt.show()","type":"content","url":"/notebooks/pycogss-spectral-change#apply-various-masks-and-calculate-ndvi","position":5},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook"},"type":"lvl1","url":"/notebooks/atmosphere-llc2160-visualization","position":0},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook"},"content":"\n\nThe DYnamics of the Atmospheric general circulation Modeled On Non-hydrostatic Domains (DYAMOND) data provides high resolution ocean circulation models, offering unprecedented detail. This dataset comprises a C1440 configuration of the Goddard Earth Observing System (GEOS) atmospheric model, with 7-km horizontal grid spacing and 72 vertical layers, coupled to a LLC2160 configuration of the Massachusetts Institute of Technology general circulation model (MITgcm) with 2–4-km grid spacing and 90 vertical levels. The C1440-LLC2160 simulation has been integrated for 14 months, starting from prescribed initial conditions on January 20, 2020.\n\nThis notebook is the second part of the DYAMOND LLC2160 Ocean Dataset Cookbook.\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization","position":1},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Overview"},"type":"lvl3","url":"/notebooks/atmosphere-llc2160-visualization#overview","position":2},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Overview"},"content":"This notebook demonstrates how to access and visualize high-resolution atmospheric data from the DYAMOND dataset using OpenVisus. The data is hosted in OSDF and served using Pelican Platform and OpenVisus. You’ll learn how to read metadata from the cloud, interactively select variables, and explore regional and depth-based slices of the data.\n\nRead the metadata file from cloud\n\nData Subset\n\nVisualize the data\n\nExplore multi-resolution data for a specific region and depth\n\nBy the end of this notebook, you will understand how to:\n\nStream and query oceanographic data using PelicanFS\n\nUse metadata to inform data exploration\n\nVisualize regional and depth-specific ocean data using Panel and Bokeh\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#overview","position":3},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Prerequisites"},"type":"lvl3","url":"/notebooks/atmosphere-llc2160-visualization#prerequisites","position":4},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Prerequisites"},"content":"This section was inspired by \n\nthis template of the wonderful \n\nThe Turing Way Jupyter Book.\n\nConcepts\n\nImportance\n\nNotes\n\nOpenVisus\n\nHelpful\n\nRequired for multiresolution data access and streaming\n\nOceanographic data formats and interpretation\n\nHelpful\n\nUnderstanding of gridded climate/ocean data such as LLC2160\n\nPelicanFS\n\nHelpful\n\nUsed for high-throughput data access from cloud storage\n\nTime to learn: 30 minutes\n\nSystem requirements:\n\nPython packages: panel, bokeh, xmltodict, colorcet, boto3, basemap, pelicanfs, OpenVisus, openvisuspy\n\nRecommended: Python ≥ 3.8, internet access for cloud-hosted data\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#prerequisites","position":5},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl2":"Step 1: Importing the libraries"},"type":"lvl2","url":"/notebooks/atmosphere-llc2160-visualization#step-1-importing-the-libraries","position":6},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl2":"Step 1: Importing the libraries"},"content":"\n\nimport numpy as np\nimport openvisuspy as ovp\nimport matplotlib.pyplot as plt\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#step-1-importing-the-libraries","position":7},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl4":"The section below shows different LLC2160 fields we have available in cloud. Each field is >200TB.","lvl2":"Step 1: Importing the libraries"},"type":"lvl4","url":"/notebooks/atmosphere-llc2160-visualization#the-section-below-shows-different-llc2160-fields-we-have-available-in-cloud-each-field-is-200tb","position":8},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl4":"The section below shows different LLC2160 fields we have available in cloud. Each field is >200TB.","lvl2":"Step 1: Importing the libraries"},"content":"\n\nvariable = 'u'\nface=0 # 6 variables are available\ntimestep=1 # There are 10000 timesteps available\n\nbase_url= \"pelican://osg-htc.org/nasa/nsdf/climate3/dyamond/GEOS/\"\nvar_dir=f\"GEOS_{variable.upper()}/{variable.lower()}_face_{face}_depth_52_time_0_10269.idx\"\nvar_url=base_url+var_dir\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#the-section-below-shows-different-llc2160-fields-we-have-available-in-cloud-each-field-is-200tb","position":9},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Step 2: Reading the metadata file from cloud","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/atmosphere-llc2160-visualization#step-2-reading-the-metadata-file-from-cloud","position":10},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Step 2: Reading the metadata file from cloud","lvl2":"Step 1: Importing the libraries"},"content":"In this section, you can select any variables that you can declared in the cells above and replace it inside LoadDataset. We are just reading the metadata for the dataset here.\n\ndb=ovp.LoadDataset(var_url)\nprint(f'Dimensions: {db.getLogicBox()[1][0]}*{db.getLogicBox()[1][1]}*{db.getLogicBox()[1][2]}')\nprint(f'Total Timesteps: {len(db.getTimesteps())}')\nprint(f'Field: {db.getField().name}')\nprint('Data Type: float32')\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#step-2-reading-the-metadata-file-from-cloud","position":11},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Step 3:  Data Selection","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/atmosphere-llc2160-visualization#step-3-data-selection","position":12},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Step 3:  Data Selection","lvl2":"Step 1: Importing the libraries"},"content":"This section shows you how to load the data you want. You can select any timestep, region (x,y,z) you want. You can set the quality or resolution of the data as well. Higher quality means the finer(more) data. Not setting any time means first timestep available. Not setting quality means full data which takes a while to load because of the higher filesize.  Since each timestep is >30GB, I am only selecting 1 level out of 90.\n\ndata=db.db.read(time=0,z=[0,1],quality=-4) #Since each timestep is >30GB, I am only selecting 1 level out of 90.\ndata.shape\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#step-3-data-selection","position":13},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/atmosphere-llc2160-visualization#step-4-visualize-the-data","position":14},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"content":"We are using a simple matplotlib here, but since the data is in numpy array, it can loaded with any python modules that support numpy. Feel free to set the vmin,vmax appropriately.\n\nfig,axes=plt.subplots(1,1,figsize=(12,8))\nim= axes.imshow(data[0,:,:], aspect='auto',origin='lower',cmap='coolwarm')\ncbar = plt.colorbar(im, ax=axes)\ncbar.set_label('Temperature (deg. C)')\nplt.show()\n\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#step-4-visualize-the-data","position":15},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"type":"lvl4","url":"/notebooks/atmosphere-llc2160-visualization#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":16},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"content":"Aashish Panta (\n\naashishpanta0@gmail​.com)\n\nGiorgio Scorzelli (\n\nscrgiorgio@gmail​.com)\n\nValerio Pascucci (\n\npascucci​.valerio@gmail​.com)","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":17},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS"},"type":"lvl1","url":"/notebooks/introduction-to-nsdf-openvisus","position":0},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS"},"content":"\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus","position":1},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl2","url":"/notebooks/introduction-to-nsdf-openvisus#openvisus-high-performance-big-data-analysis-and-visualization","position":2},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#openvisus-high-performance-big-data-analysis-and-visualization","position":3},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Overview","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#overview","position":4},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Overview","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"OpenViSUS is an open-source framework designed for efficient management, analysis, and visualization of large-scale scientific datasets. It enables interactive exploration of petabyte-scale data on a wide range of devices, from supercomputers to commodity laptops, making big data accessible to all users.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#overview","position":5},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Key Features","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#key-features","position":6},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Key Features","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Efficient Data Storage: Uses the IDX format, which stores data in a hierarchical Z (HZ) order for cache-oblivious, progressive access.\n\nScalable Visualization: Enables interactive visualization of terabyte and petabyte datasets without requiring high-end hardware.\n\nProgressive Streaming: Optimizes network utilization with state-of-the-art compression algorithms for fast data delivery and streaming.\n\nWeb-Based Dashboards: Provides customizable dashboards for data analysis accessible from any device with an internet connection.\n\nOpen Source: Distributed under the permissive BSD license.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#key-features","position":7},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Data Storage: The IDX Format","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#data-storage-the-idx-format","position":8},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Data Storage: The IDX Format","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Hierarchical Z (HZ) Order: Data is organized to allow efficient, multi-resolution access and visualization.\n\nProgressive Access: Users can interactively explore data at different resolutions, starting with coarse overviews and refining to full detail as needed.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#data-storage-the-idx-format","position":9},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Data Delivery","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#data-delivery","position":10},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Data Delivery","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Interactive Streaming: Share and stream large datasets using simple server modules (e.g., Apache), enabling teravoxel imagery delivery.\n\nCloud and Local Access: Data can be accessed from local storage or cloud repositories, with optimized streaming for remote analysis.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#data-delivery","position":11},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Interactive Analysis and Visualization","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#interactive-analysis-and-visualization","position":12},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Interactive Analysis and Visualization","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Cross-Platform: Works on supercomputers, desktops, and laptops.\n\nScripting Support: Experiment with interactive scripting for rapid data insights.\n\nUser-Friendly Querying: Abstracts complexities of file systems and cloud services, allowing scientists to focus on analysis.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#interactive-analysis-and-visualization","position":13},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"2026 IEEE SciVis  Contest","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#id-2026-ieee-scivis-contest","position":14},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"2026 IEEE SciVis  Contest","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"NSDF-OpenVISUS is directly supporting the 2026 IEEE SciVis Contest. This contest is held annually as part of the IEEE VIS Conference. In 2026, this contest will focus on the visualization of petascale oceanic and atmospheric climate data provided by NASA. This year’s challenge emphasizes advanced visualization methods for exploring vast climate datasets, encouraging innovative solutions that address real-world issues such as climate prediction, weather simulation, and environmental impact analysis.\n\nThe best submission wins $1000 cash prize. Find more information on the official \n\nSciVis website.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#id-2026-ieee-scivis-contest","position":15},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Example Use Cases","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#example-use-cases","position":16},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Example Use Cases","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"NASA LLC2160 Data Interactive Dashboard\n\nNEX-GDDP-CMIP6 Dashboard\n\nClassroom deployment for minority-serving institutions\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#example-use-cases","position":17},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Other Deplyments","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#other-deplyments","position":18},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Other Deplyments","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Cornell High Energy Synchrotron Source (CHESS)\n\nNational Center for Atmospheric Research (NCAR)\n\nNEON\n\nSOMOSPIE\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#other-deplyments","position":19},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Getting Started","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#getting-started","position":20},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Getting Started","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#getting-started","position":21},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl4":"Installation","lvl3":"Getting Started","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl4","url":"/notebooks/introduction-to-nsdf-openvisus#installation","position":22},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl4":"Installation","lvl3":"Getting Started","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Install Using pip\n\npip install OpenVisus\n\nBuild from Source:\n\nClone the repository: git clone https://github.com/sci-visus/OpenVisus.git\n\nFollow instructions in the README.md and docs/compilation.md for building on your platform.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#installation","position":23},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"References","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#references","position":24},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"References","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"National Science Data Fabric\n\nOpenVisus\n\nOpenVisuspy\n\nPlease consult these papers for technical details and use cases:\n\nWeb-based Visualization and Analytics of Petascale data: Equity as a Tide that Lifts All Boats\n\nInteractive Visualization of Terascale Data in the Browser: Fact or Fiction?\n\nFast Multiresolution Reads of Massive Simulation Datasets\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#references","position":25},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl3":"References","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl4","url":"/notebooks/introduction-to-nsdf-openvisus#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":26},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl3":"References","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Aashish Panta (\n\naashishpanta0@gmail​.com)\n\nGiorgio Scorzelli (\n\nscrgiorgio@gmail​.com)\n\nValerio Pascucci (\n\npascucci​.valerio@gmail​.com)\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":27},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook"},"type":"lvl1","url":"/notebooks/ocean-llc2160-visualization","position":0},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook"},"content":"\n\nThe DYnamics of the Atmospheric general circulation Modeled On Non-hydrostatic Domains (DYAMOND) data provides high resolution ocean circulation models, offering unprecedented detail. This dataset comprises a C1440 configuration of the Goddard Earth Observing System (GEOS) atmospheric model, with 7-km horizontal grid spacing and 72 vertical layers, coupled to a LLC2160 configuration of the Massachusetts Institute of Technology general circulation model (MITgcm) with 2–4-km grid spacing and 90 vertical levels. The C1440-LLC2160 simulation has been integrated for 14 months, starting from prescribed initial conditions on January 20, 2020.\n\n\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization","position":1},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Overview"},"type":"lvl3","url":"/notebooks/ocean-llc2160-visualization#overview","position":2},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Overview"},"content":"This notebook demonstrates how to access and visualize high-resolution ocean data from the LLC2160 dataset using OpenVisus. The data is hosted in OSDF and served using Pelican Platform and OpenVisus. You’ll learn how to read metadata from the cloud, interactively select variables, and explore regional and depth-based slices of the data.\n\nRead the metadata file from cloud\n\nData Subset\n\nVisualize the data\n\nExplore multi-resolution data for a specific region and depth\n\nBy the end of this notebook, you will understand how to:\n\nStream and query oceanographic data using PelicanFS\n\nUse metadata to inform data exploration\n\nVisualize regional and depth-specific ocean data using Panel and Bokeh\n\n\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#overview","position":3},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Prerequisites"},"type":"lvl3","url":"/notebooks/ocean-llc2160-visualization#prerequisites","position":4},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Prerequisites"},"content":"This section was inspired by \n\nthis template of the wonderful \n\nThe Turing Way Jupyter Book.\n\nConcepts\n\nImportance\n\nNotes\n\nOpenVisus\n\nHelpful\n\nRequired for multiresolution data access and streaming\n\nOceanographic data formats and interpretation\n\nHelpful\n\nUnderstanding of gridded climate/ocean data such as LLC2160\n\nPelicanFS\n\nHelpful\n\nUsed for high-throughput data access from cloud storage\n\nTime to learn: 30 minutes\n\nSystem requirements:\n\nPython packages: panel, bokeh, xmltodict, colorcet, boto3, basemap, pelicanfs, OpenVisus, openvisuspy\n\nRecommended: Python ≥ 3.8, internet access for cloud-hosted data\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#prerequisites","position":5},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 1: Importing the libraries"},"type":"lvl2","url":"/notebooks/ocean-llc2160-visualization#step-1-importing-the-libraries","position":6},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 1: Importing the libraries"},"content":"\n\nimport numpy as np\nimport openvisuspy as ovp\nimport matplotlib.pyplot as plt\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-1-importing-the-libraries","position":7},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl4":"The section below shows different LLC2160 fields we have available in cloud. Each field is >200TB.","lvl2":"Step 1: Importing the libraries"},"type":"lvl4","url":"/notebooks/ocean-llc2160-visualization#the-section-below-shows-different-llc2160-fields-we-have-available-in-cloud-each-field-is-200tb","position":8},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl4":"The section below shows different LLC2160 fields we have available in cloud. Each field is >200TB.","lvl2":"Step 1: Importing the libraries"},"content":"\n\nvariable='salt' # options are: u,v,w,salt,theta\n\n\nbase_url= \"pelican://osg-htc.org/nasa/nsdf/climate3/dyamond/\"\nif variable==\"theta\" or variable==\"w\":\n    base_dir=f\"mit_output/llc2160_{variable}/llc2160_{variable}.idx\"\nelif variable==\"u\":\n    base_dir= \"mit_output/llc2160_arco/visus.idx\"\nelse:\n    base_dir=f\"mit_output/llc2160_{variable}/{variable}_llc2160_x_y_depth.idx\"\nvar_url=base_url+base_dir\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#the-section-below-shows-different-llc2160-fields-we-have-available-in-cloud-each-field-is-200tb","position":9},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Step 2: Reading the metadata file from cloud","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc2160-visualization#step-2-reading-the-metadata-file-from-cloud","position":10},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Step 2: Reading the metadata file from cloud","lvl2":"Step 1: Importing the libraries"},"content":"In this section, you can select any variables that you can declared in the cells above and replace it inside LoadDataset. We are just reading the metadata for the dataset here.\n\ndb=ovp.LoadDataset(var_url)\nprint(f'Dimensions: {db.getLogicBox()[1][0]}*{db.getLogicBox()[1][1]}*{db.getLogicBox()[1][2]}')\nprint(f'Total Timesteps: {len(db.getTimesteps())}')\nprint(f'Field: {db.getField().name}')\nprint('Data Type: float32')\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-2-reading-the-metadata-file-from-cloud","position":11},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Step 3:  Data Selection","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc2160-visualization#step-3-data-selection","position":12},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Step 3:  Data Selection","lvl2":"Step 1: Importing the libraries"},"content":"This section shows you how to load the data you want. You can select any timestep, region (x,y,z) you want. You can set the quality or resolution of the data as well. Higher quality means the finer(more) data. Not setting any time means first timestep available. Not setting quality means full data which takes a while to load because of the higher filesize.  Since each timestep is >30GB, I am only selecting 1 level out of 90.\n\ndata=db.db.read(time=0,z=[0,1],quality=-4) #Since each timestep is >30GB, I am only selecting 1 level out of 90.\ndata.shape\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-3-data-selection","position":13},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc2160-visualization#step-4-visualize-the-data","position":14},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"content":"We are using a simple matplotlib here, but since the data is in numpy array, it can loaded with any python modules that support numpy. Feel free to set the vmin,vmax appropriately.\n\nfig,axes=plt.subplots(1,1,figsize=(12,8))\nim= axes.imshow(data[0,:,:], aspect='auto',origin='lower',cmap='turbo')\ncbar = plt.colorbar(im, ax=axes)\ncbar.set_label('Temperature (deg. C)')\nplt.show()\n\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-4-visualize-the-data","position":15},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"But, what if you want to see the full data for a certain region at a certain depth?","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc2160-visualization#but-what-if-you-want-to-see-the-full-data-for-a-certain-region-at-a-certain-depth","position":16},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"But, what if you want to see the full data for a certain region at a certain depth?","lvl2":"Step 1: Importing the libraries"},"content":"Just set the right x,y,z while reading the data. x and y are the bounding box, z is the depth/layer.\n\ndata1=db.db.read(time=1,z=[0,1],quality=-6,x=[500,2500],y=[2500,5000])\nplt.imshow(data1[0,:,:], origin='lower',cmap='turbo')\nplt.colorbar()\n\ndata1.shape #\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#but-what-if-you-want-to-see-the-full-data-for-a-certain-region-at-a-certain-depth","position":17},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 5: Save the data for the region locally"},"type":"lvl2","url":"/notebooks/ocean-llc2160-visualization#step-5-save-the-data-for-the-region-locally","position":18},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 5: Save the data for the region locally"},"content":"You can save the data locally as you want. For example, here we are only saving the region shown above as a numpy array.\n\nnp.save('test_region2.npy', data1)\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-5-save-the-data-for-the-region-locally","position":19},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 6: Load the locally saved region and visualize using matplotlib"},"type":"lvl2","url":"/notebooks/ocean-llc2160-visualization#step-6-load-the-locally-saved-region-and-visualize-using-matplotlib","position":20},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 6: Load the locally saved region and visualize using matplotlib"},"content":"\n\nlocal_data=np.load('test_region2.npy')\nplt.imshow(local_data[0,:,:], origin='lower',cmap='turbo')\nplt.colorbar()\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-6-load-the-locally-saved-region-and-visualize-using-matplotlib","position":21},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 7: Horizontal Slicing"},"type":"lvl2","url":"/notebooks/ocean-llc2160-visualization#step-7-horizontal-slicing","position":22},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 7: Horizontal Slicing"},"content":"\n\ndata1=db.db.read(time=1,x=[500,2500],y=[5100,5101])\ndata1.shape\n\n\nplt.figure(figsize=(14,8))\nplt.imshow(data1[:,0,:],cmap='turbo')\n# plt.colorbar()\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-7-horizontal-slicing","position":23},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl2":"Step 7: Horizontal Slicing"},"type":"lvl4","url":"/notebooks/ocean-llc2160-visualization#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":24},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl2":"Step 7: Horizontal Slicing"},"content":"Aashish Panta (\n\naashishpanta0@gmail​.com)\n\nGiorgio Scorzelli (\n\nscrgiorgio@gmail​.com)\n\nValerio Pascucci (\n\npascucci​.valerio@gmail​.com)","type":"content","url":"/notebooks/ocean-llc2160-visualization#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":25},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook"},"type":"lvl1","url":"/notebooks/ocean-llc4320-visualization","position":0},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook"},"content":"\n\nLLC4320, a product of \n\nEstimating the Circulation and Climate of the Ocean (ECCO) project, is the product of a 14-month simulation of ocean circulation and dynamics using MITgcm model. This simulation is similar to the ocean portion of the DYAMOND coupled simulation but was run with half the horizontal grid spacing (4\\times the cell count) and with ocean surface boundary values derived from observations and physical models. The model output has five 3D and thirteen 2D fields, including temperature, salinity, three velocity components, sea ice, and radiation. This massive dataset is 2.8 PB.\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization","position":1},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Overview"},"type":"lvl3","url":"/notebooks/ocean-llc4320-visualization#overview","position":2},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Overview"},"content":"This notebook demonstrates how to access and visualize high-resolution ocean data from the LLC4320 dataset using OpenVisus. The data is hosted in OSDF and served using Pelican Platform and OpenVisus. You’ll learn how to read metadata from the cloud, interactively select variables, and explore regional and depth-based slices of the data.\n\nRead the metadata file from cloud\n\nData Subset\n\nVisualize the data\n\nExplore multi-resolution data for a specific region and depth\n\nBy the end of this notebook, you will understand how to:\n\nStream and query oceanographic data using PelicanFS\n\nUse metadata to inform data exploration\n\nVisualize regional and depth-specific ocean data using Panel and Bokeh\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#overview","position":3},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Prerequisites"},"type":"lvl3","url":"/notebooks/ocean-llc4320-visualization#prerequisites","position":4},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Prerequisites"},"content":"This section was inspired by \n\nthis template of the wonderful \n\nThe Turing Way Jupyter Book.\n\nConcepts\n\nImportance\n\nNotes\n\nOpenVisus\n\nHelpful\n\nRequired for multiresolution data access and streaming\n\nOceanographic data formats and interpretation\n\nHelpful\n\nUnderstanding of gridded climate/ocean data such as LLC2160\n\nPelicanFS\n\nHelpful\n\nUsed for high-throughput data access from cloud storage\n\nTime to learn: 30 minutes\n\nSystem requirements:\n\nPython packages: panel, bokeh, xmltodict, colorcet, boto3, basemap, pelicanfs, OpenVisus, openvisuspy\n\nRecommended: Python ≥ 3.8, internet access for cloud-hosted data\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#prerequisites","position":5},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 1: Importing the libraries"},"type":"lvl2","url":"/notebooks/ocean-llc4320-visualization#step-1-importing-the-libraries","position":6},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 1: Importing the libraries"},"content":"\n\nimport numpy as np\nimport openvisuspy as ovp\nimport matplotlib.pyplot as plt\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-1-importing-the-libraries","position":7},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl4":"The section below shows different LLC4320 fields we have available in cloud. Each field is >400TB.","lvl2":"Step 1: Importing the libraries"},"type":"lvl4","url":"/notebooks/ocean-llc4320-visualization#the-section-below-shows-different-llc4320-fields-we-have-available-in-cloud-each-field-is-400tb","position":8},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl4":"The section below shows different LLC4320 fields we have available in cloud. Each field is >400TB.","lvl2":"Step 1: Importing the libraries"},"content":"\n\ntemperature=\"pelican://osg-htc.org/nasa/nsdf/climate1/llc4320/idx/theta/theta_llc4320_x_y_depth.idx\"\n\nsalinity=\"pelican://osg-htc.org/nasa/nsdf/climate1/llc4320/idx/salt/salt_llc4320_x_y_depth.idx\"\n\nvertical_velocity=\"pelican://osg-htc.org/nasa/nsdf/climate2/llc4320/idx/w/w_llc4320_x_y_depth.idx\"\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#the-section-below-shows-different-llc4320-fields-we-have-available-in-cloud-each-field-is-400tb","position":9},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Step 2: Reading the metadata file from cloud","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc4320-visualization#step-2-reading-the-metadata-file-from-cloud","position":10},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Step 2: Reading the metadata file from cloud","lvl2":"Step 1: Importing the libraries"},"content":"In this section, you can select any variables that you can declared in the cells above and replace it inside LoadDataset. We are just reading the metadata for the dataset here.\n\ndb=ovp.LoadDataset(temperature)\nprint(f'Dimensions: {db.getLogicBox()[1][0]}*{db.getLogicBox()[1][1]}*{db.getLogicBox()[1][2]}')\nprint(f'Total Timesteps: {len(db.getTimesteps())}')\nprint(f'Field: {db.getField().name}')\nprint('Data Type: float32')\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-2-reading-the-metadata-file-from-cloud","position":11},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Step 3:  Data Selection","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc4320-visualization#step-3-data-selection","position":12},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Step 3:  Data Selection","lvl2":"Step 1: Importing the libraries"},"content":"This section shows you how to load the data you want. You can select any timestep, region (x,y,z) you want. You can set the quality or resolution of the data as well. Higher quality means the finer(more) data. Not setting any time means first timestep available. Not setting quality means full data which takes a while to load because of the higher filesize.  Since each timestep is >30GB, I am only selecting 1 level out of 90.\n\ndata=db.db.read(time=0,z=[0,1],quality=-4) #Since each timestep is >30GB, I am only selecting 1 level out of 90.\ndata.shape\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-3-data-selection","position":13},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc4320-visualization#step-4-visualize-the-data","position":14},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"content":"We are using a simple matplotlib here, but since the data is in numpy array, it can loaded with any python modules that support numpy. Feel free to set the vmin,vmax appropriately.\n\nfig,axes=plt.subplots(1,1,figsize=(12,8))\nim= axes.imshow(data[0,:,:], aspect='auto',origin='lower',cmap='turbo')\ncbar = plt.colorbar(im, ax=axes)\ncbar.set_label('Temperature (deg. C)')\nplt.show()\n\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-4-visualize-the-data","position":15},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"But, what if you want to see the full data for a certain region at a certain depth?","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc4320-visualization#but-what-if-you-want-to-see-the-full-data-for-a-certain-region-at-a-certain-depth","position":16},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"But, what if you want to see the full data for a certain region at a certain depth?","lvl2":"Step 1: Importing the libraries"},"content":"Just set the right x,y,z while reading the data. x and y are the bounding box, z is the depth/layer.\n\ndata1=db.db.read(time=1,z=[0,1],quality=-6,x=[500,2500],y=[8500,11000])\nplt.imshow(data1[0,:,:], origin='lower',cmap='turbo')\nplt.colorbar()\n\ndata1.shape #\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#but-what-if-you-want-to-see-the-full-data-for-a-certain-region-at-a-certain-depth","position":17},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 5: Save the data for the region locally"},"type":"lvl2","url":"/notebooks/ocean-llc4320-visualization#step-5-save-the-data-for-the-region-locally","position":18},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 5: Save the data for the region locally"},"content":"You can save the data locally as you want. For example, here we are only saving the region shown above as a numpy array.\n\nnp.save('test_region.npy', data1)\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-5-save-the-data-for-the-region-locally","position":19},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 6: Load the locally saved region and visualize using matplotlib"},"type":"lvl2","url":"/notebooks/ocean-llc4320-visualization#step-6-load-the-locally-saved-region-and-visualize-using-matplotlib","position":20},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 6: Load the locally saved region and visualize using matplotlib"},"content":"\n\nlocal_data=np.load('test_region.npy')\nplt.imshow(local_data[0,:,:], origin='lower',cmap='turbo')\nplt.colorbar()\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-6-load-the-locally-saved-region-and-visualize-using-matplotlib","position":21},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 7: Horizontal Slicing"},"type":"lvl2","url":"/notebooks/ocean-llc4320-visualization#step-7-horizontal-slicing","position":22},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 7: Horizontal Slicing"},"content":"\n\ndata1=db.db.read(time=1,x=[500,2500],y=[10500,10501])\ndata1.shape\n\n\nplt.figure(figsize=(14,8))\nplt.imshow(data1[:,0,:],cmap='turbo')\n# plt.colorbar()\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-7-horizontal-slicing","position":23},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl2":"Step 7: Horizontal Slicing"},"type":"lvl4","url":"/notebooks/ocean-llc4320-visualization#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":24},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl2":"Step 7: Horizontal Slicing"},"content":"Aashish Panta (\n\naashishpanta0@gmail​.com)\n\nGiorgio Scorzelli (\n\nscrgiorgio@gmail​.com)\n\nValerio Pascucci (\n\npascucci​.valerio@gmail​.com)","type":"content","url":"/notebooks/ocean-llc4320-visualization#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":25},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in this Project Pythia Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1}]}