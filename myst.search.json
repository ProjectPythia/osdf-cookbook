{"version":"1","records":[{"hierarchy":{"lvl1":"OSDF Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"OSDF Cookbook"},"content":"\n\n\n\n\n\n\n\n\n\nThis Project Pythia Cookbook covers using the Open Science Data Federation (OSDF), a service for streaming scientific data across the globe.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":2},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Motivation"},"content":"Have you ever been frustrated by the complications of accessing scientific data?  Why can’t it “just work”, like watching a Netflix movie?\n\nThe OSDF is a service that simplifies the streaming of a wide range of scientific datasets with a goal that data access “just works”.  It\nis meant to improve data availability for researchers working at any scale from individual laptops to distributed computing services\nsuch as the OSG’s \n\nOSPool.\n\nThis cookbook gives motivating use cases from the geoscience community, including using datasets from NSF NCAR’s \n\nResearch Data Archive (RDA) and the datasets of AWS \n\nOpenData.","type":"content","url":"/#motivation","position":3},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":4},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Authors"},"content":"Harsha R. Hampapura\n\n\nBrian Bockelman\n\n\nAlexander Hoelzemann\n\n\nCarrie Wall\n\n\nEmma Turetsky\n\n\nAmandha Wingert Barok\n\n\nAashish Panta\n\n\nJoanmarie Del Vecchio\n\n\nJustin Hiemstra\n\n\nDouglas Schuster\n\n\nRiley Conroy\n\n\nKibiwott Koech","type":"content","url":"/#authors","position":5},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":6},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":7},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":8},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Structure"},"content":"This cookbook is broken up into two pieces - some background knowledge on the OSDF service itself\nand then a series of motivating examples from different repositories accessible via the OSDF.","type":"content","url":"/#structure","position":9},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"OSDF Fundamentals","lvl2":"Structure"},"type":"lvl3","url":"/#osdf-fundamentals","position":10},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"OSDF Fundamentals","lvl2":"Structure"},"content":"What is the OSDF?  Who supports it? How can it benefit from my science?  A dive into the infrastructure itself.","type":"content","url":"/#osdf-fundamentals","position":11},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using Datasets from NCAR’s Research Data Archive","lvl2":"Structure"},"type":"lvl3","url":"/#using-datasets-from-ncars-research-data-archive","position":12},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using Datasets from NCAR’s Research Data Archive","lvl2":"Structure"},"content":"NSF NCAR’s \n\nResearch Data Archive (RDA) contains a large collection of meteorological, atmospheric composition, and oceanographic observations, and operational and reanalysis model outputs, integrated with NSF NCAR High Performance Compute services to support atmospheric and geosciences research. This chapter demonstrates how to use common data science tools when streaming from the RDA.","type":"content","url":"/#using-datasets-from-ncars-research-data-archive","position":13},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using Datasets from FIU’s Envistor","lvl2":"Structure"},"type":"lvl3","url":"/#using-datasets-from-fius-envistor","position":14},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using Datasets from FIU’s Envistor","lvl2":"Structure"},"content":"Florida International University (FIU) runs the \n\nEnvistor project, aggregating climate datasets from the south Florida region.","type":"content","url":"/#using-datasets-from-fius-envistor","position":15},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using NOAA’s SONAR Fisheries Datasets","lvl2":"Structure"},"type":"lvl3","url":"/#using-noaas-sonar-fisheries-datasets","position":16},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using NOAA’s SONAR Fisheries Datasets","lvl2":"Structure"},"content":"NOAA maintains a copy of its SONAR-based datasets of Atlanta fisheries data in the popular Zarr format.  This chapter shows how to load and use the datasets and fuse it with other products.","type":"content","url":"/#using-noaas-sonar-fisheries-datasets","position":17},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using Sentinel Data from AWS","lvl2":"Structure"},"type":"lvl3","url":"/#using-sentinel-data-from-aws","position":18},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Using Sentinel Data from AWS","lvl2":"Structure"},"content":"All of AWS OpenData is connected to the OSDF!  This chapter includes examples of streaming Sentinel-2 data, stored in AWS’s OpenData program, to your notebook.","type":"content","url":"/#using-sentinel-data-from-aws","position":19},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":20},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\nBinder or on your local machine.","type":"content","url":"/#running-the-notebooks","position":21},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":22},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you’ll be able to execute\nand even change the example programs. You’ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.\n\nNote, not all Cookbook chapters are executable. If you do not see\nthe rocket ship icon, such as on this page, you are not viewing an\nexecutable book chapter.","type":"content","url":"/#running-on-binder","position":23},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":24},{"hierarchy":{"lvl1":"OSDF Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will need to follow this workflow:\n\nClone the https://github.com/ProjectPythia/osdf-cookbook repository: git clone https://github.com/ProjectPythia/osdf-cookbook.git\n\nMove into the osdf-cookbook directorycd osdf-cookbook\n\nCreate and activate your conda environment from the environment.yml fileconda env create -f environment.yml\nconda activate osdf-cookbook\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":25},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)"},"type":"lvl1","url":"/notebooks/osdf-intro","position":0},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)"},"content":"Have you ever pondered why accessing large-scale scientific data is so complicated, while accessing large-scale volumes of movies is so simple on Netflix?\n\nEach data repository has its own website or a set of unique tools for accessing data.  Users are often encouraged to download datasets locally and then do local computations, as repositories prioritize long-term storage and preservation rather than fast or distributed access.\n\nHow does Netflix do it without making you download the whole movie ahead of time?  They leverage a content distribution network (CDN), which caches copies of the most popular movies at opportune locations on the Internet closer to users. They also let you stream your favorite shows so you can start watching while later sections of the show are still downloading.\n\nThe \n\nOSDF, an NSF-funded infrastructure providing a CDN for science, makes this kind of streaming possible for scientific data.  It is connected to popular open science repositories and has hardware embedded across US and international networks and at large computing sites.\n\nThis cookbook provides examples of using the OSDF’s streaming to power science use cases in earth sciences.","type":"content","url":"/notebooks/osdf-intro","position":1},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl2":"Do First, Understand Later"},"type":"lvl2","url":"/notebooks/osdf-intro#do-first-understand-later","position":2},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl2":"Do First, Understand Later"},"content":"How Do You Use the OSDF?\n\nThe service is powered by the same protocol as the web, HTTPS.  Thus, the simplest use case is to download an object by using the browser.\n\nClick on this link:\n\nhttps://​osdf​-director​.osg​-htc​.org​/ospool​/uc​-shared​/public​/OSG​-Staff​/validation​/test​.txt\n\nIf a new tab opened with the text “Hello, World” – congratulations, you used the OSDF!\n\nOSDF is often used in conjunction with computing workflows and downloads occur as part of a script.  For this, a command line client - \n\npelican is utilized.  Try running the following:\n\npelican object get osdf:///routeviews/chicago/route-views.chicago/bgpdata/2025.03/RIBS/rib.20250319.0400.bz2 ./\n\nDepending on the speed of your Internet connection, you may see a progress bar as the download proceeds.\n\nCongratulations, you’re now the proud owner of 72MB of Internet routing data!\n\nFor both of these cases, we downloaded the entire object.  What happens if the dataset contains data for the entire planet but you are only interested in the state of Nebraska?  It’s more effective to stream the subset.  For that, we will use the \n\nPelican Python library; this library will be used throughout the remaining chapters of this cookbook.","type":"content","url":"/notebooks/osdf-intro#do-first-understand-later","position":3},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl2":"About the OSDF"},"type":"lvl2","url":"/notebooks/osdf-intro#about-the-osdf","position":4},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl2":"About the OSDF"},"content":"You don’t need to know how Netflix is built to press “play”.  Similarly, you don’t need to understand the guts of the OSDF to use it in your science.  However, a few key concepts are useful!\n\nOSDF Infrastructure: The map below shows the distributed pieces of the OSDF:\n\nEach “O” on the map is an origin; the origin service connects an existing repository to the OSDF service, making some datasets available and protecting the repository from overload.  Origins are typically placed nearby where the data lives; the origin for the NCAR Research Data Archive (RDA) is in the same datacenter as the RDA.\n\nEach “C” is a cache.  The cache makes temporary copies of objects upon access so, on subsequent accesses, the object comes from the cache and not from the repository.  This reduces the load on the repository and, ideally, increases scalability.\n\nUnified Namespace: The OSDF provides a unified namespace for all available objects.  Each repository receives a unique prefix (the IceCube experiment’s data is available from /icecube; NCAR’s RDA is available from /ncar-rda) and the object can be referenced from within the prefix.\n\nFrom our RouteViews example above, we were interested in accessing the object named chicago/route-views.chicago/bgpdata/2025.03/RIBS/rib.20250319.0400.bz2.  Since the prefix for RouteViews is /routeviews, the entire OSDF name is:osdf://routeviews/chicago/route-views.chicago/bgpdata/2025.03/RIBS/rib.20250319.0400.bz2\n\n“Objects” vs “Files”: You may have noticed that this notebook refers to downloading/streaming “objects” instead of “files”. What’s the difference, and why does OSDF bother making this distinction?\n\nBoth objects and files are ways for computers to store data, and in practice, the earth science calculations in this cookbook use them the same way—regardless of where the data comes from.\n\nThe key difference is the way we typically think about accessing or retrieving that data: When you open files like Word documents or images on your computer, you probably click through folders or directories to find them. But when you’re working with data over the internet, that folder-based structure doesn’t always apply.\n\nIn the OSDF, an object is simply a piece of data that can be shared, like a file—but without needing to think about where it’s stored or how it’s organized on someone else’s computer. “Object” is a more flexible term that works better when data is stored in large systems across many locations.\n\nWarning\n\nImmutable Objects: The OSDF assumes that objects are immutable; once created, they aren’t permitted to be changed.  This allows OSDF to effectively make copies of the objects in the caches.\n\nThis typically works well with scientific datasets: you rarely change your data after you record it!  However, if you start using OSDF more heavily, this is an important requirement to be aware of.","type":"content","url":"/notebooks/osdf-intro#about-the-osdf","position":5},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl2":"Finding My Objects"},"type":"lvl2","url":"/notebooks/osdf-intro#finding-my-objects","position":6},{"hierarchy":{"lvl1":"Streaming Data with the Open Science Data Federation (OSDF)","lvl2":"Finding My Objects"},"content":"How do you find the object you’re interested in?\n\nTypically, dataset providers connected to the OSDF provide a search, data catalog, or STAC catalog publishing OSDF-style URLs.  You can determine this from the provider’s website; additionally, OSDF maintains a \n\nlist of known links you can peruse.\n\nExplore this cookbook: This cookbook provides examples for how to use OSDF to access:\n\nNCAR’s Research Data Archive.\n\nAWS’s OpenData Program.\n\nThe Envistor platform at Florida International University.","type":"content","url":"/notebooks/osdf-intro#finding-my-objects","position":7},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF"},"type":"lvl1","url":"/notebooks/pelicanfs","position":0},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF"},"content":"\n\n","type":"content","url":"/notebooks/pelicanfs","position":1},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/pelicanfs#overview","position":2},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Overview"},"content":"Now that you’ve learned about the OSDF and the Pelican command line client, you may be wondering how you can easily access that data from within a notebook using python.\n\nYou can do this using PelicanFS, which is an FSSPec implementation of the Pelican client.","type":"content","url":"/notebooks/pelicanfs#overview","position":3},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"This notebook will contain:","lvl2":"Overview"},"type":"lvl3","url":"/notebooks/pelicanfs#this-notebook-will-contain","position":4},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"This notebook will contain:","lvl2":"Overview"},"content":"A brief explanation of FSSPec and PelicanFS\n\nA real-world example using FSSPec, Pelican, Xarray, and Zarr\n\nOther common access patterns\n\nFAQs\n\n","type":"content","url":"/notebooks/pelicanfs#this-notebook-will-contain","position":5},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/pelicanfs#prerequisites","position":6},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Prerequisites"},"content":"To better understand this notebook, please familiarize yourself with the following concepts:\n\nConcepts\n\nImportance\n\nNotes\n\nIntro to OSDF\n\nNecessary\n\n\n\nUnderstanding of Xarray\n\nHelpful\n\nTo better understand the example workflow\n\nOverview of FSSpec\n\nHelpful\n\nTo better understand the FSSpec library\n\nTime to learn: 20-30 minutes\n\n\n\n","type":"content","url":"/notebooks/pelicanfs#prerequisites","position":7},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/pelicanfs#imports","position":8},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Imports"},"content":"\n\nimport xarray as xr\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport metpy.calc as mpcalc\nfrom metpy.units import units\nimport fsspec\nimport intake\n\n","type":"content","url":"/notebooks/pelicanfs#imports","position":9},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"What are PelicanFS and FSSPec?"},"type":"lvl2","url":"/notebooks/pelicanfs#what-are-pelicanfs-and-fsspec","position":10},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"What are PelicanFS and FSSPec?"},"content":"First, let’s understand PelicanFS and how it integrates with FSSpec\n\n","type":"content","url":"/notebooks/pelicanfs#what-are-pelicanfs-and-fsspec","position":11},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"FSSPec","lvl2":"What are PelicanFS and FSSPec?"},"type":"lvl3","url":"/notebooks/pelicanfs#fsspec","position":12},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"FSSPec","lvl2":"What are PelicanFS and FSSPec?"},"content":"FileSystem Spec (fsspec) is a python library which endeavors to provide a unified interface to many different storage backends. This includes, but is not limited to, POSIX, https, and S3. It’s used by various data processing libraries such as xarray, pandas, and intake, just to name a few.\n\nTo learn more about FSSPec, visit its \n\ninformation page.","type":"content","url":"/notebooks/pelicanfs#fsspec","position":13},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl4":"Protocols","lvl3":"FSSPec","lvl2":"What are PelicanFS and FSSPec?"},"type":"lvl4","url":"/notebooks/pelicanfs#protocols","position":14},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl4":"Protocols","lvl3":"FSSPec","lvl2":"What are PelicanFS and FSSPec?"},"content":"FSSpec figures out how to interact with data from different storage backends through the scheme in the data path. For example, FSSpec knows to use the “Hyper Text Transfer Protocol” interface whenever it sees URLs with the https: scheme. This lets users interact with data from a variety of storage technologies without forcing them to know how those technologies work under the hood.","type":"content","url":"/notebooks/pelicanfs#protocols","position":15},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"PelicanFS","lvl2":"What are PelicanFS and FSSPec?"},"type":"lvl3","url":"/notebooks/pelicanfs#pelicanfs","position":16},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"PelicanFS","lvl2":"What are PelicanFS and FSSPec?"},"content":"PelicanFS is an implementation of FSSpec which introduces two new protocols to FSSpec: pelican and osdf. The pelican protocol specifies FSSpec to use the PelicanFS implementation to access data via a Pelican Federation. To use it, you must specify the federation host name. A pelican fsspec path would look like:\n\npelican://<federation-host-name>/<namespace-path>\n\nThe osdf protocol is a specific instance of the pelican protocol that knows how to access the OSDF. A path using the osdf protocol should not provide the federation root. An osdf fsspec path would look like:\n\nosdf:///<namespace-path>\n\nNote\n\nNotice the three ‘/’ after “osdf:”. This is required for a properly formed osdf path.\n\nIf you’d like to understand more about how pelican works, check out the documentation \n\nhere.","type":"content","url":"/notebooks/pelicanfs#pelicanfs","position":17},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Putting it all together","lvl2":"What are PelicanFS and FSSPec?"},"type":"lvl3","url":"/notebooks/pelicanfs#putting-it-all-together","position":18},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Putting it all together","lvl2":"What are PelicanFS and FSSPec?"},"content":"What does this mean in practice?\n\nIf you want to access data from the OSDF using FSSpec or any library which uses FSSpec, all that you need to do is give it the proper path with the osdf protocol to FSSpec and then FSSpec and PelicanFS will do all the work to resolve it behind the scenes.\n\n\n\n","type":"content","url":"/notebooks/pelicanfs#putting-it-all-together","position":19},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"A PelicanFS Example using Real Data"},"type":"lvl2","url":"/notebooks/pelicanfs#a-pelicanfs-example-using-real-data","position":20},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"A PelicanFS Example using Real Data"},"content":"The following is an example that shows how PelicanFS works on real world data using FSSPec and Xarray to access Zarr data from AWS.\n\nThis portion of the notebook is based off of the \n\nProject Pythia HRRR AWS Cookbook\n\n","type":"content","url":"/notebooks/pelicanfs#a-pelicanfs-example-using-real-data","position":21},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Setting the Proper Path","lvl2":"A PelicanFS Example using Real Data"},"type":"lvl3","url":"/notebooks/pelicanfs#setting-the-proper-path","position":22},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Setting the Proper Path","lvl2":"A PelicanFS Example using Real Data"},"content":"The data for this tutorial is part of AWS Open Data, hosted in the us-west-1 region. The OSDF provides access to that region using the /aws-opendata/us-west-1 namespace.\n\nLet’s first create a path which uses the osdf protocol.\n\n# Set the date, hour, variable, and level for the HRRR data\ndate = '20211016'\nhour = '21'\nvar = 'TMP'\nlevel = '2m_above_ground'\n\n# Construct file paths for the Zarr datasets using the osdf protocol\nnamespace_file1 = f'osdf:///aws-opendata/us-west-1/hrrrzarr/sfc/{date}/{date}_{hour}z_anl.zarr/{level}/{var}/{level}/'\nnamespace_file2 = f'osdf:///aws-opendata/us-west-1/hrrrzarr/sfc/{date}/{date}_{hour}z_anl.zarr/{level}/{var}/'\n\n","type":"content","url":"/notebooks/pelicanfs#setting-the-proper-path","position":23},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Using FSSpec to access the data","lvl2":"A PelicanFS Example using Real Data"},"type":"lvl3","url":"/notebooks/pelicanfs#using-fsspec-to-access-the-data","position":24},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Using FSSpec to access the data","lvl2":"A PelicanFS Example using Real Data"},"content":"Now we can access the data using XArray as usual. The two files will accessed using fsspec’s get_mapper function, which knows to use PelicanFS because we created the path using the osdf protocol.\n\n# Get mappers for the Zarr datasets\n\nfile1 = fsspec.get_mapper(namespace_file1)\nfile2 = fsspec.get_mapper(namespace_file2)\n\n# Open the datasets\nds = xr.open_mfdataset([file1, file2], engine='zarr', decode_timedelta=True)\n\n# Display the dataset\nds\n\n","type":"content","url":"/notebooks/pelicanfs#using-fsspec-to-access-the-data","position":25},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Continue the workflow","lvl2":"A PelicanFS Example using Real Data"},"type":"lvl3","url":"/notebooks/pelicanfs#continue-the-workflow","position":26},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Continue the workflow","lvl2":"A PelicanFS Example using Real Data"},"content":"As you can see, Xarray streamed the data correctly into the datasets. To prove the workflow works, the next cell continues the computation and generates two plots. This tutorial will not go in depth as to what this code is accomplishing.\n\nIf you’d like to know more about the following workflow, please refer to the \n\nProject Pythia HRRR AWS Cookbook\n\n# Define coordinates for projection\nlon1 = -97.5\nlat1 = 38.5\nslat = 38.5\n\n# Define the Lambert Conformal projection\nprojData = ccrs.LambertConformal(\n    central_longitude=lon1,\n    central_latitude=lat1,\n    standard_parallels=[slat, slat],\n    globe=ccrs.Globe(\n        semimajor_axis=6371229,\n        semiminor_axis=6371229\n    )\n)\n\n# Display dataset coordinates\nds.coords\n\n# Extract temperature data\nairTemp = ds.TMP\n\n# Display the temperature data\nairTemp\n\n# Convert temperature units to Celsius\nairTemp = airTemp.metpy.convert_units('degC')\n\n# Display the converted temperature data\nairTemp\n\n# Extract projection coordinates\nx = airTemp.projection_x_coordinate\ny = airTemp.projection_y_coordinate\n\n# Plot temperature data\nairTemp.plot(figsize=(11, 8.5))\n\n# Compute minimum and maximum temperatures\nminTemp = airTemp.min().compute()\nmaxTemp = airTemp.max().compute()\n\n# Display minimum and maximum temperature values\nminTemp.values, maxTemp.values\n\n# Define contour levels\nfint = np.arange(np.floor(minTemp.values), np.ceil(maxTemp.values) + 2, 2)\n\n# Define plot bounds and resolution\nlatN = 50.4\nlatS = 24.25\nlonW = -123.8\nlonE = -71.2\nres = '50m'\n\n# Create a figure and axis with projection\nfig = plt.figure(figsize=(18, 12))\nax = plt.subplot(1, 1, 1, projection=projData)\nax.set_extent([lonW, lonE, latS, latN], crs=ccrs.PlateCarree())\nax.add_feature(cfeature.COASTLINE.with_scale(res))\nax.add_feature(cfeature.STATES.with_scale(res))\n\n# Add the title\ntl1 = 'HRRR 2m temperature ($^\\\\circ$C)'\ntl2 = f'Analysis valid at: {hour}00 UTC {date}'\nplt.title(f'{tl1}\\n{tl2}', fontsize=16)\n\n# Contour fill\nCF = ax.contourf(x, y, airTemp, levels=fint, cmap=plt.get_cmap('coolwarm'))\n\n# Make a colorbar for the ContourSet returned by the contourf call\ncbar = fig.colorbar(CF, shrink=0.5)\ncbar.set_label(r'2m Temperature ($^\\circ$C)', size='large')\n\n# Show the plot\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/pelicanfs#continue-the-workflow","position":27},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Other Ways to Access"},"type":"lvl2","url":"/notebooks/pelicanfs#other-ways-to-access","position":28},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl2":"Other Ways to Access"},"content":"There are other common ways to access data and use data with FSSpec and PelicanFS. This section will will cover the following topics\n\nUsing an Intake Catalog\n\nDirectly Accessing Data\n\n","type":"content","url":"/notebooks/pelicanfs#other-ways-to-access","position":29},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Intake Catalog","lvl2":"Other Ways to Access"},"type":"lvl3","url":"/notebooks/pelicanfs#intake-catalog","position":30},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Intake Catalog","lvl2":"Other Ways to Access"},"content":"In order to use PelicanFS with an Intake catalog, the paths in the catalog itself need to use the osdf or pelican protocols.\n\nHere’s an example using the catalog located at \n\nhttps://​data​.rda​.ucar​.edu​/d850001​/catalogs​/osdf​/cmip6​-aws​/cmip6​-osdf​-zarr​.json\n\nAn entry in the catalog’s csv file looks like:\n\nHighResMIP,CMCC,CMCC-CM2-HR4,highresSST-present,r1i1p1f1,Amon,ta,gn,osdf:///aws-opendata/us-west-2/cmip6-pds/CMIP6/HighResMIP/CMCC/CMCC-CM2-HR4/highresSST-present/r1i1p1f1/Amon/ta/gn/v20170706/,,20170706\n\nNotice how the path is using the ‘osdf’ protocol and the ‘/aws-opendata/us-west-2’ namespace. If all the paths in the csv file are formatted like this, then you can use the Intake catalog exactly as usual.\n\nHere is a workflow and plot which uses an Intake catalog and the osdf protocol. If you want to understand more about the underlying workflow, please look at the \n\nGlobal Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data notebook.\n\nrda_url     =  'https://data.rda.ucar.edu/'\ncat_url     = rda_url +  'd850001/catalogs/osdf/cmip6-aws/cmip6-osdf-zarr.json'\n\ncol = intake.open_esm_datastore(cat_url)\n\nexpts = ['historical']\n\nquery = dict(\n    experiment_id=expts,\n    table_id='Amon',\n    variable_id=['tas'],\n    member_id = 'r1i1p1f1',\n    #activity_id = 'CMIP',\n)\n\ncol_subset = col.search(require_all_on=[\"source_id\"], **query)\n\nds = xr.open_zarr(col_subset.df['zstore'][0])\n\nds.tas.isel(time=0).plot()\n\n","type":"content","url":"/notebooks/pelicanfs#intake-catalog","position":31},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Direct Access of Data","lvl2":"Other Ways to Access"},"type":"lvl3","url":"/notebooks/pelicanfs#direct-access-of-data","position":32},{"hierarchy":{"lvl1":"Using PelicanFS via FSSpec to Access Data on the OSDF","lvl3":"Direct Access of Data","lvl2":"Other Ways to Access"},"content":"You can also access the data directly using normal file system calls.\n\nFor example, let’s say you want to read in a csv file from the OSDF. Just use the same pattern we’ve shown before of\n\nosdf:///<namespace-path>\n\nfor your path.\n\nwith fsspec.open('osdf:///ndp/burnpro3d/YosemiteBurnExample/burnpro3d-yosemite-example.csv') as ex_csv:\n    content = ex_csv.read()\n    print(content.decode())","type":"content","url":"/notebooks/pelicanfs#direct-access-of-data","position":33},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data"},"type":"lvl1","url":"/notebooks/cesm2-oceanheat","position":0},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data"},"content":"\n\n","type":"content","url":"/notebooks/cesm2-oceanheat","position":1},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Overview"},"type":"lvl3","url":"/notebooks/cesm2-oceanheat#overview","position":2},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Overview"},"content":"This notebook is an adpation of a \n\nworkflow in the NCAR gallery of the Pangeo collection\n\nThis notebook illustrates how to compute surface ocean heat content using potential temperature data from \n\nCESM2 Large Ensemble Dataset (Community Earth System Model 2) hosted on NCAR’s RDA.\n\nThis data is open access and is accessed via OSDF using the pelicanFS package and demonstrates how you can stream data from NCAR’s RDA\n\nPlease refer to the first chapter of this cookbook to learn more about OSDF, pelican or pelicanFS\n\n","type":"content","url":"/notebooks/cesm2-oceanheat#overview","position":3},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Prerequisites"},"type":"lvl3","url":"/notebooks/cesm2-oceanheat#prerequisites","position":4},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Intake-ESM\n\nNecessary\n\nUsed for searching CMIP6 data\n\nUnderstanding of Zarr\n\nHelpful\n\nFamiliarity with metadata structure\n\nMatplotlib\n\nHelpful\n\nPackage used for plotting\n\nPelicanFS\n\nNecessary\n\nThe python package used to stream data in this notebook\n\nOSDF\n\nHelpful\n\nOSDF is used to stream data in this notebook\n\nTime to learn: 20 mins\n\n\n\n","type":"content","url":"/notebooks/cesm2-oceanheat#prerequisites","position":5},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Imports"},"type":"lvl3","url":"/notebooks/cesm2-oceanheat#imports","position":6},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Imports"},"content":"\n\nimport intake\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nimport seaborn as sns\nimport re\nimport matplotlib.pyplot as plt\nimport dask\nfrom dask.distributed import LocalCluster\nimport pelicanfs \nimport cf_units as cf\n\n# Load Catalog URL\ncat_url = 'https://stratus.rda.ucar.edu/d010092/catalogs/d010092-osdf-zarr-gdex.json'\n\n","type":"content","url":"/notebooks/cesm2-oceanheat#imports","position":7},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Set up local dask cluster"},"type":"lvl3","url":"/notebooks/cesm2-oceanheat#set-up-local-dask-cluster","position":8},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Set up local dask cluster"},"content":"\n\nBefore we do any computation let us first set up a local cluster using dask\n\ncluster = LocalCluster()          \nclient = cluster.get_client()\n\n# Scale the cluster\nn_workers = 5\ncluster.scale(n_workers)\ncluster\n\n","type":"content","url":"/notebooks/cesm2-oceanheat#set-up-local-dask-cluster","position":9},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Data Loading"},"type":"lvl3","url":"/notebooks/cesm2-oceanheat#data-loading","position":10},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Data Loading"},"content":"","type":"content","url":"/notebooks/cesm2-oceanheat#data-loading","position":11},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl4":"Load CESM2 LENS data from NCAR’s RDA","lvl3":"Data Loading"},"type":"lvl4","url":"/notebooks/cesm2-oceanheat#load-cesm2-lens-data-from-ncars-rda","position":12},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl4":"Load CESM2 LENS data from NCAR’s RDA","lvl3":"Data Loading"},"content":"Load CESM2 LENS zarr data from RDA using an intake-ESM catalog that has OSDF links\n\nFor more details regarding the dataset. See, \n\nhttps://​rda​.ucar​.edu​/datasets​/d010092​/#\n\ncol = intake.open_esm_datastore(cat_url)\ncol\n\n# Uncomment this line to see all the variables\n# cesm_cat.df['variable'].values\n\ncesm_temp = col.search(variable ='TEMP', frequency ='monthly')\ncesm_temp\n\ncesm_temp.df['path'].values\n\nNote\n\nNote that all the file paths start with \n\nhttps://​data​-osdf​.rda​.ucar​.edu indicating that the data will be streamed via OSDF\n\ndsets_cesm = cesm_temp.to_dataset_dict()\n\ndsets_cesm.keys()\n\nhistorical       = dsets_cesm['ocn.historical.monthly.cmip6']\nfuture_smbb      = dsets_cesm['ocn.ssp370.monthly.smbb']\nfuture_cmip6     = dsets_cesm['ocn.ssp370.monthly.cmip6']\n\n","type":"content","url":"/notebooks/cesm2-oceanheat#load-cesm2-lens-data-from-ncars-rda","position":13},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl4":"Change units","lvl3":"Data Loading"},"type":"lvl4","url":"/notebooks/cesm2-oceanheat#change-units","position":14},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl4":"Change units","lvl3":"Data Loading"},"content":"\n\norig_units = cf.Unit(historical.z_t.attrs['units'])\norig_units\n\ndef change_units(ds, variable_str, variable_bounds_str, target_unit_str):\n    orig_units = cf.Unit(ds[variable_str].attrs['units'])\n    target_units = cf.Unit(target_unit_str)\n    variable_in_new_units = xr.apply_ufunc(orig_units.convert, ds[variable_bounds_str], target_units, dask='parallelized', output_dtypes=[ds[variable_bounds_str].dtype])\n    return variable_in_new_units\n\ndepth_levels_in_m = change_units(historical, 'z_t', 'z_t', 'm')\nhist_temp_in_degK = change_units(historical, 'TEMP', 'TEMP', 'degK')\nfut_cmip6_temp_in_degK = change_units(future_cmip6, 'TEMP', 'TEMP', 'degK')\nfut_smbb_temp_in_degK = change_units(future_smbb, 'TEMP', 'TEMP', 'degK')\n#\nhist_temp_in_degK  = hist_temp_in_degK.assign_coords(z_t=(\"z_t\", depth_levels_in_m['z_t'].data))\nhist_temp_in_degK[\"z_t\"].attrs[\"units\"] = \"m\"\nhist_temp_in_degK\n\ndepth_levels_in_m.isel(z_t=slice(0, -1))\n\nCompute depth level deltas using the difference of z_t levels\n\ndepth_level_deltas = depth_levels_in_m.isel(z_t=slice(1, None)).values - depth_levels_in_m.isel(z_t=slice(0, -1)).values\n# Optionally, if you want to keep it as an xarray DataArray, re-wrap the result\ndepth_level_deltas = xr.DataArray(depth_level_deltas, dims=[\"z_t\"], coords={\"z_t\": depth_levels_in_m.z_t.isel(z_t=slice(0, -1))})\ndepth_level_deltas  \n\n","type":"content","url":"/notebooks/cesm2-oceanheat#change-units","position":15},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl2":"Compute Ocean Heat content for ocean surface"},"type":"lvl2","url":"/notebooks/cesm2-oceanheat#compute-ocean-heat-content-for-ocean-surface","position":16},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl2":"Compute Ocean Heat content for ocean surface"},"content":"Ocean surface is considered to be the top 100m\n\nThe formula for this is: H = \\rho C \\int_0^z T(z) dz\n\nWhere H is ocean heat content, the value we are trying to calculate,\n\n\\rho is the density of sea water, 1026 kg/m^3  ,\n\nC is the specific heat of sea water, 3990 J/(kg K)  ,\n\nz is the depth limit of the calculation in meters,\n\nand T(z) is the temperature at each depth in degrees Kelvin.\n\ndef calc_ocean_heat(delta_level, temperature):\n    rho = 1026 #kg/m^3\n    c_p = 3990 #J/(kg K)\n    weighted_temperature = delta_level * temperature\n    heat = weighted_temperature.sum(dim=\"z_t\")*rho*c_p\n    return heat\n\n# Remember that the coordinate z_t still has values in cm\nhist_temp_ocean_surface = hist_temp_in_degK.where(hist_temp_in_degK['z_t'] < 1e4,drop=True)\nhist_temp_ocean_surface\n\ndepth_level_deltas_surface = depth_level_deltas.where(depth_level_deltas['z_t'] <1e4, drop= True)\ndepth_level_deltas_surface\n\nhist_ocean_heat = calc_ocean_heat(depth_level_deltas_surface,hist_temp_ocean_surface)\nhist_ocean_heat\n\n","type":"content","url":"/notebooks/cesm2-oceanheat#compute-ocean-heat-content-for-ocean-surface","position":17},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl4":"Plot Ocean Heat","lvl2":"Compute Ocean Heat content for ocean surface"},"type":"lvl4","url":"/notebooks/cesm2-oceanheat#plot-ocean-heat","position":18},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl4":"Plot Ocean Heat","lvl2":"Compute Ocean Heat content for ocean surface"},"content":"\n\nAttention\n\nHas the surface ocean heat content increased with time due to Global Warming ?\nLet us plot the difference between the annual, ensemble mean heat content between the years 2014 and 1850 to check!\n\n%%time\n# Compute annual and ensemble mean\nhist_oceanheat_ann_mean = hist_ocean_heat.mean('member_id').groupby('time.year').mean()\nhist_oceanheat_ann_mean \n\nhist_oceanheat_ano = \\\nhist_oceanheat_ann_mean.sel(year=2014) - hist_oceanheat_ann_mean.sel(year=1850)\n\n%%time\nhist_oceanheat_ano.plot()\n\nIndeed! The surface ocean is trapping heat as the globe warms!\n\n\n\n","type":"content","url":"/notebooks/cesm2-oceanheat#plot-ocean-heat","position":19},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Summary","lvl2":"Compute Ocean Heat content for ocean surface"},"type":"lvl3","url":"/notebooks/cesm2-oceanheat#summary","position":20},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Summary","lvl2":"Compute Ocean Heat content for ocean surface"},"content":"In this notebook we used sea temperature data from the Community Earth System Model (CESM2) Large Ensemble dataset to compute surface ocean heat and convince ourselves that the surface ocean is trapping extra heat as the globe warms. We used an intake-ESM catalog backed by pelican links to stream data from NCAR’s Research Data Archive via NCAR’s OSDF origin!","type":"content","url":"/notebooks/cesm2-oceanheat#summary","position":21},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl4":"What’s next?","lvl3":"Summary","lvl2":"Compute Ocean Heat content for ocean surface"},"type":"lvl4","url":"/notebooks/cesm2-oceanheat#whats-next","position":22},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl4":"What’s next?","lvl3":"Summary","lvl2":"Compute Ocean Heat content for ocean surface"},"content":"In the next notebook, we will see how to load data from multiple OSDF origins into a workflow. We will stream CMIP6 model data from AWS and observational data from RDA.\n\n","type":"content","url":"/notebooks/cesm2-oceanheat#whats-next","position":23},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Resources and references","lvl2":"Compute Ocean Heat content for ocean surface"},"type":"lvl3","url":"/notebooks/cesm2-oceanheat#resources-and-references","position":24},{"hierarchy":{"lvl1":"Calculate Surface Ocean Heat using CESM2 LENS data","lvl3":"Resources and references","lvl2":"Compute Ocean Heat content for ocean surface"},"content":"Original \n\nnotebook on the pangeo NCAR gallery\n\nCESM2 Large Ensemble Dataset (Community Earth System Model 2) hosted on NCAR’s RDA.","type":"content","url":"/notebooks/cesm2-oceanheat#resources-and-references","position":25},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data"},"type":"lvl1","url":"/notebooks/cmip6-gmst","position":0},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data"},"content":"\n\n","type":"content","url":"/notebooks/cmip6-gmst","position":1},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/cmip6-gmst#overview","position":2},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Overview"},"content":"In this notebook we will compute the Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data and compare it with observations. This notebook is heavily inspired by the \n\nGMST example in the CMIP6 cookbook and we thank the authors for their workflow.\n\nWe will get the CMIP6 temperature data from the AWS open data program via the us-west-2 origin\n\nIn order to do this, we will use an intake-ESM catalog (hosted on NCAR’s RDA) that uses pelicanFS backed links instead of https or s3 links\n\nWe will grab observational data hosted on NCAR’s RDA, which is accessible via the NCAR origin\n\nPlease refer to the first chapter of this cookbook to learn more about OSDF, pelican or pelicanFS\n\nThis notebook demonstrates that you can seamlessly stream data from multiple OSDF origins in your workflow\n\n","type":"content","url":"/notebooks/cmip6-gmst#overview","position":3},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/cmip6-gmst#prerequisites","position":4},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Intake-ESM\n\nNecessary\n\nUsed for searching CMIP6 data\n\nUnderstanding of Zarr\n\nHelpful\n\nFamiliarity with metadata structure\n\nSeaborn\n\nHelpful\n\nUsed for plotting\n\nPelicanFS\n\nNecessary\n\nThe python package used to stream data in this notebook\n\nOSDF\n\nHelpful\n\nOSDF is used to stream data in this notebook\n\nTime to learn: 20 mins\n\n","type":"content","url":"/notebooks/cmip6-gmst#prerequisites","position":5},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/cmip6-gmst#imports","position":6},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Imports"},"content":"\n\nfrom matplotlib import pyplot as plt\nimport xarray as xr\nimport numpy as np\nfrom dask.diagnostics import progress\nfrom tqdm.autonotebook import tqdm\nimport intake\nimport fsspec\nimport seaborn as sns\nimport aiohttp\nimport dask\nfrom dask.distributed import LocalCluster\nimport pelicanfs \n\nWe will use an intake-ESM catalog hosted on NCAR’s Research Data Archive. This is nothing but the AWS cmip6 catalog modified to use OSDF\n\n# Load catalog URL\nrda_url     =  'https://data.rda.ucar.edu/'\ncat_url     = rda_url +  'd850001/catalogs/osdf/cmip6-aws/cmip6-osdf-zarr.json'\nprint(cat_url)\n\n","type":"content","url":"/notebooks/cmip6-gmst#imports","position":7},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Set up local dask cluster"},"type":"lvl2","url":"/notebooks/cmip6-gmst#set-up-local-dask-cluster","position":8},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Set up local dask cluster"},"content":"\n\nBefore we do any computation let us first set up a local cluster using dask\n\ncluster = LocalCluster()          \nclient = cluster.get_client()\n\n# Scale the cluster\nn_workers = 5\ncluster.scale(n_workers)\ncluster\n\n","type":"content","url":"/notebooks/cmip6-gmst#set-up-local-dask-cluster","position":9},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Data Loading"},"type":"lvl2","url":"/notebooks/cmip6-gmst#data-loading","position":10},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Data Loading"},"content":"","type":"content","url":"/notebooks/cmip6-gmst#data-loading","position":11},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Load CMIP6 data from AWS","lvl2":"Data Loading"},"type":"lvl3","url":"/notebooks/cmip6-gmst#load-cmip6-data-from-aws","position":12},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Load CMIP6 data from AWS","lvl2":"Data Loading"},"content":"\n\ncol = intake.open_esm_datastore(cat_url)\ncol\n\n# there is currently a significant amount of data for these runs\nexpts = ['historical', 'ssp245', 'ssp370']\n\nquery = dict(\n    experiment_id=expts,\n    table_id='Amon',\n    variable_id=['tas'],\n    member_id = 'r1i1p1f1',\n    #activity_id = 'CMIP',\n)\n\ncol_subset = col.search(require_all_on=[\"source_id\"], **query)\ncol_subset\n\nLet us inspect the zarr store paths to see if we are using the pelican protocol.\n\nWe see that zstore column has paths that start with ‘osdf:///’ instead of ‘https://’ which tells us that we are not using a simple ‘https’ GET request to fetch the data.\n\nIn order to know more about the pelican protocol, please refer to the first chapter of this cookbook.\n\ncol_subset.df\n\n","type":"content","url":"/notebooks/cmip6-gmst#load-cmip6-data-from-aws","position":13},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Grab some Observational time series data for comparison with ensemble spread","lvl2":"Data Loading"},"type":"lvl3","url":"/notebooks/cmip6-gmst#grab-some-observational-time-series-data-for-comparison-with-ensemble-spread","position":14},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Grab some Observational time series data for comparison with ensemble spread","lvl2":"Data Loading"},"content":"The observational data we will use is the HadCRUT5 dataset from the UK Met Office\n\nThe data has been downloaded to NCAR’s Research Data Archive (RDA) from \n\nhttps://​www​.metoffice​.gov​.uk​/hadobs​/hadcrut5/\n\nWe will use an OSDF to access this copy from the RDA. Again the links will start with ‘osdf:///’\n\n%%time\nobs_url    = 'osdf:///ncar/rda/d850001/HadCRUT.5.0.2.0.analysis.summary_series.global.monthly.nc'\n#\nobs_ds = xr.open_dataset(obs_url, engine='h5netcdf').tas_mean\nobs_ds\n\n","type":"content","url":"/notebooks/cmip6-gmst#grab-some-observational-time-series-data-for-comparison-with-ensemble-spread","position":15},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Some helpful functions","lvl2":"Data Loading"},"type":"lvl3","url":"/notebooks/cmip6-gmst#some-helpful-functions","position":16},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Some helpful functions","lvl2":"Data Loading"},"content":"\n\ndef drop_all_bounds(ds):\n    drop_vars = [vname for vname in ds.coords\n                 if (('_bounds') in vname ) or ('_bnds') in vname]\n    return ds.drop_vars(drop_vars)\n\ndef open_dset(df):\n    assert len(df) == 1\n    mapper = fsspec.get_mapper(df.zstore.values[0])\n    #path = df.zstore.values[0][7:]+\".zmetadata\"\n    ds = xr.open_zarr(mapper, consolidated=True)\n    return drop_all_bounds(ds)\n\ndef open_delayed(df):\n    return dask.delayed(open_dset)(df)\n\nfrom collections import defaultdict\ndsets = defaultdict(dict)\n\nfor group, df in col_subset.df.groupby(by=['source_id', 'experiment_id']):\n    dsets[group[0]][group[1]] = open_delayed(df)\n\ndsets_ = dask.compute(dict(dsets))[0]\n\n#calculate global means\ndef get_lat_name(ds):\n    for lat_name in ['lat', 'latitude']:\n        if lat_name in ds.coords:\n            return lat_name\n    raise RuntimeError(\"Couldn't find a latitude coordinate\")\n\ndef global_mean(ds):\n    lat = ds[get_lat_name(ds)]\n    weight = np.cos(np.deg2rad(lat))\n    weight /= weight.mean()\n    other_dims = set(ds.dims) - {'time'}\n    return (ds * weight).mean(other_dims)\n\n#calculate global means\ndef get_lat_name(ds):\n    for lat_name in ['lat', 'latitude']:\n        if lat_name in ds.coords:\n            return lat_name\n    raise RuntimeError(\"Couldn't find a latitude coordinate\")\n\ndef global_mean(ds):\n    lat = ds[get_lat_name(ds)]\n    weight = np.cos(np.deg2rad(lat))\n    weight /= weight.mean()\n    other_dims = set(ds.dims) - {'time'}\n    return (ds * weight).mean(other_dims)\n\n","type":"content","url":"/notebooks/cmip6-gmst#some-helpful-functions","position":17},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"GMST computation"},"type":"lvl2","url":"/notebooks/cmip6-gmst#gmst-computation","position":18},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"GMST computation"},"content":"\n\nexpt_da = xr.DataArray(expts, dims='experiment_id', name='experiment_id',\n                       coords={'experiment_id': expts})\n\ndsets_aligned = {}\n\nfor k, v in tqdm(dsets_.items()):\n    expt_dsets = v.values()\n    if any([d is None for d in expt_dsets]):\n        print(f\"Missing experiment for {k}\")\n        continue\n\n    for ds in expt_dsets:\n        ds.coords['year'] = ds.time.dt.year\n\n    # workaround for\n    # https://github.com/pydata/xarray/issues/2237#issuecomment-620961663\n    dsets_ann_mean = [v[expt].pipe(global_mean).swap_dims({'time': 'year'})\n                             .drop_vars('time').coarsen(year=12).mean()\n                      for expt in expts]\n\n    # align everything with the 4xCO2 experiment\n    dsets_aligned[k] = xr.concat(dsets_ann_mean, join='outer',dim=expt_da)\n\n%%time\nwith progress.ProgressBar():\n    dsets_aligned_ = dask.compute(dsets_aligned)[0]\n\nsource_ids = list(dsets_aligned_.keys())\nsource_da = xr.DataArray(source_ids, dims='source_id', name='source_id',\n                         coords={'source_id': source_ids})\n\nbig_ds = xr.concat([ds.reset_coords(drop=True)\n                    for ds in dsets_aligned_.values()],\n                    dim=source_da)\n\nbig_ds\n\nHint\n\nNote that even though the variable is called tas, the DataArray big_ds actually has the global and annual mean of surface temperatures! If you are wondering why this is the case, take a look at all the functions that were applied to obtain dsets_ann_mean!\n\n# Compute annual mean temperatures anomalies of observational data\nobs_gmsta = obs_ds.resample(time='YS').mean(dim='time')\n# obs_gmsta\n\n","type":"content","url":"/notebooks/cmip6-gmst#gmst-computation","position":19},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Compute anomlaies and plot","lvl2":"GMST computation"},"type":"lvl3","url":"/notebooks/cmip6-gmst#compute-anomlaies-and-plot","position":20},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl3":"Compute anomlaies and plot","lvl2":"GMST computation"},"content":"We will compute the temperature anomalies w.r.t 1960-1990 baseline period\n\nConvert xarray datasets to pandas dataframes\n\nUse Seaborn to plot GMSTA\n\ndf_all = big_ds.to_dataframe().reset_index()\ndf_all.head()\n\n# Define the baseline period\nbaseline_df = df_all[(df_all[\"year\"] >= 1960) & (df_all[\"year\"] <= 1990)]\n\n# Compute the baseline mean\nbaseline_mean = baseline_df[\"tas\"].mean()\n\n# Compute anomalies\ndf_all[\"tas_anomaly\"] = df_all[\"tas\"] - baseline_mean\ndf_all\n\nobs_df = obs_gmsta.to_dataframe(name='tas_anomaly').reset_index()\n\n# Convert 'time' to 'year' (keeping only the year)\nobs_df['year'] = obs_df['time'].dt.year\n\n# Drop the original 'time' column since we extracted 'year'\nobs_df = obs_df[['year', 'tas_anomaly']]\nobs_df\n\nAlmost there! Let us now use seaborn to plot all the anomalies\n\ng = sns.relplot(data=df_all, x=\"year\", y=\"tas_anomaly\",\n                hue='experiment_id', kind=\"line\", errorbar=\"sd\", aspect=2, palette=\"Set2\")  # Adjust the color palette)\n\n# Get the current axis from the FacetGrid\nax = g.ax\n\n# Overlay the observational data in red\nsns.lineplot(data=obs_df, x=\"year\", y=\"tas_anomaly\",color=\"red\", \n             linestyle=\"dashed\", linewidth=2,label=\"Observations\", ax=ax)\n\n# Adjust the legend to include observations\nax.legend(title=\"Experiment ID + Observations\")\n\n# Show the plot\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/cmip6-gmst#compute-anomlaies-and-plot","position":21},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/cmip6-gmst#summary","position":22},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Summary"},"content":"In this notebook, we used surface air temperature data from several CMIP6 models for the ‘historical’, ‘SSP245’ and ‘SSP370’ runs to compute Global Mean Surface Temperature Anomaly (GMSTA) relative to the 1960-1990 baseline period and compare it with anomalies computed from the HadCRUT monthly surface temperature dataset. We used a modified intake-ESM catalog and pelicanFS to ‘stream/download’ temperature data from two different OSDF origins. The CMIP6 model data was streamed from the AWS OpenData origin in the us-west-2 region and the observational data was streamed from NCAR’s OSDF origin.\n\n","type":"content","url":"/notebooks/cmip6-gmst#summary","position":23},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/cmip6-gmst#resources-and-references","position":24},{"hierarchy":{"lvl1":"Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data","lvl2":"Resources and references"},"content":"Original notebook in the Pangeo Gallery by Henri Drake and Ryan Abernathey\n\nCMIP6 cookbook by Ryan Abernathey, Henri Drake, Robert Ford and Max Grover\n\nCoupled Model Intercomparison Project 6 was accessed from \n\nhttps://​registry​.opendata​.aws​/cmip6 using a modified intake-ESM catalog hosted on NCAR’s RDA\n\nWe thank the UK Met Office Hadley Center for providing the observational data","type":"content","url":"/notebooks/cmip6-gmst#resources-and-references","position":25},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive"},"type":"lvl1","url":"/notebooks/ncar-intro","position":0},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive"},"content":"\n\n\n\n","type":"content","url":"/notebooks/ncar-intro","position":1},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/ncar-intro#overview","position":2},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Overview"},"content":"The Research Data Archive (RDA) is a large, publicly accessible collection of atmospheric, oceanic, and related geophysical data managed at the National Center for Atmposheric Research (NCAR) sponsored by the National Science Foundation (NSF).\n\nCurrently the Research Data Archive can be visited using the link \n\nhttps://​rda​.ucar​.edu","type":"content","url":"/notebooks/ncar-intro#overview","position":3},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl3":"In this tutorial you will learn:","lvl2":"Overview"},"type":"lvl3","url":"/notebooks/ncar-intro#in-this-tutorial-you-will-learn","position":4},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl3":"In this tutorial you will learn:","lvl2":"Overview"},"content":"Purpose of the RDA\n\nTypes of data\n\nFeatures and tools\n\nAccess and connection to OSDF\n\n\n\n","type":"content","url":"/notebooks/ncar-intro#in-this-tutorial-you-will-learn","position":5},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Purpose of the RDA"},"type":"lvl2","url":"/notebooks/ncar-intro#purpose-of-the-rda","position":6},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Purpose of the RDA"},"content":"The RDA provides free access to curated data for research, with an emphasis on datasets which provide high value to NCAR and member university researchers. Additionally, the RDA provides value added services and tools to help scientists discover, access, and manipulate data. There is also an increasing empahsis on enabling users to stream data to their local computational environment or apply computations directly to data.\n\n","type":"content","url":"/notebooks/ncar-intro#purpose-of-the-rda","position":7},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Types of Data"},"type":"lvl2","url":"/notebooks/ncar-intro#types-of-data","position":8},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Types of Data"},"content":"The RDA data has a wide variety of datasets that cover many different domains of geosciences including:\n\nGlobal and regional reanalysis datasets (e.g., \n\nERA5, \n\nNCEP/NCAR, \n\nJRA-3Q)\n\nNumerical weather prediction model output (e.g. \n\nGFS)\n\nObservational data (e.g., surface, radiosonde, satellite)\n\nClimate model simulations (e.g., \n\nCESM)\n\nOceanographic datasets (e.g. \n\nICOADS)","type":"content","url":"/notebooks/ncar-intro#types-of-data","position":9},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl3":"File Formats","lvl2":"Types of Data"},"type":"lvl3","url":"/notebooks/ncar-intro#file-formats","position":10},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl3":"File Formats","lvl2":"Types of Data"},"content":"NetCDF4\n\nGrib\n\nProprietary binary\n\nBUFR\n\nZarr\n\nKerchunk\n\n","type":"content","url":"/notebooks/ncar-intro#file-formats","position":11},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Features and Tools"},"type":"lvl2","url":"/notebooks/ncar-intro#features-and-tools","position":12},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Features and Tools"},"content":"\n\nSearch and filtering tools for finding datasets\n\nData subsetting and format conversion\n\nDocumentation and metadata for reproducibility\n\nAPIs and scripts for automated access\n\n","type":"content","url":"/notebooks/ncar-intro#features-and-tools","position":13},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Access and OSDF"},"type":"lvl2","url":"/notebooks/ncar-intro#access-and-osdf","position":14},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Access and OSDF"},"content":"All datasets are open and free to download, however registration (via ORCID) is required for subset requests.\n\nIf you made it this far, you might wonder what the RDA has to do with the Open Science Data Federation (OSDF)?\n\nA:\nThe RDA is a member of the OSDF and its data holdings are served via an origin.\nThe implications are that subsequent requests of the same data will result in lower latency as data will be stored at a geographically close cache.\n\n\n\n","type":"content","url":"/notebooks/ncar-intro#access-and-osdf","position":15},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/ncar-intro#summary","position":16},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Summary"},"content":"In essence, the NCAR RDA is a vital resource for the climate and weather research community, providing long-term, reliable access to high-quality Earth system data.","type":"content","url":"/notebooks/ncar-intro#summary","position":17},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/ncar-intro#whats-next","position":18},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl3":"What’s next?","lvl2":"Summary"},"content":"RDA is currently undergoing a rebranding effort and will soon be renamed GDEX, which stands for Geoscience Data Exchange. Essentially, all references to RDA can be replaced with GDEX as of Sep 9, 2025.\n\n","type":"content","url":"/notebooks/ncar-intro#whats-next","position":19},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/ncar-intro#resources-and-references","position":20},{"hierarchy":{"lvl1":"Introduction to the Research Data Archive","lvl2":"Resources and references"},"content":"https://​rda​.ucar​.edu/\n\nContact \n\nrdahelp@ucar.edu if you want to learn more about the RDA.","type":"content","url":"/notebooks/ncar-intro#resources-and-references","position":21},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters"},"type":"lvl1","url":"/notebooks/envistor-foundations","position":0},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters"},"content":"This notebook introduces a user-facing science workflow that explores salinity patterns in South Florida using curated buoy data published through the EnviStor smart data pipeline and made accessible via PelicanFS as part of the Open Science Data Federation (OSDF).\n\nEnviStor is an AI-assisted, modular data pipeline developed at Florida International University (FIU) to process and publish environmental datasets. It automates tasks like file type detection, metadata generation, geospatial transformations, and dataset publication. For this use case, EnviStor ingested buoy data, processed and cleaned it, and published it through the OSDF federation, making it available to science users via PelicanFS.\n\n\n\n","type":"content","url":"/notebooks/envistor-foundations","position":1},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/envistor-foundations#overview","position":2},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Overview"},"content":"This notebook introduces a real-world scientific workflow built around the question: “What are the salinity patterns in South Florida?” Using curated data from FIU’s buoy network—processed by the EnviStor smart pipeline and accessed through OSDF using PelicanFS, we explore how environmental datasets can be made reproducible, discoverable, and usable by domain scientists.\n\nBelow is a breakdown of what this notebook covers:\n\n1. What is Salinity?\nA brief introduction to salinity, its units, and its role in coastal science.\n\n2. Why South Florida?\nContext around why this region is important for salinity monitoring, including environmental and societal implications.\n\n3. About the Dataset\nDescription of the FIU CREST buoy network, the variables collected, and how the data is prepared through EnviStor.\n\n4. Data Access with PelicanFS\nExplanation of how the dataset is accessed directly from OSDF using PelicanFS, and how that supports open science.\n\n5. Research Question\nA clear framing of the central question that will be explored in the analysis notebook.\n\n6. What’s Next?\nA preview of what readers will do in the second notebook: loading, analyzing, and visualizing the data.\n\nBy the end of this notebook, the reader should understand the scientific motivation, the data context, and how curated environmental data made available via OSDF can be leveraged in reproducible Jupyter-based workflows.\n\n","type":"content","url":"/notebooks/envistor-foundations#overview","position":3},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/envistor-foundations#prerequisites","position":4},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Prerequisites"},"content":"To fully understand the context and content of this notebook, readers should be familiar with a few foundational concepts and tools. While this notebook itself is explanatory and doesn’t run any code, it introduces ideas and data structures that are implemented in the accompanying technical notebook.\n\nBelow is a table of key concepts and how important they are for this material:\n\nConcepts\n\nImportance\n\nNotes\n\nWhat is Salinity?\n\nNecessary\n\nCore environmental concept used in the notebook\n\nIntro to Buoy-based monitoring\n\nHelpful\n\nUnderstanding how buoy data is collected\n\nPelicanFS Overview\n\nNecessary\n\nExplains how the curated data is accessed from OSDF\n\nWhat is OSDF\n\nHelpful\n\nHigh-level context for how EnviStor data is shared\n\nTime to learn: 20-30 minutes. (~5–10 minutes per external concept; extra time optional for those new to Pelican/OSDF)\n\n\n\n","type":"content","url":"/notebooks/envistor-foundations#prerequisites","position":5},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"What is Salinity?"},"type":"lvl2","url":"/notebooks/envistor-foundations#what-is-salinity","position":6},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"What is Salinity?"},"content":"Salinity measures how much salt is dissolved in water, typically in Practical Salinity Units (PSU).\n\nSalinity influences ocean circulation, marine life, and freshwater availability. In coastal regions like South Florida, it’s a key indicator of environmental change, particularly for detecting saltwater intrusion into freshwater systems.\n\n","type":"content","url":"/notebooks/envistor-foundations#what-is-salinity","position":7},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Why South Florida?"},"type":"lvl2","url":"/notebooks/envistor-foundations#why-south-florida","position":8},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Why South Florida?"},"content":"South Florida is particularly sensitive to salinity changes due to its mix of freshwater inflows, tidal dynamics, sea level rise, and storm surge events. Monitoring salinity helps scientists detect:\n\nSaltwater intrusion into aquifers\n\nSeasonal or storm-related shifts in water quality\n\nLong-term climate-driven changes\n\n","type":"content","url":"/notebooks/envistor-foundations#why-south-florida","position":9},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"About the Dataset"},"type":"lvl2","url":"/notebooks/envistor-foundations#about-the-dataset","position":10},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"About the Dataset"},"content":"This dataset comes from FIU’s CREST buoy network, which continuously measures water quality parameters including salinity, temperature, turbidity, and more.\n\nFor this analysis, we selected three stations:\n\nBuoy 2: NW Biscayne Bay\n\nBuoy 3: Haulover Inlet\n\nBuoy 3-2: Little River\n\nThese datasets were processed through the EnviStor smart pipeline, which cleaned, standardized, and published them into the Open Science Data Federation (OSDF). The data is now directly accessible using PelicanFS, a high-performance data access layer.\n\n","type":"content","url":"/notebooks/envistor-foundations#about-the-dataset","position":11},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Data Access with PelicanFS"},"type":"lvl2","url":"/notebooks/envistor-foundations#data-access-with-pelicanfs","position":12},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Data Access with PelicanFS"},"content":"The curated data is available in the /envistor namespace on OSDF. We will access it directly in the next notebook using PelicanFS, a high-performance file system interface developed by the OSG and Pathfinders community.\n\nNo pre-downloaded files are needed — all data will be loaded live from OSDF via PelicanFS.\n\nNote\n\nIf you’re running this notebook as part of the Pythia Cook-off, PelicanFS is already set up in the environment — no extra steps are needed.\n\nHowever, if you’re adapting this workflow for use outside the Cook-off platform, you’ll need to install and mount PelicanFS on your system to access data from OSDF. For instructions, see the \n\nPelican documentation.\n\n","type":"content","url":"/notebooks/envistor-foundations#data-access-with-pelicanfs","position":13},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Research Question"},"type":"lvl2","url":"/notebooks/envistor-foundations#research-question","position":14},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Research Question"},"content":"We aim to explore:“How does water salinity vary across time and space in South Florida coastal waters?”\n\nThis question is central to understanding environmental patterns related to:\n\nSeasonality (e.g., wet vs. dry season)\n\nFreshwater discharge (e.g., from canals or rivers)\n\nSaltwater intrusion (e.g., due to sea level rise or storm surge)\n\nTo answer it, we will compare salinity data collected from three distinct coastal monitoring stations (NW Biscayne Bay, Haulover Inlet, and Little River) over a multi-month period. These stations span both urban and natural areas, helping us examine spatial variation.\n\nBy plotting and analyzing this data over time, we can begin to identify how salinity responds to both natural processes and human-driven impacts — providing insights useful for coastal management and long-term monitoring.\n\n\n\n","type":"content","url":"/notebooks/envistor-foundations#research-question","position":15},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/envistor-foundations#summary","position":16},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl2":"Summary"},"content":"In this notebook, we explored the motivation, context, and scientific importance of analyzing salinity patterns in South Florida. We introduced the curated buoy datasets processed through the EnviStor smart data pipeline and made accessible via PelicanFS, part of the Open Science Data Federation. The reader now understands why salinity matters, how the data was collected and prepared, and what question the upcoming technical notebook will answer. This context sets the stage for a reproducible, user-facing environmental analysis powered by FAIR data infrastructure.","type":"content","url":"/notebooks/envistor-foundations#summary","position":17},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl3":"What’s Next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/envistor-foundations#whats-next","position":18},{"hierarchy":{"lvl1":"Exploring Salinity Patterns in South Florida Coastal Waters","lvl3":"What’s Next?","lvl2":"Summary"},"content":"In the next notebook, we will:\n\nLoad salinity datasets for three buoy stations (NW Biscayne Bay, Haulover Inlet, and Little River) directly from OSDF via PelicanFS.\n\nCombine and clean the data: We’ll merge the datasets into a single time series, convert timestamps, label each station, and handle any missing or invalid salinity readings.\n\nResample to daily averages to reduce noise and highlight broader trends.\n\nVisualize the results using line plots to show salinity over time for each location, making it easy to compare patterns across stations.\n\nAnalyze temporal and spatial patterns, such as seasonal variations, sudden shifts, or location-specific behaviors that may suggest environmental changes like saltwater intrusion or storm impact.\n\nThis notebook will give users a complete, reproducible workflow that demonstrates how curated environmental data can be analyzed using standard Python tools — all made possible by the EnviStor pipeline and OSDF infrastructure.","type":"content","url":"/notebooks/envistor-foundations#whats-next","position":19},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida"},"type":"lvl1","url":"/notebooks/envistor-technical","position":0},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida"},"content":"This notebook demonstrates how to analyze salinity patterns in South Florida using curated buoy data processed by the EnviStor smart data pipeline and made available through PelicanFS, a high-performance file system interface for the Open Science Data Federation (OSDF).\n\nThe data used here comes from three monitoring stations managed by FIU, each located in a distinct part of South Florida’s coastal waters. These datasets were curated and made analysis-ready by the EnviStor pipeline.\n\nThe central question is:\n\nWhat are the salinity patterns in South Florida?\n\nWe’ll answer this by loading the data, cleaning and merging it, and visualizing salinity trends over time across multiple locations.\n\n\n\n","type":"content","url":"/notebooks/envistor-technical","position":1},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/envistor-technical#imports","position":2},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Imports"},"content":"\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pelicanfs.core import OSDFFileSystem\nfrom io import BytesIO\n\nsns.set(style=\"whitegrid\")\n\n","type":"content","url":"/notebooks/envistor-technical#imports","position":3},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Load the Curated Salinity Datasets"},"type":"lvl2","url":"/notebooks/envistor-technical#load-the-curated-salinity-datasets","position":4},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Load the Curated Salinity Datasets"},"content":"We’ll load data from three buoy stations directly from PelicanFS using the OSDFFileSystem() interface. These Excel files have already been curated and prepared by the EnviStor smart pipeline, and are stored in the /envistor namespace in OSDF.\n\nEach file corresponds to a different buoy location:\n\nBiscayne Bay\n\nHaulover Inlet\n\nLittle River\n\nWe use BytesIO to read the content as a stream before passing it to pandas.read_excel(). Each resulting DataFrame includes a \"Station\" column to identify its source location.\n\npelfs = OSDFFileSystem()\nfile_buoy1 = pelfs.cat('/envistor/CREST_Buoy_2_NW_Biscayne_Bay_-_S_of_Biscayne_Canal_082720-112221.xlsx')\nfile_buoy2 = pelfs.cat('/envistor/CREST_Buoy_3_Haulover_Inlet_100518_-_073020_updated.xlsx')\nfile_buoy3 = pelfs.cat('/envistor/CREST_Buoy_3-2_Little_River_042121-050624.xlsx')\n\nexcel_file1 = BytesIO(file_buoy1)\ndf_file_buoy1 = pd.read_excel(excel_file1)\ndf_file_buoy1['Station'] = 'Buoy - Biscayne Bay'\n\nexcel_file2 = BytesIO(file_buoy2)\ndf_file_buoy2 = pd.read_excel(excel_file2)\ndf_file_buoy2['Station'] = 'Buoy - Haulover Inlet'\n\nexcel_file3 = BytesIO(file_buoy3)\ndf_file_buoy3 = pd.read_excel(excel_file3)\ndf_file_buoy3['Station'] = 'Little River'\n\n\n","type":"content","url":"/notebooks/envistor-technical#load-the-curated-salinity-datasets","position":5},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Clean and Combine the Data"},"type":"lvl2","url":"/notebooks/envistor-technical#clean-and-combine-the-data","position":6},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Clean and Combine the Data"},"content":"Now that we’ve loaded the individual DataFrames, we’ll prepare them for analysis.\n\nHere’s what we do:\n\nCreate a timestamp: Combine the \"Date\" and \"Time\" columns into a single datetime column.\n\nStandardize salinity: Rename the \"Sal_psu\" column to \"Salinity\" and convert its values to numeric (in case of string or error-prone entries).\n\nMerge datasets: Concatenate the three cleaned DataFrames into one (df_all) so we can analyze salinity trends across all buoy stations together. We also drop any rows with missing salinity values and set the datetime column as the index to enable time-based operations later.\n\nfor df in [df_file_buoy1, df_file_buoy2, df_file_buoy3]:\n    df[\"datetime\"] = pd.to_datetime(df[\"Date\"].astype(str) + \" \" + df[\"Time\"].astype(str))\n\n    df.rename(columns={\"Sal_psu\": \"Salinity\"}, inplace=True)\n    df[\"Salinity\"] = pd.to_numeric(df[\"Salinity\"], errors=\"coerce\")\n\ndf_all = pd.concat([df_file_buoy1, df_file_buoy2, df_file_buoy3], ignore_index=True)\ndf_all.dropna(subset=[\"Salinity\"], inplace=True)\ndf_all.set_index(\"datetime\", inplace=True)\n\n\n","type":"content","url":"/notebooks/envistor-technical#clean-and-combine-the-data","position":7},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Resample and Aggregate"},"type":"lvl2","url":"/notebooks/envistor-technical#resample-and-aggregate","position":8},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Resample and Aggregate"},"content":"Salinity readings are taken multiple times per day. To reveal broader trends, we resample the data to daily averages.\n\nThis step:\n\nReduces noise\n\nMakes it easier to compare across time\n\nPrepares the data for visualization\n\nWe group by station and resample by day ('1D').\n\ndf_daily = (\n    df_all\n    .groupby(\"Station\")\n    .resample(\"1D\")\n    [\"Salinity\"]\n    .mean()\n    .reset_index()\n)\n\n\n","type":"content","url":"/notebooks/envistor-technical#resample-and-aggregate","position":9},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Visualize the Salinity Patterns"},"type":"lvl2","url":"/notebooks/envistor-technical#visualize-the-salinity-patterns","position":10},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Visualize the Salinity Patterns"},"content":"Now we can plot daily salinity patterns to compare how they evolve over time across the three locations.\n\nWe’ll use Seaborn for a clean, readable line chart.\n\nplt.figure(figsize=(14, 6))\nsns.lineplot(data=df_daily, x=\"datetime\", y=\"Salinity\", hue=\"Station\")\nplt.title(\"Daily Average Salinity in South Florida (by Buoy Station)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Salinity (psu)\")\nplt.legend(title=\"Station\")\nplt.tight_layout()\nplt.show()\n\n\nNote\n\n💡 Why this is possible\n\nThis visualization is only possible thanks to the EnviStor Smart Pipeline, which curated, cleaned, and enriched the buoy data, and published it to OSDF.\n\nAdditionally, the Pelican platform allowed us to access the data on-demand using PelicanFS — no local downloads or manual data wrangling required. This is a great example of how data infrastructure can directly support scientific insight.\n\n","type":"content","url":"/notebooks/envistor-technical#visualize-the-salinity-patterns","position":11},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Interpret the Results"},"type":"lvl2","url":"/notebooks/envistor-technical#interpret-the-results","position":12},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Interpret the Results"},"content":"With the visualization in hand, we can start identifying key salinity patterns in South Florida:\n\nBiscayne Bay (blue) shows a clear seasonal fluctuation, with higher salinity in dry months and noticeable drops likely linked to storm events or freshwater inflow.\n\nHaulover Inlet (orange) tends to have consistently higher salinity levels, suggesting stronger tidal mixing and less influence from freshwater discharge.\n\nLittle River (green) displays the most variability — sharp dips and spikes in salinity hint at frequent freshwater input, possibly from canals, rain events, or upstream runoff.\n\nThese differences illustrate how geographic location and local hydrology impact salinity levels. By comparing trends across stations, we gain insight into how dynamic and localized coastal salinity can be.\n\nNote\n\nThis type of analysis can support environmental monitoring, resource management, and research on saltwater intrusion and estuarine health.\n\n","type":"content","url":"/notebooks/envistor-technical#interpret-the-results","position":13},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Next Steps"},"type":"lvl2","url":"/notebooks/envistor-technical#next-steps","position":14},{"hierarchy":{"lvl1":"Analyzing Salinity Patterns in South Florida","lvl2":"Next Steps"},"content":"This notebook provided a foundation for analyzing coastal salinity patterns using curated data from the EnviStor pipeline. If you’re interested in extending this work, here are a few ideas:\n\nIncorporate other environmental variables: Analyze how temperature, turbidity, or dissolved oxygen vary alongside salinity to build a more holistic view of water quality.\n\nAdd spatial analysis: Use GIS tools or libraries (e.g., Cartopy or Folium) to visualize station locations and explore spatial gradients in salinity.\n\nCompare across years: Investigate long-term salinity trends and identify anomalies across different seasons or years.\n\nInclude other datasets from EnviStor: Expand the workflow by pulling additional datasets from OSDF, such as ocean currents, rainfall, or metadata-enriched observations.\n\nDevelop alert thresholds: Identify salinity levels that may signal ecological stress or risk, potentially integrating this with decision-making tools.\n\nHint\n\nCurious about how the data got so clean? Check out the \n\nEnviStor smart pipeline’s role in preparing these files — from metadata tagging to anomaly filtering — to better understand the power of backend automation.","type":"content","url":"/notebooks/envistor-technical#next-steps","position":15},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data"},"type":"lvl1","url":"/notebooks/sonarai-foundations","position":0},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data"},"content":"This notebook provides a workflow to access NOAA water column sonar data and integrate and co-visualize with water temperature from a near-by National Data Buoy Center buoy, made accessible via PelicanFS as part of the Open Science Data Federation (OSDF).\n\nBy integrating these two datasets, we aim to provide context on the oceanographic conditions when the sonar data were collected, and more specifically provide information on the environment that the ensonified marine organisms inhabited.\n\n\n\n","type":"content","url":"/notebooks/sonarai-foundations","position":1},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/sonarai-foundations#overview","position":2},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Overview"},"content":"Below describes the main topics this notebook will cover\n\nWhat are water column sonar data? An overview of these data, how they are collected, who collects them, their role supporting federal resource management.\n\nWhy including oceanographic data is important? Context on what influences the marine organisms captured by the sonar systems.\n\nAbout the sonar dataset. Specifics on the sonar data selected for this notebook, including the format, preprocessing steps, and importance\n\nAbout the buoy dataset. Specifics on the location and temporal resolution for the temperature datasets selected for this notebook.\n\nAccess from PelicanFS. Where, how, and why to access data from PelicanFS.\n\nWhat are our science drivers? Additional scientific motivation that is driving the project’s goals and impact to the scientific community\n\nWhat’s next? A brief look to the content of the next notebook focused on visualization.\n\nBy the end, readers will have a better understanding of what sonar data are, why integrating other oceangraphic data are important, and how to do that -- all in a Jupyter workflow that leverages data accessible through the OSDF data!\n\n","type":"content","url":"/notebooks/sonarai-foundations#overview","position":3},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/sonarai-foundations#prerequisites","position":4},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nWhat is split-beam sonar?\n\nNecessary\n\nFoundational concepts for the sonar data\n\nSonar applications for NOAA\n\nHelpful\n\nAdditional information to understand how and why NOAA collects water column sonar data\n\nUses of buoy data\n\nHelpful\n\nInformation on ocean buoys and what they are used for\n\nWhat is the Pelican Platform?\n\nHelpful\n\nFamiliarity with the Pelican Project\n\nWhat is the OSDF?\n\nHelpful\n\nFamiliarity with the Open Science Data Federation\n\nTime to learn: Approximately 35 minutes. More if you’re new to the concepts or extra curious!\n\n\n\n","type":"content","url":"/notebooks/sonarai-foundations#prerequisites","position":5},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"What are water column sonar data?"},"type":"lvl2","url":"/notebooks/sonarai-foundations#what-are-water-column-sonar-data","position":6},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"What are water column sonar data?"},"content":"\n\nThe health of our ocean ecosystems are vital to global economies. One way to learn more about our ocean is to examine the water column, which is the volume of water from the ocean surface to the ocean floor.\n\nfrom IPython.display import Image, display\ndisplay(Image(filename='../images/water column zones.png'))\n\nThe water column of the open ocean is divided into five zones from the surface to the seafloor. Each zone varies in pressure, light, temperature, oxygen, nutrients, and biological diversity. Credit: Barbara Ambrose, NOAA, from the \n\nUnderstanding Our Ocean with Water-Column Sonar Data story map\n\nScientists collect data on the water column using sonars. These instruments emit sound (“a ping”) at set frequencies that travels down through the water to the seafloor, bouncing or “scattering” off whatever is in it’s path. The instruments “listen” for the time and angle of return of the ping to capture information on the pings journey. If you’ve ever been on a boat that had a fish finder beeping away, these instruments are very similar - just more complex and more expensive. They’re scientific fish finders!\n\ndisplay(Image(filename='../images/sonar graphic.png'))\n\nSounds waves transmitted from ships using sonar instruments reflect back to the ship when they have hit an object(s), such as a school of fish. Credit: Barbara Ambrose, NOAA, from the \n\nUnderstanding Our Ocean with Water-Column Sonar Data story map\n\n","type":"content","url":"/notebooks/sonarai-foundations#what-are-water-column-sonar-data","position":7},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl3":"Supporting NOAA’s mission","lvl2":"What are water column sonar data?"},"type":"lvl3","url":"/notebooks/sonarai-foundations#supporting-noaas-mission","position":8},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl3":"Supporting NOAA’s mission","lvl2":"What are water column sonar data?"},"content":"\n\nNOAA Fisheries routinely use this technology to identify fish schools and estimate biomass for fisheries stock assessments.\n\ndisplay(Image(filename='../images/Fisheries_Science-to-Mangaement.png'))\n\nVisualization of the NOAA Fisheries science to management workflow. Credit \n\nNOAA Fisheries website\n\nWater column sonar systems are integrated on NOAA Fisheries Survey Vessels that traverse the waters of every coast.\n\ndisplay(Image(filename='../images/Schematics_ActiveAcoustics.jpg'))\n\nSchematic of NOAA Fisheries survey vessel using water column sonar to map fish. Credit \n\nNOAA Fisheries\n\n","type":"content","url":"/notebooks/sonarai-foundations#supporting-noaas-mission","position":9},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl3":"Centralized repositories and cloud access","lvl2":"What are water column sonar data?"},"type":"lvl3","url":"/notebooks/sonarai-foundations#centralized-repositories-and-cloud-access","position":10},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl3":"Centralized repositories and cloud access","lvl2":"What are water column sonar data?"},"content":"Because of their value to our nation, NOAA and other agencies water column sonar data are stewarded at the \n\nNOAA Water Column Sonar Data Archive. This archive currently holds over 350 TB of data collected over 20 years in all areas of the U.S. Exclusive Economic Zone. A copy of these archived data are accessible on Amazon Web Services (AWS) through the NOAA Open Data Dissemination Program.\n\n","type":"content","url":"/notebooks/sonarai-foundations#centralized-repositories-and-cloud-access","position":11},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Why including oceanographic data is important?"},"type":"lvl2","url":"/notebooks/sonarai-foundations#why-including-oceanographic-data-is-important","position":12},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Why including oceanographic data is important?"},"content":"\n\nWater column sonars provide information about fish and zooplankton inhabiting our waters. These marine organisms are heavily influenced by their surrounding ocenaographic conditions.\n\nKey drivers include\n\ntemperature\n\nsalinity\n\ndissoloved oxygen\n\nproductivity\n\ncurrents\n\n","type":"content","url":"/notebooks/sonarai-foundations#why-including-oceanographic-data-is-important","position":13},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"About the sonar dataset"},"type":"lvl2","url":"/notebooks/sonarai-foundations#about-the-sonar-dataset","position":14},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"About the sonar dataset"},"content":"\n\nRaw sonar files are complex and binary. To make them more accessible, analysis-ready and cloud-optimized, a subset of the data have been converted into \n\nZarr stores. The team is currently focusing on \n\nEK60 sonar systems run on the \n\nNOAA Ship Henry B. Bigelow by the \n\nNOAA Northeast Fisheries Science Center (NEFSC). NEFSC’s main objective for collecting these data is to determine the biomass of Atlantic herring Clupea harengus, which contributes to the commercial lobster industry.\n\nYou can explore the Zarr translated files using \n\nEchoFish, the team’s AWS-hosted interactive portal for data exploration.\n\nWe have selected a subset of data from NEFSC’s HB1906 survey, specifically from October 16, 2019.\n\ndisplay(Image(filename='../images/HB1906.png'))\n\nGeographic location for the NEFSC HB1906 cruise off the northeast coast of the U.S. Credit \n\nNOAA Water Column Sonar Data Archive\n\ndisplay(Image(filename='../images/HB1906_16Oct2019_afternoon_labeled.png'))\n\nVisualization of the HB1906 38 kHz sonar data used in the analysis.\n\n","type":"content","url":"/notebooks/sonarai-foundations#about-the-sonar-dataset","position":15},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"About the buoy dataset"},"type":"lvl2","url":"/notebooks/sonarai-foundations#about-the-buoy-dataset","position":16},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"About the buoy dataset"},"content":"\n\nWe will pull data from the \n\nNDBC stion 44005 located on George’s Bank near Nantuck Shoals. This buoy records multiple variables continuously from it’s moored location.\n\nSome of the data available on this buoy recorded at the ocean surface include\n\nwater temperature\n\nwind speed, direction, and gust\n\nwave height and direction\n\nair temperature\n\n","type":"content","url":"/notebooks/sonarai-foundations#about-the-buoy-dataset","position":17},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Access from PelicanFS"},"type":"lvl2","url":"/notebooks/sonarai-foundations#access-from-pelicanfs","position":18},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Access from PelicanFS"},"content":"\n\nWe will access it directly in the next notebook using PelicanFS, a high-performance file system interface developed by the \n\nOSG and Pathfinders community.\n\nAdditional details of PelicanFS can be found in Chapter 1 - PelicanFS\n\n","type":"content","url":"/notebooks/sonarai-foundations#access-from-pelicanfs","position":19},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"What are our science drivers?"},"type":"lvl2","url":"/notebooks/sonarai-foundations#what-are-our-science-drivers","position":20},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"What are our science drivers?"},"content":"\n\nWe aim to explore the correlation between patterns extracted from the sonar data and the associated ocean temperature.\n\nAs we expand our workflow to longer time periods, larger areas, and multiple years, we will be able to further examine\n\nSeasonal and interannual variability in biological assemblages (i.e., fish and zooplankton)\n\nSpatial variability in biological assemblages\n\nThe influence of water temperature on marine organisms\n\n\n\n","type":"content","url":"/notebooks/sonarai-foundations#what-are-our-science-drivers","position":21},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/sonarai-foundations#summary","position":22},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Summary"},"content":"Throughout this notebook, we introduced the objectives, approach, and value of integrating water column sonar with oceanographic data. We reviewed the collection methods and availability of both sonar and buoy datasets, as well as their accessibility through PelicanFS. Readers should now understand what water column sonar data are and why they are important to NOAA, resource management, and supporting healthy oceans. They will also be familiar with oceanographic datasets that can provide additional context for interpreting patterns of marine organisms observed in the sonar data.\n\n","type":"content","url":"/notebooks/sonarai-foundations#summary","position":23},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/sonarai-foundations#resources-and-references","position":24},{"hierarchy":{"lvl1":"Integrating sonar and buoy-based temperature data","lvl2":"Resources and references"},"content":"NOAA Water Column Sonar Data\n\nNortheast Ecosystem Surveys","type":"content","url":"/notebooks/sonarai-foundations#resources-and-references","position":25},{"hierarchy":{"lvl1":"Notebook Structure"},"type":"lvl1","url":"/notebooks/sonarai-technical","position":0},{"hierarchy":{"lvl1":"Notebook Structure"},"content":"\n\n","type":"content","url":"/notebooks/sonarai-technical","position":1},{"hierarchy":{"lvl1":"Notebook Structure"},"type":"lvl1","url":"/notebooks/sonarai-technical#notebook-structure","position":2},{"hierarchy":{"lvl1":"Notebook Structure"},"content":"This notebook walks through an end-to-end workflow to relate shipboard sonar backscatter (Sv) to local environmental conditions. We (1) open EK60 data from a public NOAA S3 Zarr, (2) gather co-located environmental variables from OISST and IOOS ERDDAP, (3) compute hourly mean Sv, (4) assemble a depth×time error map for reference, and (5) synchronize timestamps to produce an interactive line-plus-heatmap visualization. All selections (time/depth/frequency) and conversions are kept explicit for reproducibility.\n\nImports\nLoad core libraries for data access (xarray, s3fs), analysis (numpy, pandas), plotting (plotly), and I/O.\n\nInitializing the datasets\nAccess HB1906 EK60 Zarr data from public S3; subset by time/depth, select 38 kHz, and mask bins below bottom.\n\nAccess buoy data\nDefine Georges Bank buoy coordinates, sample daily OISST SST at the nearest grid cell (±1 day), and download the model error map (.npy).\n\nCalculate the temperature anomaly, sun elevation in degree and azimuth\n\nDownloading external error map for the specific location\nDownloading the error map comes from a fixed file\n\nHelper Function: Mean Sv (dB)\nConvert Sv from dB→linear, compute mean, convert back to dB.\n\nGroup Cruise Data into Hourly Chunks\nAdd an hourly label and split the EK60 dataset into per-hour xarray.Dataset chunks.\n\nCompute Hourly Mean Sv & Attach to env_df\nAggregate Sv per hour and append results as a new column in the environmental dataframe.\n\nBuild Depth×Time Error-Map DataFrame & Align Timestamps\nConstruct a depth-by-time matrix from the error map, guard for size mismatches, and align env_df endpoints to the heatmap timestamps.\n\nData Visualization: Synchronized Lines + Heatmap\nPlot environmental time series above a depth×time heatmap with shared x-axis; save interactive HTML output.\n\n\n\n","type":"content","url":"/notebooks/sonarai-technical#notebook-structure","position":3},{"hierarchy":{"lvl1":"Prerequisites"},"type":"lvl1","url":"/notebooks/sonarai-technical#prerequisites","position":4},{"hierarchy":{"lvl1":"Prerequisites"},"content":"This section was inspired by \n\nthis template of the wonderful \n\nThe Turing Way Jupyter Book.\n\nThis notebook opens public NOAA EK60 Zarr data from S3, subsets by time/depth, reads daily OISST SST near a buoy, fetches ERDDAP environmental variables, computes hourly mean Sv (dB), aligns with a depth×time error map, and renders synchronized line/heatmap plots.\n\nLabel the importance of each concept explicitly as helpful/necessary.\n\nConcepts\n\nImportance\n\nNotes\n\nXarray + Zarr basics\n\nNecessary\n\nOpening Zarr stores, selecting by coords/dims, .compute() semantics\n\ns3fs & public S3 access\n\nNecessary\n\nAnonymous reads from AWS S3 (anon=True)\n\nPandas time series\n\nNecessary\n\nDatetimeIndex, sorting, filtering, timezone-naive vs. aware\n\nNumPy fundamentals\n\nNecessary\n\nArray slicing, stats, type conversion\n\nAcoustic backscatter (Sv) & dB averaging\n\nNecessary\n\nConvert dB→linear, mean, then linear→dB, Understanding results\n\nERDDAP tabledap & info endpoints\n\nHelpful\n\nReading CSV responses; unit metadata lookup\n\nPlotly fundamentals\n\nHelpful\n\nSubplots, heatmaps, interactive HTML export\n\nUnderstanding of NetCDF/CF\n\nHelpful\n\nVariable metadata and geospatial conventions\n\nDask awareness\n\nHelpful\n\nLazy arrays; when/why to call .compute()\n\nGeographic coordinates\n\nHelpful\n\n0–360 vs. −180–180 longitude handling\n\nHTTP/IO with requests\n\nHelpful\n\nDownloading .npy assets for local use\n\nTime to learn: ~75 minutes\n\nSystem requirements:\n\nPython 3.9+ with Jupyter Notebook/Lab\n\nRequired packages: xarray, s3fs, numpy, pandas, plotly, requests, netCDF4 (optional but helpful: dask)\n\nExecute the script below to install all required packages\n\nimport os\nimport sys\nimport yaml\nimport shutil\nimport subprocess\n\nENV_PATH = \"../environment.yml\"\n\nwith open(ENV_PATH, \"r\", encoding=\"utf-8\") as f:\n    env = yaml.safe_load(f)\n\nchannels = env.get(\"channels\", [])\ndeps = env.get(\"dependencies\", [])\n\nconda_pkgs = []\npip_pkgs = []\n\nfor dep in deps:\n    if isinstance(dep, str):\n        # Skip python pin and the literal 'pip' meta-package entry\n        name = dep.split(\"=\")[0].strip().lower()\n        if name in {\"python\", \"pip\"}:\n            continue\n        conda_pkgs.append(dep)\n    elif isinstance(dep, dict) and \"pip\" in dep:\n        pip_pkgs.extend(dep[\"pip\"])\n\n# Prefer mamba if present; fallback to conda\nconda_exe = shutil.which(\"mamba\") or shutil.which(\"conda\")\n\n# Install Conda packages\nif conda_pkgs:\n    if not conda_exe:\n        raise RuntimeError(\"Conda/mamba not found in PATH. Run this inside a Conda environment.\")\n    # Install into the current environment prefix\n    env_prefix = os.environ.get(\"CONDA_PREFIX\", sys.prefix)\n    cmd = [conda_exe, \"install\", \"-y\", \"-p\", env_prefix]\n    for ch in channels:\n        cmd += [\"-c\", ch]\n    cmd += conda_pkgs\n    print(\"Running:\", \" \".join(cmd))\n    subprocess.check_call(cmd)\nelse:\n    print(\"No Conda packages to install.\")\n\n# Install pip packages into the current kernel's Python\nif pip_pkgs:\n    cmd = [sys.executable, \"-m\", \"pip\", \"install\", *pip_pkgs]\n    print(\"Running:\", \" \".join(cmd))\n    subprocess.check_call(cmd)\nelse:\n    print(\"No pip packages to install.\")\n\n\n\n\n","type":"content","url":"/notebooks/sonarai-technical#prerequisites","position":5},{"hierarchy":{"lvl1":"1) Imports"},"type":"lvl1","url":"/notebooks/sonarai-technical#id-1-imports","position":6},{"hierarchy":{"lvl1":"1) Imports"},"content":"Core libraries used throughout the notebook.\n\nKey roles:xarray/s3fs for reading NOAA Zarr data from S3\nnumpy/pandas for arrays & tables\nplotly for interactive plotting\nrequests/io/os for file I/O and downloads\ndatetime for time calculations\n\nimport xarray as xr\nimport s3fs\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport pandas as pd\nimport requests\nimport io\nimport os\nimport rioxarray\nfrom datetime import datetime, timedelta\nimport pvlib\n\n","type":"content","url":"/notebooks/sonarai-technical#id-1-imports","position":7},{"hierarchy":{"lvl1":"2) Initializing the datasets"},"type":"lvl1","url":"/notebooks/sonarai-technical#id-2-initializing-the-datasets","position":8},{"hierarchy":{"lvl1":"2) Initializing the datasets"},"content":"\n\nBuilds the S3 path to the HB1906 EK60 Zarr dataset and opens it anonymously.\nSubsets by time window and depth range, selects the 38 kHz channel, and masks samples below the estimated bottom.\n.compute() materializes the selection; hm_timestamps will be reused for time alignment later. All datasets are accessed\nusing the OSDF infrastructure\n\nbucket_name = 'noaa-wcsd-zarr-pds'\nship_name = \"Henry_B._Bigelow\"\ncruise_name = \"HB1906\"\nsensor_name = \"EK60\"\n\n# Accessing the NOAA HB1906 dataset using OSDF (anonymous S3)\ns3_file_system = s3fs.S3FileSystem(anon=True)\nzarr_store = f'{cruise_name}.zarr'\ns3_zarr_store_path = f\"{bucket_name}/level_2/{ship_name}/{cruise_name}/{sensor_name}/{zarr_store}\"\n\n# Map S3 path to a zarr store and open (consolidated=None to let xarray infer metadata)\nstore = s3fs.S3Map(root=s3_zarr_store_path, s3=s3_file_system, check=False)\ncruise = xr.open_zarr(store=store, consolidated=None)\n\n# Time/depth subset and single-frequency selection\nstart_time = \"2019-10-16T15:00:00\"\nend_time = \"2019-10-16T23:11:09\"\ntimeslice = slice(start_time, end_time)\ndepths = slice(10, 250)\ncruise = cruise.sel(time=timeslice, depth=depths, drop=False)\ncruise = cruise.sel(frequency=38000, method='nearest').compute()  # materialize after selection\ncruise = cruise.where(cruise.depth < cruise.bottom + 2, drop=True)  # remove bins below bottom\n\n# Timestamps for later alignment\nhm_timestamps = cruise.time.values.tolist()\n\n","type":"content","url":"/notebooks/sonarai-technical#id-2-initializing-the-datasets","position":9},{"hierarchy":{"lvl1":"3) Accessing buoy data"},"type":"lvl1","url":"/notebooks/sonarai-technical#id-3-accessing-buoy-data","position":10},{"hierarchy":{"lvl1":"3) Accessing buoy data"},"content":"Defines a buoy location on Georges Bank (longitude converted to 0–360).\n\nOptional: Loads three daily OISST files and samples SST at the nearest grid point (day before, day of, day after).\n\nERDDAP buoy environmental data.\n\nSets ERDDAP dataset parameters and enforces a max_days cap by adjusting end_date_time if needed.\nReads station metadata to extract lon/lat and wind-speed units; prepares a conversion to knots.\nPulls a table of time, wind_speed, SST, significant wave height, converts wind speed to knots, indexes by time.\nFilters to the requested window and keeps the first nine rows (intentional truncation for later alignment).\n\n# Location of one specific buoy located on Georges Bank\ntarget_lon = 360 - 66.546  # convert from -180..180 to 0..360\ntarget_lat = 41.088\n\n# ______________OPTIONAL BUOY DATA FROM NCAR/UCAR______________\n# print(f\"Target coordinates: Longitude: {target_lon}, Latitude: {target_lat}\")\n#\n# # Accessing stationary buoy data (daily OISST files); select nearest grid cell\n# buoy_data_day_before = xr.open_dataset(\n#     'https://data.rda.ucar.edu/d277007/avhrr_v2.1/2019/oisst-avhrr-v02r01.20191015.nc#mode=bytes', engine='netcdf4')\n# buoy_data_actual_day = xr.open_dataset(\n#     'https://data.rda.ucar.edu/d277007/avhrr_v2.1/2019/oisst-avhrr-v02r01.20191016.nc#mode=bytes',\n#     engine='netcdf4')\n# buoy_data_day_after = xr.open_dataset(\n#     'https://data.rda.ucar.edu/d277007/avhrr_v2.1/2019/oisst-avhrr-v02r01.20191017.nc#mode=bytes',\n#     engine='netcdf4')\n#\n# sst_day_before = buoy_data_day_before['sst'].sel(lon=target_lon, lat=target_lat, method='nearest').values[0][0]\n# sst_actual_day = buoy_data_actual_day['sst'].sel(lon=target_lon, lat=target_lat, method='nearest').values[0][0]\n# sst_day_after = buoy_data_day_after['sst'].sel(lon=target_lon, lat=target_lat, method='nearest').values[0][0]\n# ______________________________________________________________\n\n# The following code has been copied from https://github.com/ioos/soundcoop/blob/main/3_HMD_environmental_data/plot_sound_environmental_and_climatology_data.ipynb in order to access buoy environmental data.\n\nerddap_dataset = 'gov-ndbc-44005'\nsound_dataset = 'Monh'\nmax_days = 25\nstart_date_time = '2019-10-16T14:00:00.000'\nend_date_time = '2021-10-16T23:30:00.000'\nmin_frequency = 21\nmax_frequency = 24000\n\nerddap_base_url = 'https://erddap.sensors.ioos.us/erddap'\n\n# Cap the end date if requested range exceeds max_days\ntime_delta = datetime.fromisoformat(end_date_time) - datetime.fromisoformat(start_date_time)\nif time_delta.days > max_days:\n    end_date_time = str(datetime.fromisoformat(start_date_time) + timedelta(days=max_days))\n    print(f'end_date_time updated to {end_date_time}')\n\n# Get station lon/lat and units from ERDDAP metadata (CSV)\nerddap_metadata_url = f'{erddap_base_url}/info/{erddap_dataset}/index.csv'\nenv_metadata_df = pd.read_csv(erddap_metadata_url)\n\nenv_station_x = env_metadata_df.loc[env_metadata_df['Attribute Name'] == 'geospatial_lon_min']['Value'].item()\nenv_station_y = env_metadata_df.loc[env_metadata_df['Attribute Name'] == 'geospatial_lat_min']['Value'].item()\nswt_var = 'sea_surface_temperature'\n\n# __________OPTIONAL: Add Wind__________\n# Determine wind_speed units to convert to knots\n# wind_speed_units_row = env_metadata_df[\n#     (env_metadata_df['Row Type'] == 'attribute') &\n#     (env_metadata_df['Attribute Name'] == 'units') &\n#     # (env_metadata_df['Variable Name'] == 'wind_speed')\n#     ]\n# wind_speed_units = wind_speed_units_row.iloc[0]['Value']\n# print(wind_speed_units)\n\n# wind_speed_to_kts_factors = {\n#     \"m.s-1\": 1.94384,\n#     \"mph\": 0.86897423357831,\n#     \"kmh\": 0.53995555554212126825,\n#     \"ft.s-1\": 0.59248243198521155506\n# }\n\n# if wind_speed_units in wind_speed_to_kts_factors:\n#     print(\"Success! Units can be converted from\", wind_speed_units, 'to', 'kts')\n# else:\n#     print(\"Error! Wind speed cannot be converted from\", wind_speed_units, 'to', 'kts')\n\n# wind_var = 'wind_speed'\n# wave_var = 'sea_surface_wave_significant_height'\n# anomaly_var = 'swt_anomaly'\n# wind_var_kts = 'wind_speed_kts'\n# ________________________________________\n\n# Build ERDDAP tabledap query URL\nerddap_dataset_url = (\n    f'{erddap_base_url}/tabledap/{erddap_dataset}.csv'\n    f'?time,{swt_var}'\n)\n\n# Read dataset (skip the second row of units)\nenv_df = pd.read_csv(\n    erddap_dataset_url,\n    skiprows=[1]  # The second row (index 1) are the column units, which we don't need\n)\n\n# Format time, convert wind speed to knots, index by time\nenv_df['time'] = pd.to_datetime(env_df['time'])\n# env_df['wind_speed_kts'] = env_df['wind_speed'].apply(lambda x: x * wind_speed_to_kts_factors[wind_speed_units])\n# del env_df['wind_speed']\nenv_df = env_df.set_index('time').sort_index()\n\n# Filter by requested time window and keep first 9 rows (drops the rest)\nenv_df = env_df[(env_df.index > start_date_time) & (env_df.index < end_date_time)]\nenv_df.drop(env_df.tail(-9).index, inplace=True)\n# env_df\n\n","type":"content","url":"/notebooks/sonarai-technical#id-3-accessing-buoy-data","position":11},{"hierarchy":{"lvl1":"3) Accessing buoy data","lvl2":"4) Calculate the temperature anomaly, sun elevation in degree and azimuth"},"type":"lvl2","url":"/notebooks/sonarai-technical#id-4-calculate-the-temperature-anomaly-sun-elevation-in-degree-and-azimuth","position":12},{"hierarchy":{"lvl1":"3) Accessing buoy data","lvl2":"4) Calculate the temperature anomaly, sun elevation in degree and azimuth"},"content":"Extracts World Ocean Atlas 2023 temperature data for a specific location and month and calculates temperature anomaly (optional), sun elevation in degree and azimuth (optional).\n\ndef get_woa23_temp_at_xy(x, y, month, var='t_mn', depth=0):\n    \"\"\"\n    Get 1-degree WOA 2023 temperature values for a given point and month.\n\n    Args:\n        x: A longitude value given in decimal degrees\n        y: A latitude value given in decimal degrees\n        month: The month asn integer from which to extract the value\n        var (optional): The temperature variable to use. Defaults to the statistical mean.\n        depth (optional): The depth at which to extract the value. Defaults to the surface.\n    \"\"\"\n    url = (\n        'https://www.ncei.noaa.gov/thredds-ocean/dodsC/woa23/DATA/'\n        f'temperature/netcdf/decav/1.00/woa23_decav_t{month:02}_01.nc'\n    )\n    ds = xr.open_dataset(\n        url,\n        decode_times=False  # xarray can't handle times defined as \"months since ...\"\n    )\n\n    da = ds.isel(depth=depth)[var]  # Pull out just the variable we're interested in\n\n    # Because nearshore locations are often NaN due to the grid's low resolution\n    # we need to interpolate the NaNs to the nearest non-NaN before extracting our value.\n    # We use rioxarray to do the interpolations in two dimensions because plain vanilla xarray\n    # can only interpolate in one dimension.\n    da = da.rio.write_crs(4326)\n    da = da.rio.interpolate_na(method='nearest')\n\n    # Then we extract the value, also using the nearest neighbor method because the given\n    # x and y values are unlikely to fall exactly on one of the grid's lat/lon coordinate pairs\n    val = da.sel(lon=x, lat=y, method='nearest').item()\n\n    return val\n\n\n# Define the location of our selected ERDDAP dataset\n# Override here if needed\nx = env_station_x\ny = env_station_y\n\nurl = (\n    'https://www.ncei.noaa.gov/thredds-ocean/dodsC/woa23/DATA/'\n    f'temperature/netcdf/decav/1.00/woa23_decav_t07_01.nc'\n)\nda = xr.open_dataset(\n    url,\n    decode_times=False  # xarray can't handle times defined as \"months since ...\"\n).isel(depth=0)['t_mn']  # Pull out just the variable we're interested in\n\n# Because nearshore locations are often NaN due to the grid's low resolution\n# we need to interpolate the NaNs to the nearest non-NaN before extracting our value.\n# We use rioxarray to do the interpolations in two dimensions because plain vanilla xarray\n# can only interpolate in one dimension.\nda = da.rio.write_crs(4326)\nda = da.rio.interpolate_na(method='nearest')\n\n# Then we extract the value, also using the nearest neighbor method because the given\n# x and y values are unlikely to fall exactly on one of the grid's lat/lon coordinate pairs\nval = da.sel(lon=x, lat=y, method='nearest').item()\n\n# Assemble a mapping between months and WOA 2023 temperature values\nmonths = list(range(1, 13))\ntemps = [get_woa23_temp_at_xy(x, y, m) for m in months]\nclim_dict = {m: t for m, t in zip(months, temps)}\n\n# Calculate the sea water temperature anomaly by subtracting the monthly WOA 2023 temperature value\n# from each measured sea water temperature value and store it as a new variable\nanomaly_var = env_df[swt_var] - [clim_dict[10]]\n# We are not adding the temperature_anomaly variable to our dataset, because we were able to see that it follows the sea surface temperature.\n# env_df[\"temperature_anomaly\"] = anomaly_var\n\n# ---- Time range in UTC ----\ntimes_utc = pd.date_range(\n    start=start_date_time,\n    end=end_date_time,\n    freq=\"1h\",\n    tz=\"UTC\"  # <-- key: set timezone to UTC\n)\n\n# ---- Calculate solar position ----\nsolpos = pvlib.solarposition.get_solarposition(times_utc, target_lat, target_lon)\n\n# ---- Extract elevation ----\ndf = pd.DataFrame({\n    \"time_utc\": times_utc,\n    \"elevation_deg\": solpos[\"elevation\"],\n    \"azimuth_deg\": solpos[\"azimuth\"]\n})\n\nenv_df[\"elevation_deg\"] = solpos[\"elevation\"].tolist()[1:10]\n# env_df[\"azimuth_deg\"] = solpos[\"azimuth\"].tolist()[:9]\nenv_df\n\n","type":"content","url":"/notebooks/sonarai-technical#id-4-calculate-the-temperature-anomaly-sun-elevation-in-degree-and-azimuth","position":13},{"hierarchy":{"lvl1":"5) Downloading external error map for the specific location."},"type":"lvl1","url":"/notebooks/sonarai-technical#id-5-downloading-external-error-map-for-the-specific-location","position":14},{"hierarchy":{"lvl1":"5) Downloading external error map for the specific location."},"content":"Currently the error map comes from a fixed file; our plan is to switch to a dynamic AWS download that accepts location parameters.\n\n# Downloading anomaly detection model error map from NCAR via OSDF\nresponse = requests.get('https://data-osdf.rda.ucar.edu/ncar/rda/pythia_2025/osdf-cookbook/mae_error_map.npy')\nresponse.raise_for_status()\nsonar_clusters = np.load(io.BytesIO(response.content))\n\n\n\n","type":"content","url":"/notebooks/sonarai-technical#id-5-downloading-external-error-map-for-the-specific-location","position":15},{"hierarchy":{"lvl1":"6) Helper: mean Sv in dB"},"type":"lvl1","url":"/notebooks/sonarai-technical#id-6-helper-mean-sv-in-db","position":16},{"hierarchy":{"lvl1":"6) Helper: mean Sv in dB"},"content":"Computes the mean of Sv correctly by converting dB → linear, averaging, then linear → dB.\nAccepts array-like input (NumPy/xarray/dask); returns a scalar in dB.\n\ndef calculate_sv_mean(input_sv):\n    # Convert dB to linear, mean in linear space, convert back to dB\n    sv = 10. ** (input_sv / 10.)\n    return 10 * np.log10(np.mean(sv))\n\n","type":"content","url":"/notebooks/sonarai-technical#id-6-helper-mean-sv-in-db","position":17},{"hierarchy":{"lvl1":"7) Group cruise data into hourly chunks"},"type":"lvl1","url":"/notebooks/sonarai-technical#id-7-group-cruise-data-into-hourly-chunks","position":18},{"hierarchy":{"lvl1":"7) Group cruise data into hourly chunks"},"content":"Adds an hourly label and groups the cruise data by hour.\nProduces a list of per-hour xarray.Dataset chunks for downstream aggregation.\n\ncruise['time_hour'] = cruise['time'].dt.floor('1h')  # hourly bin label\n\n# Group by each hour\ngrouped = cruise.groupby('time_hour')\n\n# Extract each 1-hour Dataset as a chunk (drop helper label)\nchunks = [group.drop_vars('time_hour') for _, group in grouped]\n\n","type":"content","url":"/notebooks/sonarai-technical#id-7-group-cruise-data-into-hourly-chunks","position":19},{"hierarchy":{"lvl1":"8) Compute hourly mean Sv and attach to env_df"},"type":"lvl1","url":"/notebooks/sonarai-technical#id-8-compute-hourly-mean-sv-and-attach-to-env-df","position":20},{"hierarchy":{"lvl1":"8) Compute hourly mean Sv and attach to env_df"},"content":"Iterates over hourly chunks, computes mean Sv per hour using calculate_sv_mean.\nConverts dask→NumPy→Python float and appends to a list.\nAssigns the resulting hourly series to env_df[“sv_hourly”].\nAssumes the number/order of hours matches rows retained in env_df.\n\nsv_hourly = []\ntimestamps = []\n\nfor i in range(0, len(chunks)):\n    sv_data = chunks[i]['Sv']\n    result = calculate_sv_mean(sv_data)\n\n    # Use first time in hour as representative timestamp\n    ts = pd.to_datetime(chunks[i]['time'].values[0])\n    result = result.compute()  # dask -> numpy\n    result = float(result.values)  # numpy -> Python float\n\n    sv_hourly.append(result)\n\nenv_df[\"sv_hourly\"] = sv_hourly\n\n","type":"content","url":"/notebooks/sonarai-technical#id-8-compute-hourly-mean-sv-and-attach-to-env-df","position":21},{"hierarchy":{"lvl1":"9) Build (depth × time) error-map DataFrame and align timestamps"},"type":"lvl1","url":"/notebooks/sonarai-technical#id-9-build-depth-time-error-map-dataframe-and-align-timestamps","position":22},{"hierarchy":{"lvl1":"9) Build (depth × time) error-map DataFrame and align timestamps"},"content":"Extracts one channel from sonar_clusters and pairs it with cruise depths and timestamps to form a DataFrame.\nUses min(...) to guard against size mismatches in depth/time dimensions.\nAligns only the first and last timestamps in env_df to the heatmap’s time range (keeps interior indices unchanged, sets UTC).\n\n# Prepare axes\ndepths = np.asarray(cruise.depth.values)\ntimes = pd.to_datetime(hm_timestamps)\n\n# Select channel/slice from sonar_clusters\nvals = sonar_clusters[:, :, 1]  # (1088, 28096)\n\n# Guard against mismatched sizes\nn_depth = min(len(depths), vals.shape[0])\nn_time = min(len(times), vals.shape[1])\n\n# DataFrame: rows=depths, cols=timestamps\ndf = pd.DataFrame(\n    data=vals[:n_depth, :n_time],\n    index=depths[:n_depth],\n    columns=times[:n_time]\n)\n\n# Align env_df index endpoints to heatmap timestamps (keeps interior unchanged)\nidx = env_df.index.tolist()\ndf_timestamps = pd.to_datetime(df.columns).tz_localize(None)\nidx[0] = pd.Timestamp(df_timestamps.values[0], tz='UTC').floor(\"s\")\nidx[-1] = pd.Timestamp(df_timestamps.values[-1], tz='UTC').floor(\"s\")\nenv_df.index = idx\n\n","type":"content","url":"/notebooks/sonarai-technical#id-9-build-depth-time-error-map-dataframe-and-align-timestamps","position":23},{"hierarchy":{"lvl1":"10) Data Visualization: Synchronized Lines + Heatmap"},"type":"lvl1","url":"/notebooks/sonarai-technical#id-10-data-visualization-synchronized-lines-heatmap","position":24},{"hierarchy":{"lvl1":"10) Data Visualization: Synchronized Lines + Heatmap"},"content":"\n\nPlots synchronized data: top = time series from line_df; bottom = depth×time heatmap from heatmap_df.\nExpects line_df to have a DatetimeIndex (timezone-naive or converted).\nDepth axis is reversed (surface at top).\nSaves an interactive HTML file to the parent directory (out.html) and shows the figure if show=True.\n\ndef plot_synchronized_heatmaps_from_df(\n        heatmap_df: pd.DataFrame,\n        line_df: pd.DataFrame,\n        colorscale: str = \"Reds\",\n        show_markers: bool = False,\n        show: bool = False,\n):\n    if not isinstance(line_df.index, pd.DatetimeIndex):\n        raise TypeError(\"line_df must have a DatetimeIndex\")\n    line_df = line_df.copy()\n    if line_df.index.tz is not None:\n        line_df.index = line_df.index.tz_convert(None)\n\n    depths = np.asarray(heatmap_df.index)\n    heatmap_timestamps = pd.to_datetime(heatmap_df.columns)\n    z = heatmap_df.to_numpy()\n\n    n = len(line_df.columns)\n    fig = make_subplots(\n        rows=n + 1,\n        cols=1,\n        shared_xaxes=True,\n        vertical_spacing=0.02,\n        row_heights=[0.1] * n + [0.7],  # n small rows + large heatmap row\n    )\n\n    mode = \"lines+markers\" if show_markers else \"lines\"\n\n    # Define units for each variable\n    units = {\n        'sea_surface_temperature': '°C',\n        'elevation_deg': 'degrees',\n        'sv_hourly': 'dB'\n    }\n\n    for i, col in enumerate(line_df.columns, start=1):\n        fig.add_trace(\n            go.Scatter(x=line_df.index, y=line_df[col], name=str(col), mode=mode, showlegend=True),\n            row=i, col=1\n        )\n\n        # Add units as y-axis titles\n        unit = units.get(col, '')  # Default to empty string if variable not found\n        fig.update_yaxes(\n            title_text=unit,\n            row=i, col=1\n        )\n\n        if i < len(line_df.columns):\n            fig.update_xaxes(showticklabels=False, row=i, col=1)\n\n    fig.add_trace(\n        go.Heatmap(\n            z=z, x=heatmap_timestamps, y=depths, colorscale=colorscale,\n            zmin=np.nanmin(z), zmax=np.nanmax(z),\n            hovertemplate=\"t=%{x}<br>depth=%{y}<br>value=%{z}<extra></extra>\",\n        ),\n        row=n + 1, col=1\n    )\n    fig.update_yaxes(autorange=\"reversed\", row=n + 1, col=1, title_text=\"Depth\")\n\n    fig.update_layout(\n        margin=dict(l=40, r=40, t=60, b=40),  # Reduced left margin, increased top for legend\n        hovermode=\"x unified\",\n        template=\"plotly_white\",\n        # height=10 * n + 500,  # scale height with number of signals\n    )\n\n    # Enhanced horizontal legend positioning\n    fig.update_layout(\n        legend=dict(\n            orientation='h',\n            x=0,\n            y=1.02,\n            xanchor='left',\n            yanchor='bottom',\n            bgcolor='rgba(255,255,255,0.8)',\n            bordercolor='rgba(0,0,0,0.1)',\n            borderwidth=1\n        )\n    )\n\n    save_path = os.path.join(os.path.dirname(os.getcwd()), \"out.html\")\n    fig.write_html(save_path)\n    print(f\"Plot saved to: {save_path}\")\n    if show:\n        fig.show()\n    return fig\nfig = plot_synchronized_heatmaps_from_df(heatmap_df=df, line_df=env_df)\nfig.show()","type":"content","url":"/notebooks/sonarai-technical#id-10-data-visualization-synchronized-lines-heatmap","position":25},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook"},"type":"lvl1","url":"/notebooks/atmosphere-llc2160-visualization","position":0},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook"},"content":"\n\nThe DYnamics of the Atmospheric general circulation Modeled On Non-hydrostatic Domains (DYAMOND) data provides high resolution ocean circulation models, offering unprecedented detail. This dataset comprises a C1440 configuration of the Goddard Earth Observing System (GEOS) atmospheric model, with 7-km horizontal grid spacing and 72 vertical layers, coupled to a LLC2160 configuration of the Massachusetts Institute of Technology general circulation model (MITgcm) with 2–4-km grid spacing and 90 vertical levels. The C1440-LLC2160 simulation has been integrated for 14 months, starting from prescribed initial conditions on January 20, 2020.\n\nThis notebook is the second part of the DYAMOND LLC2160 Ocean Dataset Cookbook.\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization","position":1},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Overview"},"type":"lvl3","url":"/notebooks/atmosphere-llc2160-visualization#overview","position":2},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Overview"},"content":"This notebook demonstrates how to access and visualize high-resolution atmospheric data from the DYAMOND dataset using OpenVisus. The data is hosted in OSDF and served using Pelican Platform and OpenVisus. You’ll learn how to read metadata from the cloud, interactively select variables, and explore regional and depth-based slices of the data.\n\nRead the metadata file from cloud\n\nData Subset\n\nVisualize the data\n\nExplore multi-resolution data for a specific region and depth\n\nBy the end of this notebook, you will understand how to:\n\nStream and query oceanographic data using PelicanFS\n\nUse metadata to inform data exploration\n\nVisualize regional and depth-specific ocean data using Panel and Bokeh\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#overview","position":3},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Prerequisites"},"type":"lvl3","url":"/notebooks/atmosphere-llc2160-visualization#prerequisites","position":4},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Prerequisites"},"content":"This section was inspired by \n\nthis template of the wonderful \n\nThe Turing Way Jupyter Book.\n\nConcepts\n\nImportance\n\nNotes\n\nOpenVisus\n\nHelpful\n\nRequired for multiresolution data access and streaming\n\nOceanographic data formats and interpretation\n\nHelpful\n\nUnderstanding of gridded climate/ocean data such as LLC2160\n\nPelicanFS\n\nHelpful\n\nUsed for high-throughput data access from cloud storage\n\nTime to learn: 30 minutes\n\nSystem requirements:\n\nPython packages: panel, bokeh, xmltodict, colorcet, boto3, basemap, pelicanfs, OpenVisus, openvisuspy\n\nRecommended: Python ≥ 3.8, internet access for cloud-hosted data\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#prerequisites","position":5},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl2":"Step 1: Importing the libraries"},"type":"lvl2","url":"/notebooks/atmosphere-llc2160-visualization#step-1-importing-the-libraries","position":6},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl2":"Step 1: Importing the libraries"},"content":"\n\nimport numpy as np\nimport openvisuspy as ovp\nimport matplotlib.pyplot as plt\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#step-1-importing-the-libraries","position":7},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl4":"The section below shows different LLC2160 fields we have available in cloud. Each field is >200TB.","lvl2":"Step 1: Importing the libraries"},"type":"lvl4","url":"/notebooks/atmosphere-llc2160-visualization#the-section-below-shows-different-llc2160-fields-we-have-available-in-cloud-each-field-is-200tb","position":8},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl4":"The section below shows different LLC2160 fields we have available in cloud. Each field is >200TB.","lvl2":"Step 1: Importing the libraries"},"content":"\n\nvariable = 'u'\nface=0 # 6 variables are available\ntimestep=1 # There are 10000 timesteps available\n\nbase_url= \"pelican://osg-htc.org/nasa/nsdf/climate3/dyamond/GEOS/\"\nvar_dir=f\"GEOS_{variable.upper()}/{variable.lower()}_face_{face}_depth_52_time_0_10269.idx\"\nvar_url=base_url+var_dir\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#the-section-below-shows-different-llc2160-fields-we-have-available-in-cloud-each-field-is-200tb","position":9},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Step 2: Reading the metadata file from cloud","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/atmosphere-llc2160-visualization#step-2-reading-the-metadata-file-from-cloud","position":10},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Step 2: Reading the metadata file from cloud","lvl2":"Step 1: Importing the libraries"},"content":"In this section, you can select any variables that you can declared in the cells above and replace it inside LoadDataset. We are just reading the metadata for the dataset here.\n\ndb=ovp.LoadDataset(var_url)\nprint(f'Dimensions: {db.getLogicBox()[1][0]}*{db.getLogicBox()[1][1]}*{db.getLogicBox()[1][2]}')\nprint(f'Total Timesteps: {len(db.getTimesteps())}')\nprint(f'Field: {db.getField().name}')\nprint('Data Type: float32')\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#step-2-reading-the-metadata-file-from-cloud","position":11},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Step 3:  Data Selection","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/atmosphere-llc2160-visualization#step-3-data-selection","position":12},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Step 3:  Data Selection","lvl2":"Step 1: Importing the libraries"},"content":"This section shows you how to load the data you want. You can select any timestep, region (x,y,z) you want. You can set the quality or resolution of the data as well. Higher quality means the finer(more) data. Not setting any time means first timestep available. Not setting quality means full data which takes a while to load because of the higher filesize.  Since each timestep is >30GB, I am only selecting 1 level out of 90.\n\ndata=db.db.read(time=0,z=[0,1],quality=-4) #Since each timestep is >30GB, I am only selecting 1 level out of 90.\ndata.shape\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#step-3-data-selection","position":13},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/atmosphere-llc2160-visualization#step-4-visualize-the-data","position":14},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"content":"We are using a simple matplotlib here, but since the data is in numpy array, it can loaded with any python modules that support numpy. Feel free to set the vmin,vmax appropriately.\n\nfig,axes=plt.subplots(1,1,figsize=(12,8))\nim= axes.imshow(data[0,:,:], aspect='auto',origin='lower',cmap='coolwarm')\ncbar = plt.colorbar(im, ax=axes)\ncbar.set_label('Temperature (deg. C)')\nplt.show()\n\n\n","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#step-4-visualize-the-data","position":15},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"type":"lvl4","url":"/notebooks/atmosphere-llc2160-visualization#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":16},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Atmospheric Dataset Notebook","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"content":"Aashish Panta (\n\naashishpanta0@gmail​.com)\n\nGiorgio Scorzelli (\n\nscrgiorgio@gmail​.com)\n\nValerio Pascucci (\n\npascucci​.valerio@gmail​.com)","type":"content","url":"/notebooks/atmosphere-llc2160-visualization#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":17},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS"},"type":"lvl1","url":"/notebooks/introduction-to-nsdf-openvisus","position":0},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS"},"content":"\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus","position":1},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl2","url":"/notebooks/introduction-to-nsdf-openvisus#openvisus-high-performance-big-data-analysis-and-visualization","position":2},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#openvisus-high-performance-big-data-analysis-and-visualization","position":3},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Overview","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#overview","position":4},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Overview","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"OpenViSUS is an open-source framework designed for efficient management, analysis, and visualization of large-scale scientific datasets. It enables interactive exploration of petabyte-scale data on a wide range of devices, from supercomputers to commodity laptops, making big data accessible to all users.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#overview","position":5},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Key Features","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#key-features","position":6},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Key Features","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Efficient Data Storage: Uses the IDX format, which stores data in a hierarchical Z (HZ) order for cache-oblivious, progressive access.\n\nScalable Visualization: Enables interactive visualization of terabyte and petabyte datasets without requiring high-end hardware.\n\nProgressive Streaming: Optimizes network utilization with state-of-the-art compression algorithms for fast data delivery and streaming.\n\nWeb-Based Dashboards: Provides customizable dashboards for data analysis accessible from any device with an internet connection.\n\nOpen Source: Distributed under the permissive BSD license.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#key-features","position":7},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Data Storage: The IDX Format","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#data-storage-the-idx-format","position":8},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Data Storage: The IDX Format","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Hierarchical Z (HZ) Order: Data is organized to allow efficient, multi-resolution access and visualization.\n\nProgressive Access: Users can interactively explore data at different resolutions, starting with coarse overviews and refining to full detail as needed.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#data-storage-the-idx-format","position":9},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Data Delivery","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#data-delivery","position":10},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Data Delivery","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Interactive Streaming: Share and stream large datasets using simple server modules (e.g., Apache), enabling teravoxel imagery delivery.\n\nCloud and Local Access: Data can be accessed from local storage or cloud repositories, with optimized streaming for remote analysis.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#data-delivery","position":11},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Interactive Analysis and Visualization","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#interactive-analysis-and-visualization","position":12},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Interactive Analysis and Visualization","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Cross-Platform: Works on supercomputers, desktops, and laptops.\n\nScripting Support: Experiment with interactive scripting for rapid data insights.\n\nUser-Friendly Querying: Abstracts complexities of file systems and cloud services, allowing scientists to focus on analysis.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#interactive-analysis-and-visualization","position":13},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"2026 IEEE SciVis  Contest","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#id-2026-ieee-scivis-contest","position":14},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"2026 IEEE SciVis  Contest","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"NSDF-OpenVISUS is directly supporting the 2026 IEEE SciVis Contest. This contest is held annually as part of the IEEE VIS Conference. In 2026, this contest will focus on the visualization of petascale oceanic and atmospheric climate data provided by NASA. This year’s challenge emphasizes advanced visualization methods for exploring vast climate datasets, encouraging innovative solutions that address real-world issues such as climate prediction, weather simulation, and environmental impact analysis.\n\nThe best submission wins $1000 cash prize. Find more information on the official \n\nSciVis website.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#id-2026-ieee-scivis-contest","position":15},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Example Use Cases","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#example-use-cases","position":16},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Example Use Cases","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"NASA LLC2160 Data Interactive Dashboard\n\nNEX-GDDP-CMIP6 Dashboard\n\nClassroom deployment for minority-serving institutions\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#example-use-cases","position":17},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Other Deplyments","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#other-deplyments","position":18},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Other Deplyments","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Cornell High Energy Synchrotron Source (CHESS)\n\nNational Center for Atmospheric Research (NCAR)\n\nNEON\n\nSOMOSPIE\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#other-deplyments","position":19},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Getting Started","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#getting-started","position":20},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"Getting Started","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#getting-started","position":21},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl4":"Installation","lvl3":"Getting Started","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl4","url":"/notebooks/introduction-to-nsdf-openvisus#installation","position":22},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl4":"Installation","lvl3":"Getting Started","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Install Using pip\n\npip install OpenVisus\n\nBuild from Source:\n\nClone the repository: git clone https://github.com/sci-visus/OpenVisus.git\n\nFollow instructions in the README.md and docs/compilation.md for building on your platform.\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#installation","position":23},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"References","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl3","url":"/notebooks/introduction-to-nsdf-openvisus#references","position":24},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl3":"References","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"National Science Data Fabric\n\nOpenVisus\n\nOpenVisuspy\n\nPlease consult these papers for technical details and use cases:\n\nWeb-based Visualization and Analytics of Petascale data: Equity as a Tide that Lifts All Boats\n\nInteractive Visualization of Terascale Data in the Browser: Fact or Fiction?\n\nFast Multiresolution Reads of Massive Simulation Datasets\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#references","position":25},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl3":"References","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"type":"lvl4","url":"/notebooks/introduction-to-nsdf-openvisus#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":26},{"hierarchy":{"lvl1":"Introduction to NSDF-OpenVISUS","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl3":"References","lvl2":"OpenViSUS: High Performance Big Data Analysis and Visualization"},"content":"Aashish Panta (\n\naashishpanta0@gmail​.com)\n\nGiorgio Scorzelli (\n\nscrgiorgio@gmail​.com)\n\nValerio Pascucci (\n\npascucci​.valerio@gmail​.com)\n\n","type":"content","url":"/notebooks/introduction-to-nsdf-openvisus#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":27},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook"},"type":"lvl1","url":"/notebooks/ocean-llc2160-visualization","position":0},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook"},"content":"\n\nThe DYnamics of the Atmospheric general circulation Modeled On Non-hydrostatic Domains (DYAMOND) data provides high resolution ocean circulation models, offering unprecedented detail. This dataset comprises a C1440 configuration of the Goddard Earth Observing System (GEOS) atmospheric model, with 7-km horizontal grid spacing and 72 vertical layers, coupled to a LLC2160 configuration of the Massachusetts Institute of Technology general circulation model (MITgcm) with 2–4-km grid spacing and 90 vertical levels. The C1440-LLC2160 simulation has been integrated for 14 months, starting from prescribed initial conditions on January 20, 2020.\n\n\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization","position":1},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Overview"},"type":"lvl3","url":"/notebooks/ocean-llc2160-visualization#overview","position":2},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Overview"},"content":"This notebook demonstrates how to access and visualize high-resolution ocean data from the LLC2160 dataset using OpenVisus. The data is hosted in OSDF and served using Pelican Platform and OpenVisus. You’ll learn how to read metadata from the cloud, interactively select variables, and explore regional and depth-based slices of the data.\n\nRead the metadata file from cloud\n\nData Subset\n\nVisualize the data\n\nExplore multi-resolution data for a specific region and depth\n\nBy the end of this notebook, you will understand how to:\n\nStream and query oceanographic data using PelicanFS\n\nUse metadata to inform data exploration\n\nVisualize regional and depth-specific ocean data using Panel and Bokeh\n\n\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#overview","position":3},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Prerequisites"},"type":"lvl3","url":"/notebooks/ocean-llc2160-visualization#prerequisites","position":4},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Prerequisites"},"content":"This section was inspired by \n\nthis template of the wonderful \n\nThe Turing Way Jupyter Book.\n\nConcepts\n\nImportance\n\nNotes\n\nOpenVisus\n\nHelpful\n\nRequired for multiresolution data access and streaming\n\nOceanographic data formats and interpretation\n\nHelpful\n\nUnderstanding of gridded climate/ocean data such as LLC2160\n\nPelicanFS\n\nHelpful\n\nUsed for high-throughput data access from cloud storage\n\nTime to learn: 30 minutes\n\nSystem requirements:\n\nPython packages: panel, bokeh, xmltodict, colorcet, boto3, basemap, pelicanfs, OpenVisus, openvisuspy\n\nRecommended: Python ≥ 3.8, internet access for cloud-hosted data\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#prerequisites","position":5},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 1: Importing the libraries"},"type":"lvl2","url":"/notebooks/ocean-llc2160-visualization#step-1-importing-the-libraries","position":6},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 1: Importing the libraries"},"content":"\n\nimport numpy as np\nimport openvisuspy as ovp\nimport matplotlib.pyplot as plt\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-1-importing-the-libraries","position":7},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl4":"The section below shows different LLC2160 fields we have available in cloud. Each field is >200TB.","lvl2":"Step 1: Importing the libraries"},"type":"lvl4","url":"/notebooks/ocean-llc2160-visualization#the-section-below-shows-different-llc2160-fields-we-have-available-in-cloud-each-field-is-200tb","position":8},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl4":"The section below shows different LLC2160 fields we have available in cloud. Each field is >200TB.","lvl2":"Step 1: Importing the libraries"},"content":"\n\nvariable='salt' # options are: u,v,w,salt,theta\n\n\nbase_url= \"pelican://osg-htc.org/nasa/nsdf/climate3/dyamond/\"\nif variable==\"theta\" or variable==\"w\":\n    base_dir=f\"mit_output/llc2160_{variable}/llc2160_{variable}.idx\"\nelif variable==\"u\":\n    base_dir= \"mit_output/llc2160_arco/visus.idx\"\nelse:\n    base_dir=f\"mit_output/llc2160_{variable}/{variable}_llc2160_x_y_depth.idx\"\nvar_url=base_url+base_dir\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#the-section-below-shows-different-llc2160-fields-we-have-available-in-cloud-each-field-is-200tb","position":9},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Step 2: Reading the metadata file from cloud","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc2160-visualization#step-2-reading-the-metadata-file-from-cloud","position":10},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Step 2: Reading the metadata file from cloud","lvl2":"Step 1: Importing the libraries"},"content":"In this section, you can select any variables that you can declared in the cells above and replace it inside LoadDataset. We are just reading the metadata for the dataset here.\n\ndb=ovp.LoadDataset(var_url)\nprint(f'Dimensions: {db.getLogicBox()[1][0]}*{db.getLogicBox()[1][1]}*{db.getLogicBox()[1][2]}')\nprint(f'Total Timesteps: {len(db.getTimesteps())}')\nprint(f'Field: {db.getField().name}')\nprint('Data Type: float32')\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-2-reading-the-metadata-file-from-cloud","position":11},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Step 3:  Data Selection","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc2160-visualization#step-3-data-selection","position":12},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Step 3:  Data Selection","lvl2":"Step 1: Importing the libraries"},"content":"This section shows you how to load the data you want. You can select any timestep, region (x,y,z) you want. You can set the quality or resolution of the data as well. Higher quality means the finer(more) data. Not setting any time means first timestep available. Not setting quality means full data which takes a while to load because of the higher filesize.  Since each timestep is >30GB, I am only selecting 1 level out of 90.\n\ndata=db.db.read(time=0,z=[0,1],quality=-4) #Since each timestep is >30GB, I am only selecting 1 level out of 90.\ndata.shape\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-3-data-selection","position":13},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc2160-visualization#step-4-visualize-the-data","position":14},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"content":"We are using a simple matplotlib here, but since the data is in numpy array, it can loaded with any python modules that support numpy. Feel free to set the vmin,vmax appropriately.\n\nfig,axes=plt.subplots(1,1,figsize=(12,8))\nim= axes.imshow(data[0,:,:], aspect='auto',origin='lower',cmap='turbo')\ncbar = plt.colorbar(im, ax=axes)\ncbar.set_label('Temperature (deg. C)')\nplt.show()\n\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-4-visualize-the-data","position":15},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"But, what if you want to see the full data for a certain region at a certain depth?","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc2160-visualization#but-what-if-you-want-to-see-the-full-data-for-a-certain-region-at-a-certain-depth","position":16},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl3":"But, what if you want to see the full data for a certain region at a certain depth?","lvl2":"Step 1: Importing the libraries"},"content":"Just set the right x,y,z while reading the data. x and y are the bounding box, z is the depth/layer.\n\ndata1=db.db.read(time=1,z=[0,1],quality=-6,x=[500,2500],y=[2500,5000])\nplt.imshow(data1[0,:,:], origin='lower',cmap='turbo')\nplt.colorbar()\n\ndata1.shape #\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#but-what-if-you-want-to-see-the-full-data-for-a-certain-region-at-a-certain-depth","position":17},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 5: Save the data for the region locally"},"type":"lvl2","url":"/notebooks/ocean-llc2160-visualization#step-5-save-the-data-for-the-region-locally","position":18},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 5: Save the data for the region locally"},"content":"You can save the data locally as you want. For example, here we are only saving the region shown above as a numpy array.\n\nnp.save('test_region2.npy', data1)\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-5-save-the-data-for-the-region-locally","position":19},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 6: Load the locally saved region and visualize using matplotlib"},"type":"lvl2","url":"/notebooks/ocean-llc2160-visualization#step-6-load-the-locally-saved-region-and-visualize-using-matplotlib","position":20},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 6: Load the locally saved region and visualize using matplotlib"},"content":"\n\nlocal_data=np.load('test_region2.npy')\nplt.imshow(local_data[0,:,:], origin='lower',cmap='turbo')\nplt.colorbar()\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-6-load-the-locally-saved-region-and-visualize-using-matplotlib","position":21},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 7: Horizontal Slicing"},"type":"lvl2","url":"/notebooks/ocean-llc2160-visualization#step-7-horizontal-slicing","position":22},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl2":"Step 7: Horizontal Slicing"},"content":"\n\ndata1=db.db.read(time=1,x=[500,2500],y=[5100,5101])\ndata1.shape\n\n\nplt.figure(figsize=(14,8))\nplt.imshow(data1[:,0,:],cmap='turbo')\n# plt.colorbar()\n\n","type":"content","url":"/notebooks/ocean-llc2160-visualization#step-7-horizontal-slicing","position":23},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl2":"Step 7: Horizontal Slicing"},"type":"lvl4","url":"/notebooks/ocean-llc2160-visualization#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":24},{"hierarchy":{"lvl1":"DYAMOND LLC2160 Ocean Dataset Notebook","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl2":"Step 7: Horizontal Slicing"},"content":"Aashish Panta (\n\naashishpanta0@gmail​.com)\n\nGiorgio Scorzelli (\n\nscrgiorgio@gmail​.com)\n\nValerio Pascucci (\n\npascucci​.valerio@gmail​.com)","type":"content","url":"/notebooks/ocean-llc2160-visualization#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":25},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook"},"type":"lvl1","url":"/notebooks/ocean-llc4320-visualization","position":0},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook"},"content":"\n\nLLC4320, a product of \n\nEstimating the Circulation and Climate of the Ocean (ECCO) project, is the product of a 14-month simulation of ocean circulation and dynamics using MITgcm model. This simulation is similar to the ocean portion of the DYAMOND coupled simulation but was run with half the horizontal grid spacing (4\\times the cell count) and with ocean surface boundary values derived from observations and physical models. The model output has five 3D and thirteen 2D fields, including temperature, salinity, three velocity components, sea ice, and radiation. This massive dataset is 2.8 PB.\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization","position":1},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Overview"},"type":"lvl3","url":"/notebooks/ocean-llc4320-visualization#overview","position":2},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Overview"},"content":"This notebook demonstrates how to access and visualize high-resolution ocean data from the LLC4320 dataset using OpenVisus. The data is hosted in OSDF and served using Pelican Platform and OpenVisus. You’ll learn how to read metadata from the cloud, interactively select variables, and explore regional and depth-based slices of the data.\n\nRead the metadata file from cloud\n\nData Subset\n\nVisualize the data\n\nExplore multi-resolution data for a specific region and depth\n\nBy the end of this notebook, you will understand how to:\n\nStream and query oceanographic data using PelicanFS\n\nUse metadata to inform data exploration\n\nVisualize regional and depth-specific ocean data using Panel and Bokeh\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#overview","position":3},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Prerequisites"},"type":"lvl3","url":"/notebooks/ocean-llc4320-visualization#prerequisites","position":4},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Prerequisites"},"content":"This section was inspired by \n\nthis template of the wonderful \n\nThe Turing Way Jupyter Book.\n\nConcepts\n\nImportance\n\nNotes\n\nOpenVisus\n\nHelpful\n\nRequired for multiresolution data access and streaming\n\nOceanographic data formats and interpretation\n\nHelpful\n\nUnderstanding of gridded climate/ocean data such as LLC2160\n\nPelicanFS\n\nHelpful\n\nUsed for high-throughput data access from cloud storage\n\nTime to learn: 30 minutes\n\nSystem requirements:\n\nPython packages: panel, bokeh, xmltodict, colorcet, boto3, basemap, pelicanfs, OpenVisus, openvisuspy\n\nRecommended: Python ≥ 3.8, internet access for cloud-hosted data\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#prerequisites","position":5},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 1: Importing the libraries"},"type":"lvl2","url":"/notebooks/ocean-llc4320-visualization#step-1-importing-the-libraries","position":6},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 1: Importing the libraries"},"content":"\n\nimport numpy as np\nimport openvisuspy as ovp\nimport matplotlib.pyplot as plt\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-1-importing-the-libraries","position":7},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl4":"The section below shows different LLC4320 fields we have available in cloud. Each field is >400TB.","lvl2":"Step 1: Importing the libraries"},"type":"lvl4","url":"/notebooks/ocean-llc4320-visualization#the-section-below-shows-different-llc4320-fields-we-have-available-in-cloud-each-field-is-400tb","position":8},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl4":"The section below shows different LLC4320 fields we have available in cloud. Each field is >400TB.","lvl2":"Step 1: Importing the libraries"},"content":"\n\ntemperature=\"pelican://osg-htc.org/nasa/nsdf/climate1/llc4320/idx/theta/theta_llc4320_x_y_depth.idx\"\n\nsalinity=\"pelican://osg-htc.org/nasa/nsdf/climate1/llc4320/idx/salt/salt_llc4320_x_y_depth.idx\"\n\nvertical_velocity=\"pelican://osg-htc.org/nasa/nsdf/climate2/llc4320/idx/w/w_llc4320_x_y_depth.idx\"\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#the-section-below-shows-different-llc4320-fields-we-have-available-in-cloud-each-field-is-400tb","position":9},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Step 2: Reading the metadata file from cloud","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc4320-visualization#step-2-reading-the-metadata-file-from-cloud","position":10},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Step 2: Reading the metadata file from cloud","lvl2":"Step 1: Importing the libraries"},"content":"In this section, you can select any variables that you can declared in the cells above and replace it inside LoadDataset. We are just reading the metadata for the dataset here.\n\ndb=ovp.LoadDataset(temperature)\nprint(f'Dimensions: {db.getLogicBox()[1][0]}*{db.getLogicBox()[1][1]}*{db.getLogicBox()[1][2]}')\nprint(f'Total Timesteps: {len(db.getTimesteps())}')\nprint(f'Field: {db.getField().name}')\nprint('Data Type: float32')\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-2-reading-the-metadata-file-from-cloud","position":11},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Step 3:  Data Selection","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc4320-visualization#step-3-data-selection","position":12},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Step 3:  Data Selection","lvl2":"Step 1: Importing the libraries"},"content":"This section shows you how to load the data you want. You can select any timestep, region (x,y,z) you want. You can set the quality or resolution of the data as well. Higher quality means the finer(more) data. Not setting any time means first timestep available. Not setting quality means full data which takes a while to load because of the higher filesize.  Since each timestep is >30GB, I am only selecting 1 level out of 90.\n\ndata=db.db.read(time=0,z=[0,1],quality=-4) #Since each timestep is >30GB, I am only selecting 1 level out of 90.\ndata.shape\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-3-data-selection","position":13},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc4320-visualization#step-4-visualize-the-data","position":14},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"Step 4:  Visualize the data","lvl2":"Step 1: Importing the libraries"},"content":"We are using a simple matplotlib here, but since the data is in numpy array, it can loaded with any python modules that support numpy. Feel free to set the vmin,vmax appropriately.\n\nfig,axes=plt.subplots(1,1,figsize=(12,8))\nim= axes.imshow(data[0,:,:], aspect='auto',origin='lower',cmap='turbo')\ncbar = plt.colorbar(im, ax=axes)\ncbar.set_label('Temperature (deg. C)')\nplt.show()\n\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-4-visualize-the-data","position":15},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"But, what if you want to see the full data for a certain region at a certain depth?","lvl2":"Step 1: Importing the libraries"},"type":"lvl3","url":"/notebooks/ocean-llc4320-visualization#but-what-if-you-want-to-see-the-full-data-for-a-certain-region-at-a-certain-depth","position":16},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl3":"But, what if you want to see the full data for a certain region at a certain depth?","lvl2":"Step 1: Importing the libraries"},"content":"Just set the right x,y,z while reading the data. x and y are the bounding box, z is the depth/layer.\n\ndata1=db.db.read(time=1,z=[0,1],quality=-6,x=[500,2500],y=[8500,11000])\nplt.imshow(data1[0,:,:], origin='lower',cmap='turbo')\nplt.colorbar()\n\ndata1.shape #\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#but-what-if-you-want-to-see-the-full-data-for-a-certain-region-at-a-certain-depth","position":17},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 5: Save the data for the region locally"},"type":"lvl2","url":"/notebooks/ocean-llc4320-visualization#step-5-save-the-data-for-the-region-locally","position":18},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 5: Save the data for the region locally"},"content":"You can save the data locally as you want. For example, here we are only saving the region shown above as a numpy array.\n\nnp.save('test_region.npy', data1)\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-5-save-the-data-for-the-region-locally","position":19},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 6: Load the locally saved region and visualize using matplotlib"},"type":"lvl2","url":"/notebooks/ocean-llc4320-visualization#step-6-load-the-locally-saved-region-and-visualize-using-matplotlib","position":20},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 6: Load the locally saved region and visualize using matplotlib"},"content":"\n\nlocal_data=np.load('test_region.npy')\nplt.imshow(local_data[0,:,:], origin='lower',cmap='turbo')\nplt.colorbar()\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-6-load-the-locally-saved-region-and-visualize-using-matplotlib","position":21},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 7: Horizontal Slicing"},"type":"lvl2","url":"/notebooks/ocean-llc4320-visualization#step-7-horizontal-slicing","position":22},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl2":"Step 7: Horizontal Slicing"},"content":"\n\ndata1=db.db.read(time=1,x=[500,2500],y=[10500,10501])\ndata1.shape\n\n\nplt.figure(figsize=(14,8))\nplt.imshow(data1[:,0,:],cmap='turbo')\n# plt.colorbar()\n\n","type":"content","url":"/notebooks/ocean-llc4320-visualization#step-7-horizontal-slicing","position":23},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl2":"Step 7: Horizontal Slicing"},"type":"lvl4","url":"/notebooks/ocean-llc4320-visualization#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":24},{"hierarchy":{"lvl1":"ECCO LLC4320 Ocean Dataset Notebook","lvl4":"Please reach out to Aashish Panta, Giorgio Scorzelli or Valerio Pascucci for any concerns about the notebook. Thank you!","lvl2":"Step 7: Horizontal Slicing"},"content":"Aashish Panta (\n\naashishpanta0@gmail​.com)\n\nGiorgio Scorzelli (\n\nscrgiorgio@gmail​.com)\n\nValerio Pascucci (\n\npascucci​.valerio@gmail​.com)","type":"content","url":"/notebooks/ocean-llc4320-visualization#please-reach-out-to-aashish-panta-giorgio-scorzelli-or-valerio-pascucci-for-any-concerns-about-the-notebook-thank-you","position":25},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in this Project Pythia Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1}]}