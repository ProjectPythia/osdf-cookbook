{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SonarAI Technical Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Structure\n",
    "\n",
    "This notebook walks through an end-to-end workflow to relate shipboard sonar backscatter (Sv) to local environmental conditions. We (1) open EK60 data from a public NOAA S3 Zarr, (2) gather co-located environmental variables from OISST and IOOS ERDDAP, (3) compute hourly mean Sv, (4) assemble a depth×time error map for reference, and (5) synchronize timestamps to produce an interactive line-plus-heatmap visualization. All selections (time/depth/frequency) and conversions are kept explicit for reproducibility.\n",
    "\n",
    "1. **Imports**\n",
    "   Load core libraries for data access (xarray, s3fs), analysis (numpy, pandas), plotting (plotly), and I/O.\n",
    "\n",
    "2. **Initializing the datasets**\n",
    "   Access HB1906 EK60 Zarr data from public S3; subset by time/depth, select 38 kHz, and mask bins below bottom.\n",
    "\n",
    "3. **Access buoy data**\n",
    "   Define Georges Bank buoy coordinates, sample daily OISST SST at the nearest grid cell (±1 day), and download the model error map (`.npy`).\n",
    "\n",
    "4. **Downloading external error map for the specific location**\n",
    "   Downloading the error map comes from a fixed file\n",
    "\n",
    "5. **Helper Function: Mean Sv (dB)**\n",
    "   Convert Sv from dB→linear, compute mean, convert back to dB.\n",
    "\n",
    "6. **Group Cruise Data into Hourly Chunks**\n",
    "   Add an hourly label and split the EK60 dataset into per-hour `xarray.Dataset` chunks.\n",
    "\n",
    "7. **Compute Hourly Mean Sv & Attach to `env_df`**\n",
    "   Aggregate Sv per hour and append results as a new column in the environmental dataframe.\n",
    "\n",
    "8. **Build Depth×Time Error-Map DataFrame & Align Timestamps**\n",
    "   Construct a depth-by-time matrix from the error map, guard for size mismatches, and align `env_df` endpoints to the heatmap timestamps.\n",
    "\n",
    "9. **Data Visualization: Synchronized Lines + Heatmap**\n",
    "   Plot environmental time series above a depth×time heatmap with shared x-axis; save interactive HTML output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This section was inspired by [this template](https://github.com/alan-turing-institute/the-turing-way/blob/master/book/templates/chapter-template/chapter-landing-page.md) of the wonderful [The Turing Way](https://the-turing-way.netlify.app) Jupyter Book.\n",
    "\n",
    "This notebook opens public NOAA EK60 Zarr data from S3, subsets by time/depth, reads daily OISST SST near a buoy, fetches ERDDAP environmental variables, computes hourly mean **Sv** (dB), aligns with a depth×time error map, and renders synchronized line/heatmap plots.\n",
    "\n",
    "Label the importance of each concept explicitly as **helpful/necessary**.\n",
    "\n",
    "| Concepts | Importance    | Notes                                                                 |\n",
    "| --- |---------------|-----------------------------------------------------------------------|\n",
    "| Xarray + Zarr basics | **Necessary** | Opening Zarr stores, selecting by coords/dims, `.compute()` semantics |\n",
    "| s3fs & public S3 access | **Necessary** | Anonymous reads from AWS S3 (`anon=True`)                             |\n",
    "| Pandas time series | **Necessary** | `DatetimeIndex`, sorting, filtering, timezone-naive vs. aware         |\n",
    "| NumPy fundamentals | **Necessary** | Array slicing, stats, type conversion                                 |\n",
    "| Acoustic backscatter (Sv) & dB averaging | **Necessary** | Convert dB→linear, mean, then linear→dB, Understanding results        |\n",
    "| ERDDAP tabledap & info endpoints | **Helpful**   | Reading CSV responses; unit metadata lookup                           |\n",
    "| Plotly fundamentals | **Helpful**   | Subplots, heatmaps, interactive HTML export                           |\n",
    "| Understanding of NetCDF/CF | **Helpful**   | Variable metadata and geospatial conventions                          |\n",
    "| Dask awareness | **Helpful**   | Lazy arrays; when/why to call `.compute()`                            |\n",
    "| Geographic coordinates | **Helpful**   | 0–360 vs. −180–180 longitude handling                                 |\n",
    "| HTTP/IO with `requests` | **Helpful**   | Downloading `.npy` assets for local use                               |\n",
    "\n",
    "- **Time to learn**: ~75 minutes\n",
    "\n",
    "- **System requirements**:\n",
    "  - Python 3.9+ with Jupyter Notebook/Lab\n",
    "  - Required packages: `xarray`, `s3fs`, `numpy`, `pandas`, `plotly`, `requests`, `netCDF4` *(optional but helpful: `dask`)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Imports\n",
    "Core libraries used throughout the notebook.\n",
    "\n",
    "Key roles:\n",
    "\n",
    "    xarray/s3fs for reading NOAA Zarr data from S3\n",
    "    numpy/pandas for arrays & tables\n",
    "    plotly for interactive plotting\n",
    "    requests/io/os for file I/O and downloads\n",
    "    datetime for time calculations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import xarray as xr\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime, timedelta"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2) Initializing the datasets"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Builds the S3 path to the HB1906 EK60 Zarr dataset and opens it anonymously.\n",
    "Subsets by time window and depth range, selects the 38 kHz channel, and masks samples below the estimated bottom.\n",
    ".compute() materializes the selection; hm_timestamps will be reused for time alignment later. All datasets are accessed\n",
    "using the OSDF infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bucket_name = 'noaa-wcsd-zarr-pds'\n",
    "ship_name = \"Henry_B._Bigelow\"\n",
    "cruise_name = \"HB1906\"\n",
    "sensor_name = \"EK60\"\n",
    "\n",
    "# Accessing the NOAA HB1906 dataset using OSDF (anonymous S3)\n",
    "s3_file_system = s3fs.S3FileSystem(anon=True)\n",
    "zarr_store = f'{cruise_name}.zarr'\n",
    "s3_zarr_store_path = f\"{bucket_name}/level_2/{ship_name}/{cruise_name}/{sensor_name}/{zarr_store}\"\n",
    "\n",
    "# Map S3 path to a zarr store and open (consolidated=None to let xarray infer metadata)\n",
    "store = s3fs.S3Map(root=s3_zarr_store_path, s3=s3_file_system, check=False)\n",
    "cruise = xr.open_zarr(store=store, consolidated=None)\n",
    "\n",
    "# Time/depth subset and single-frequency selection\n",
    "start_time = \"2019-10-16T15:00:00\"\n",
    "end_time = \"2019-10-16T23:30:00\"\n",
    "timeslice = slice(start_time, end_time)\n",
    "depths=slice(10, 250)\n",
    "cruise = cruise.sel(time=timeslice, depth=depths, drop=False)\n",
    "cruise = cruise.sel(frequency=38000, method='nearest').compute()  # materialize after selection\n",
    "cruise = cruise.where(cruise.depth < cruise.bottom, drop=True)    # remove bins below bottom\n",
    "\n",
    "# Timestamps for later alignment\n",
    "hm_timestamps = cruise.time.values.tolist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3) Accessing buoy data\n",
    "Defines a buoy location on Georges Bank (longitude converted to 0–360).\n",
    "1) Optional: Loads three daily OISST files and samples SST at the nearest grid point (day before, day of, day after).\n",
    "2) ERDDAP buoy environmental data.\n",
    "\n",
    "Sets ERDDAP dataset parameters and enforces a max_days cap by adjusting end_date_time if needed.\n",
    "Reads station metadata to extract lon/lat and wind-speed units; prepares a conversion to knots.\n",
    "Pulls a table of time, wind_speed, SST, significant wave height, converts wind speed to knots, indexes by time.\n",
    "Filters to the requested window and keeps the first nine rows (intentional truncation for later alignment)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Location of one specific buoy located on Georges Bank\n",
    "target_lon = 360 - 66.546  # convert from -180..180 to 0..360\n",
    "target_lat = 41.088\n",
    "\n",
    "# ______________OPTIONAL BUOY DATA FROM NCAR/UCAR______________\n",
    "# print(f\"Target coordinates: Longitude: {target_lon}, Latitude: {target_lat}\")\n",
    "#\n",
    "# # Accessing stationary buoy data (daily OISST files); select nearest grid cell\n",
    "# buoy_data_day_before = xr.open_dataset(\n",
    "#     'https://data.rda.ucar.edu/d277007/avhrr_v2.1/2019/oisst-avhrr-v02r01.20191015.nc#mode=bytes', engine='netcdf4')\n",
    "# buoy_data_actual_day = xr.open_dataset(\n",
    "#     'https://data.rda.ucar.edu/d277007/avhrr_v2.1/2019/oisst-avhrr-v02r01.20191016.nc#mode=bytes',\n",
    "#     engine='netcdf4')\n",
    "# buoy_data_day_after = xr.open_dataset(\n",
    "#     'https://data.rda.ucar.edu/d277007/avhrr_v2.1/2019/oisst-avhrr-v02r01.20191017.nc#mode=bytes',\n",
    "#     engine='netcdf4')\n",
    "#\n",
    "# sst_day_before = buoy_data_day_before['sst'].sel(lon=target_lon, lat=target_lat, method='nearest').values[0][0]\n",
    "# sst_actual_day = buoy_data_actual_day['sst'].sel(lon=target_lon, lat=target_lat, method='nearest').values[0][0]\n",
    "# sst_day_after = buoy_data_day_after['sst'].sel(lon=target_lon, lat=target_lat, method='nearest').values[0][0]\n",
    "# ______________________________________________________________\n",
    "\n",
    "# The following code has been copied from https://github.com/ioos/soundcoop/blob/main/3_HMD_environmental_data/plot_sound_environmental_and_climatology_data.ipynb in order to access buoy environmental data.\n",
    "\n",
    "erddap_dataset = 'gov-ndbc-44005'\n",
    "sound_dataset = 'Monh'\n",
    "max_days = 25\n",
    "start_date_time = '2019-10-16T14:00:00.000'\n",
    "end_date_time = '2021-10-16T23:30:00.000'\n",
    "min_frequency = 21\n",
    "max_frequency = 24000\n",
    "\n",
    "erddap_base_url = 'https://erddap.sensors.ioos.us/erddap'\n",
    "\n",
    "# Cap the end date if requested range exceeds max_days\n",
    "time_delta = datetime.fromisoformat(end_date_time) - datetime.fromisoformat(start_date_time)\n",
    "if time_delta.days > max_days:\n",
    "    end_date_time = str(datetime.fromisoformat(start_date_time) + timedelta(days=max_days))\n",
    "    print(f'end_date_time updated to {end_date_time}')\n",
    "\n",
    "# Get station lon/lat and units from ERDDAP metadata (CSV)\n",
    "erddap_metadata_url = f'{erddap_base_url}/info/{erddap_dataset}/index.csv'\n",
    "env_metadata_df = pd.read_csv(erddap_metadata_url)\n",
    "\n",
    "env_station_x = env_metadata_df.loc[env_metadata_df['Attribute Name'] == 'geospatial_lon_min']['Value'].item()\n",
    "env_station_y = env_metadata_df.loc[env_metadata_df['Attribute Name'] == 'geospatial_lat_min']['Value'].item()\n",
    "\n",
    "# Determine wind_speed units to convert to knots\n",
    "wind_speed_units_row = env_metadata_df[\n",
    "    (env_metadata_df['Row Type'] == 'attribute') &\n",
    "    (env_metadata_df['Attribute Name'] == 'units') &\n",
    "    (env_metadata_df['Variable Name'] == 'wind_speed')\n",
    "    ]\n",
    "wind_speed_units = wind_speed_units_row.iloc[0]['Value']\n",
    "print(wind_speed_units)\n",
    "\n",
    "wind_speed_to_kts_factors = {\n",
    "    \"m.s-1\": 1.94384,\n",
    "    \"mph\": 0.86897423357831,\n",
    "    \"kmh\": 0.53995555554212126825,\n",
    "    \"ft.s-1\": 0.59248243198521155506\n",
    "}\n",
    "\n",
    "if wind_speed_units in wind_speed_to_kts_factors:\n",
    "    print(\"Success! Units can be converted from\", wind_speed_units, 'to', 'kts')\n",
    "else:\n",
    "    print(\"Error! Wind speed cannot be converted from\", wind_speed_units, 'to', 'kts')\n",
    "\n",
    "wind_var = 'wind_speed'\n",
    "swt_var = 'sea_surface_temperature'\n",
    "wave_var = 'sea_surface_wave_significant_height'\n",
    "anomaly_var = 'swt_anomaly'\n",
    "wind_var_kts = 'wind_speed_kts'\n",
    "\n",
    "# Build ERDDAP tabledap query URL\n",
    "erddap_dataset_url = (\n",
    "    f'{erddap_base_url}/tabledap/{erddap_dataset}.csv'\n",
    "    f'?time,{wind_var},{swt_var},{wave_var}'\n",
    ")\n",
    "\n",
    "# Read dataset (skip the second row of units)\n",
    "env_df = pd.read_csv(\n",
    "    erddap_dataset_url,\n",
    "    skiprows=[1]  # The second row (index 1) are the column units, which we don't need\n",
    ")\n",
    "\n",
    "# Format time, convert wind speed to knots, index by time\n",
    "env_df['time'] = pd.to_datetime(env_df['time'])\n",
    "env_df['wind_speed_kts'] = env_df['wind_speed'].apply(lambda x: x * wind_speed_to_kts_factors[wind_speed_units])\n",
    "del env_df['wind_speed']\n",
    "env_df = env_df.set_index('time').sort_index()\n",
    "\n",
    "# Filter by requested time window and keep first 9 rows (drops the rest)\n",
    "env_df = env_df[(env_df.index > start_date_time) & (env_df.index < end_date_time)]\n",
    "env_df.drop(env_df.tail(-9).index,inplace=True)\n",
    "# env_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4) Downloading external error map for the specific location.\n",
    "Currently the error map comes from a fixed file; our plan is to switch to a dynamic AWS download that accepts location parameters."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Downloading anomaly detection model error map from NCAR via OSDF\n",
    "response = requests.get('https://data-osdf.rda.ucar.edu/ncar/rda/pythia_2025/osdf-cookbook/mae_error_map.npy')\n",
    "response.raise_for_status()\n",
    "error_map = np.load(io.BytesIO(response.content))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Helper: mean Sv in dB\n",
    "Computes the mean of Sv correctly by converting dB → linear, averaging, then linear → dB.\n",
    "Accepts array-like input (NumPy/xarray/dask); returns a scalar in dB."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_sv_mean(input_sv):\n",
    "    # Convert dB to linear, mean in linear space, convert back to dB\n",
    "    sv = 10. ** (input_sv / 10.)\n",
    "    return 10 * np.log10(np.mean(sv))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6) Group cruise data into hourly chunks\n",
    "Adds an hourly label and groups the cruise data by hour.\n",
    "Produces a list of per-hour xarray.Dataset chunks for downstream aggregation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cruise['time_hour'] = cruise['time'].dt.floor('1h')  # hourly bin label\n",
    "\n",
    "# Group by each hour\n",
    "grouped = cruise.groupby('time_hour')\n",
    "\n",
    "# Extract each 1-hour Dataset as a chunk (drop helper label)\n",
    "chunks = [group.drop_vars('time_hour') for _, group in grouped]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7) Compute hourly mean Sv and attach to env_df\n",
    "Iterates over hourly chunks, computes mean Sv per hour using calculate_sv_mean.\n",
    "Converts dask→NumPy→Python float and appends to a list.\n",
    "Assigns the resulting hourly series to env_df[\"sv_hourly\"].\n",
    "Assumes the number/order of hours matches rows retained in env_df."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sv_hourly = []\n",
    "timestamps = []\n",
    "\n",
    "for i in range(0,len(chunks)):\n",
    "    sv_data = chunks[i]['Sv']\n",
    "    result = calculate_sv_mean(sv_data)\n",
    "\n",
    "    # Use first time in hour as representative timestamp\n",
    "    ts = pd.to_datetime(chunks[i]['time'].values[0])\n",
    "    result = result.compute()           # dask -> numpy\n",
    "    result = float(result.values)       # numpy -> Python float\n",
    "\n",
    "    sv_hourly.append(result)\n",
    "\n",
    "env_df[\"sv_hourly\"] = sv_hourly"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8) Build (depth × time) error-map DataFrame and align timestamps\n",
    "Extracts one channel from error_map and pairs it with cruise depths and timestamps to form a DataFrame.\n",
    "Uses min(...) to guard against size mismatches in depth/time dimensions.\n",
    "Aligns only the first and last timestamps in env_df to the heatmap’s time range (keeps interior indices unchanged, sets UTC)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare axes\n",
    "depths = np.asarray(cruise.depth.values)\n",
    "times  = pd.to_datetime(hm_timestamps)\n",
    "\n",
    "# Select channel/slice from error_map\n",
    "vals = error_map[:, :, 1]  # (1088, 28096)\n",
    "\n",
    "# Guard against mismatched sizes\n",
    "n_depth = min(len(depths), vals.shape[0])\n",
    "n_time  = min(len(times),  vals.shape[1])\n",
    "\n",
    "# DataFrame: rows=depths, cols=timestamps\n",
    "df = pd.DataFrame(\n",
    "    data=vals[:n_depth, :n_time],\n",
    "    index=depths[:n_depth],\n",
    "    columns=times[:n_time]\n",
    ")\n",
    "\n",
    "# Align env_df index endpoints to heatmap timestamps (keeps interior unchanged)\n",
    "idx = env_df.index.tolist()\n",
    "df_timestamps = pd.to_datetime(df.columns).tz_localize(None)\n",
    "idx[0] = pd.Timestamp(df_timestamps.values[0], tz='UTC').floor(\"s\")\n",
    "idx[-1] = pd.Timestamp(df_timestamps.values[-1], tz='UTC').floor(\"s\")\n",
    "env_df.index = idx"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9) Data Visualization: Synchronized Lines + Heatmap"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots synchronized data: top = time series from line_df; bottom = depth×time heatmap from heatmap_df.\n",
    "Expects line_df to have a DatetimeIndex (timezone-naive or converted).\n",
    "Depth axis is reversed (surface at top).\n",
    "Saves an interactive HTML file to the parent directory (out.html) and shows the figure if show=True."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_synchronized_heatmaps_from_df(\n",
    "    heatmap_df: pd.DataFrame,  # rows = depths, columns = timestamps\n",
    "    line_df: pd.DataFrame,\n",
    "    colorscale: str = \"Reds\",\n",
    "    show_markers: bool = False,\n",
    "    show: bool = False,\n",
    "):\n",
    "    # --- Validate line_df ---\n",
    "    if not isinstance(line_df.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"line_df must have a DatetimeIndex\")\n",
    "    line_df = line_df.copy()\n",
    "    if line_df.index.tz is not None:\n",
    "        line_df.index = line_df.index.tz_convert(None)\n",
    "\n",
    "    # --- Extract axis values from heatmap_df ---\n",
    "    depths = np.asarray(heatmap_df.index)\n",
    "    heatmap_timestamps = pd.to_datetime(heatmap_df.columns)\n",
    "    z = heatmap_df.to_numpy()\n",
    "\n",
    "    # --- Build figure ---\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.03,\n",
    "        row_heights=[0.3, 0.7]  # smaller line plot, bigger heatmap\n",
    "    )\n",
    "\n",
    "    # Top panel: line plots\n",
    "    mode = \"lines+markers\" if show_markers else \"lines\"\n",
    "    for col in line_df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=line_df.index,\n",
    "                y=line_df[col],\n",
    "                name=str(col),\n",
    "                mode=mode\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "    # Bottom panel: heatmap\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=z,\n",
    "            x=heatmap_timestamps,\n",
    "            y=depths,\n",
    "            colorscale=colorscale,\n",
    "            zmin=np.nanmin(z),\n",
    "            zmax=np.nanmax(z),\n",
    "            colorbar_len=0.28,\n",
    "            colorbar_y=0.14,\n",
    "            hovertemplate=\"t=%{x}<br>depth=%{y}<br>value=%{z}<extra></extra>\",\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(autorange=\"reversed\", row=2, col=1, title_text=\"Depth\")\n",
    "    fig.update_layout(\n",
    "        legend_title_text=\"Signals\",\n",
    "        margin=dict(l=60, r=40, t=40, b=40),\n",
    "        hovermode=\"x unified\",\n",
    "        template=\"plotly_white\",\n",
    "        height=700\n",
    "    )\n",
    "\n",
    "    # Save/show\n",
    "    save_path = os.path.join(os.path.dirname(os.getcwd()), \"out.html\")  # saves HTML in parent dir\n",
    "    fig.write_html(save_path)\n",
    "    print(f\"Plot saved to: {save_path}\")\n",
    "    if show:\n",
    "        fig.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Create synchronized line+heatmap figure\n",
    "fig = plot_synchronized_heatmaps_from_df(heatmap_df=df, line_df=env_df, show=True)\n",
    "\n",
    "# fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ]
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
