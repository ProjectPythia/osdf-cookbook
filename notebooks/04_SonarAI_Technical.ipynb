{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SonarAI Technical Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "In this notebook, we aim to investigate potential correlations between environmental variables derived from NCAR datasets—such as **Sea Surface Temperature** and **Mean Evaporation Rate**(tbd) — and the **abundance of fish observed within the water column**, as recorded by sonar.\n",
    "\n",
    "The **sonar dataset** was collected by **NOAA Fisheries** during a research cruise conducted aboard the vessel *H.B. Bigelow* between **October and November 2019**. To facilitate analysis, we integrate this dataset with two NCAR environmental datasets (*[tbd]* and *[tbd]*), aligning them both **spatially** and **temporally**.\n",
    "\n",
    "This integration enables us to examine statistical relationships between physical oceanographic conditions and fish distribution patterns.\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "1. **Imports**\n",
    "2. **Data Loading**\n",
    "2. **Data Preprocessing**\n",
    "3. **Data Visualization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This section was inspired by [this template](https://github.com/alan-turing-institute/the-turing-way/blob/master/book/templates/chapter-template/chapter-landing-page.md) of the wonderful [The Turing Way](https://the-turing-way.netlify.app) Jupyter Book.\n",
    "\n",
    "Following your overview, tell your reader what concepts, packages, or other background information they'll **need** before learning your material. Tie this explicitly with links to other pages here in Foundations or to relevant external resources. Remove this body text, then populate the Markdown table, denoted in this cell with `|` vertical brackets, below, and fill out the information following. In this table, lay out prerequisite concepts by explicitly linking to other Foundations material or external resources, or describe generally helpful concepts.\n",
    "\n",
    "Label the importance of each concept explicitly as **helpful/necessary**.\n",
    "\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Intro to Cartopy](https://foundations.projectpythia.org/core/cartopy/cartopy) | Necessary | |\n",
    "| [Understanding of NetCDF](https://foundations.projectpythia.org/core/data-formats/netcdf-cf) | Helpful | Familiarity with metadata structure |\n",
    "| Project management | Helpful | |\n",
    "\n",
    "- **Time to learn**: estimate in minutes. For a rough idea, use 5 mins per subsection, 10 if longer; add these up for a total. Safer to round up and overestimate.\n",
    "- **System requirements**:\n",
    "    - Populate with any system, version, or non-Python software requirements if necessary\n",
    "    - Otherwise use the concepts table above and the Imports section below to describe required packages as necessary\n",
    "    - If no extra requirements, remove the **System requirements** point altogether"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Begin your body of content with another `---` divider before continuing into this section, then remove this body text and populate the following code cell with all necessary Python imports **up-front**:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import xarray as xr\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime, timedelta"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All datasets are accessed using the OSDF infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bucket_name = 'noaa-wcsd-zarr-pds'\n",
    "ship_name = \"Henry_B._Bigelow\"\n",
    "cruise_name = \"HB1906\"\n",
    "sensor_name = \"EK60\"\n",
    "\n",
    "# Accessing the NOAA HB1906 dataset using OSDF\n",
    "s3_file_system = s3fs.S3FileSystem(anon=True)\n",
    "zarr_store = f'{cruise_name}.zarr'\n",
    "s3_zarr_store_path = f\"{bucket_name}/level_2/{ship_name}/{cruise_name}/{sensor_name}/{zarr_store}\"\n",
    "store = s3fs.S3Map(root=s3_zarr_store_path, s3=s3_file_system, check=False)\n",
    "cruise = xr.open_zarr(store=store, consolidated=None)\n",
    "start_time = \"2019-10-16T15:00:00\"\n",
    "end_time = \"2019-10-16T23:30:00\"\n",
    "timeslice = slice(start_time, end_time)\n",
    "depths=slice(10, 250)\n",
    "cruise = cruise.sel(time=timeslice, depth=depths, drop=False)\n",
    "cruise = cruise.sel(frequency=38000, method='nearest').compute()\n",
    "cruise = cruise.where(cruise.depth < cruise.bottom, drop=True)\n",
    "hm_timestamps = cruise.time.values.tolist()\n",
    "\n",
    "# location of one specific buoy located on Georges Bank\n",
    "target_lon = 360 - 66.546\n",
    "target_lat = 41.088\n",
    "print(f\"Target coordinates: Longitude: {target_lon}, Latitude: {target_lat}\")\n",
    "# Accessing stationary buoy data from a specific buoy located on Georges Bank, sampled daily\n",
    "bouy_data_day_before = xr.open_dataset(\n",
    "    'https://data.rda.ucar.edu/d277007/avhrr_v2.1/2019/oisst-avhrr-v02r01.20191015.nc#mode=bytes', engine='netcdf4')\n",
    "buoy_data_actual_day = xr.open_dataset(\n",
    "    'https://data.rda.ucar.edu/d277007/avhrr_v2.1/2019/oisst-avhrr-v02r01.20191016.nc#mode=bytes',\n",
    "    engine='netcdf4')\n",
    "buoy_data_day_after = xr.open_dataset(\n",
    "    'https://data.rda.ucar.edu/d277007/avhrr_v2.1/2019/oisst-avhrr-v02r01.20191017.nc#mode=bytes',\n",
    "    engine='netcdf4')\n",
    "\n",
    "sst_day_before = bouy_data_day_before['sst'].sel(lon=target_lon, lat=target_lat, method='nearest').values[0][0]\n",
    "sst_actual_day = buoy_data_actual_day['sst'].sel(lon=target_lon, lat=target_lat, method='nearest').values[0][0]\n",
    "sst_day_after = buoy_data_day_after['sst'].sel(lon=target_lon, lat=target_lat, method='nearest').values[0][0]\n",
    "# Downloading anomaly detection model from NCAR via OSDF\n",
    "response = requests.get('https://data-osdf.rda.ucar.edu/ncar/rda/pythia_2025/osdf-cookbook/mae_error_map.npy')\n",
    "response.raise_for_status()\n",
    "error_map = np.load(io.BytesIO(response.content))"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# This code has been copied from https://github.com/ioos/soundcoop/blob/main/3_HMD_environmental_data/plot_sound_environmental_and_climatology_data.ipynb in order to access buoy environmental data.\n",
    "erddap_dataset = 'gov-ndbc-44005'\n",
    "sound_dataset = 'Monh'\n",
    "max_days = 25\n",
    "start_date_time = '2019-10-16T14:00:00.000'\n",
    "end_date_time = '2021-10-16T23:30:00.000'\n",
    "min_frequency = 21\n",
    "max_frequency = 24000\n",
    "\n",
    "erddap_base_url = 'https://erddap.sensors.ioos.us/erddap'\n",
    "# Update end_date_time if defined temporal range exceeds max_days\n",
    "time_delta = datetime.fromisoformat(end_date_time) - datetime.fromisoformat(start_date_time)\n",
    "if time_delta.days > max_days:\n",
    "    end_date_time = str(datetime.fromisoformat(start_date_time) + timedelta(days=max_days))\n",
    "    print(f'end_date_time updated to {end_date_time}')\n",
    "\n",
    "# Get environmental sensor station lat/lon for use in mapping and querying water temperature climatology data\n",
    "erddap_metadata_url = f'{erddap_base_url}/info/{erddap_dataset}/index.csv'\n",
    "env_metadata_df = pd.read_csv(erddap_metadata_url)\n",
    "\n",
    "env_station_x = env_metadata_df.loc[env_metadata_df['Attribute Name'] == 'geospatial_lon_min']['Value'].item()\n",
    "env_station_y = env_metadata_df.loc[env_metadata_df['Attribute Name'] == 'geospatial_lat_min']['Value'].item()\n",
    "\n",
    "wind_speed_units_row = env_metadata_df[\n",
    "    (env_metadata_df['Row Type'] == 'attribute') &\n",
    "    (env_metadata_df['Attribute Name'] == 'units') &\n",
    "    (env_metadata_df['Variable Name'] == 'wind_speed')\n",
    "    ]\n",
    "wind_speed_units = wind_speed_units_row.iloc[0]['Value']\n",
    "print(wind_speed_units)\n",
    "\n",
    "wind_speed_to_kts_factors = {\n",
    "    \"m.s-1\": 1.94384,\n",
    "    \"mph\": 0.86897423357831,\n",
    "    \"kmh\": 0.53995555554212126825,\n",
    "    \"ft.s-1\": 0.59248243198521155506\n",
    "}\n",
    "\n",
    "if wind_speed_units in wind_speed_to_kts_factors:\n",
    "    print(\"Success! Units can be converted from\", wind_speed_units, 'to', 'kts')\n",
    "else:\n",
    "    print(\"Error! Wind speed cannot be converted from\", wind_speed_units, 'to', 'kts')\n",
    "\n",
    "wind_var = 'wind_speed'\n",
    "swt_var = 'sea_surface_temperature'\n",
    "wave_var = 'sea_surface_wave_significant_height'\n",
    "anomaly_var = 'swt_anomaly'\n",
    "wind_var_kts = 'wind_speed_kts'\n",
    "\n",
    "erddap_dataset_url = (\n",
    "    f'{erddap_base_url}/tabledap/{erddap_dataset}.csv'\n",
    "    f'?time,{wind_var},{swt_var},{wave_var}'\n",
    ")\n",
    "\n",
    "env_df = pd.read_csv(\n",
    "    erddap_dataset_url,\n",
    "    skiprows=[1]  # The second row (index 1) are the column units, which we don't need\n",
    ")\n",
    "\n",
    "# Format the time field and set it as the index\n",
    "env_df['time'] = pd.to_datetime(env_df['time'])\n",
    "env_df['wind_speed_kts'] = env_df['wind_speed'].apply(lambda x: x * wind_speed_to_kts_factors[wind_speed_units])\n",
    "del env_df['wind_speed']\n",
    "env_df = env_df.set_index('time').sort_index()\n",
    "\n",
    "#filtering by timestamps\n",
    "env_df = env_df[(env_df.index > start_date_time) & (env_df.index < end_date_time)]\n",
    "env_df.drop(env_df.tail(-9).index,inplace=True)\n",
    "# env_df"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_sv_mean(input_sv):\n",
    "    sv = 10. ** (input_sv / 10.)\n",
    "    return 10 * np.log10(np.mean(sv))"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cruise['time_hour'] = cruise['time'].dt.floor('1h')\n",
    "\n",
    "# Step 2: Group by each hour\n",
    "grouped = cruise.groupby('time_hour')\n",
    "\n",
    "# Step 3: Extract each 1-hour Dataset as a chunk\n",
    "chunks = [group.drop_vars('time_hour') for _, group in grouped]"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sv_hourly = []\n",
    "timestamps = []\n",
    "\n",
    "for i in range(0,len(chunks)):\n",
    "    sv_data = chunks[i]['Sv']\n",
    "    result = calculate_sv_mean(sv_data)\n",
    "\n",
    "    ts = pd.to_datetime(chunks[i]['time'].values[0])\n",
    "    result = result.compute()\n",
    "    result = float(result.values)\n",
    "\n",
    "    sv_hourly.append(result)\n",
    "\n",
    "env_df[\"sv_hourly\"] = sv_hourly"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "depths = np.asarray(cruise.depth.values)\n",
    "times  = pd.to_datetime(hm_timestamps)\n",
    "\n",
    "vals = error_map[:, :, 1]  # (1088, 28096)\n",
    "\n",
    "n_depth = min(len(depths), vals.shape[0])\n",
    "n_time  = min(len(times),  vals.shape[1])\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data=vals[:n_depth, :n_time],\n",
    "    index=depths[:n_depth],\n",
    "    columns=times[:n_time]\n",
    ")\n",
    "\n",
    "# Get current index from env_df\n",
    "idx = env_df.index.tolist()\n",
    "\n",
    "# Use the timestamps from df (columns) instead of hm_timestamps\n",
    "df_timestamps = pd.to_datetime(df.columns).tz_localize(None)\n",
    "\n",
    "# Replace first and last index entries in env_df\n",
    "idx[0] = pd.Timestamp(df_timestamps.values[0], tz='UTC').floor(\"s\")\n",
    "idx[-1] = pd.Timestamp(df_timestamps.values[-1], tz='UTC').floor(\"s\")\n",
    "\n",
    "# Assign back to env_df\n",
    "env_df.index = idx"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decription"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_synchronized_heatmaps_from_df(\n",
    "    heatmap_df: pd.DataFrame,  # rows = depths, columns = timestamps\n",
    "    line_df: pd.DataFrame,\n",
    "    colorscale: str = \"Reds\",\n",
    "    show_markers: bool = False,\n",
    "    show: bool = True,\n",
    "):\n",
    "    # --- Validate line_df ---\n",
    "    if not isinstance(line_df.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"line_df must have a DatetimeIndex\")\n",
    "    line_df = line_df.copy()\n",
    "    if line_df.index.tz is not None:\n",
    "        line_df.index = line_df.index.tz_convert(None)\n",
    "\n",
    "    # --- Extract axis values from heatmap_df ---\n",
    "    depths = np.asarray(heatmap_df.index)\n",
    "    heatmap_timestamps = pd.to_datetime(heatmap_df.columns)\n",
    "    z = heatmap_df.to_numpy()\n",
    "\n",
    "    # --- Build figure ---\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.03,\n",
    "        row_heights=[0.3, 0.7]  # smaller line plot, bigger heatmap\n",
    "    )\n",
    "\n",
    "    # Top panel: line plots\n",
    "    mode = \"lines+markers\" if show_markers else \"lines\"\n",
    "    for col in line_df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=line_df.index,\n",
    "                y=line_df[col],\n",
    "                name=str(col),\n",
    "                mode=mode\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "    # Bottom panel: heatmap\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=z,\n",
    "            x=heatmap_timestamps,\n",
    "            y=depths,\n",
    "            colorscale=colorscale,\n",
    "            zmin=np.nanmin(z),\n",
    "            zmax=np.nanmax(z),\n",
    "            colorbar_len=0.28,\n",
    "            colorbar_y=0.14,\n",
    "            hovertemplate=\"t=%{x}<br>depth=%{y}<br>value=%{z}<extra></extra>\",\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(autorange=\"reversed\", row=2, col=1, title_text=\"Depth\")\n",
    "    fig.update_layout(\n",
    "        legend_title_text=\"Signals\",\n",
    "        margin=dict(l=60, r=40, t=40, b=40),\n",
    "        hovermode=\"x unified\",\n",
    "        template=\"plotly_white\",\n",
    "        height=700\n",
    "    )\n",
    "\n",
    "    # Save/show\n",
    "    save_path = os.path.join(os.path.dirname(os.getcwd()), \"out.html\")\n",
    "    fig.write_html(save_path)\n",
    "    print(f\"Plot saved to: {save_path}\")\n",
    "    if show:\n",
    "        fig.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "# env_df = repeat_first_drop_last(env_df)\n",
    "fig = plot_synchronized_heatmaps_from_df(heatmap_df=df, line_df=env_df)\n",
    "\n",
    "# fig.show()"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ]
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
