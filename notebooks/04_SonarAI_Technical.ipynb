{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SonarAI Technical Notebook"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "In this notebook, we aim to investigate potential correlations between environmental variables derived from NCAR datasets—such as **Sea Surface Temperature** and **Mean Evaporation Rate**(tbd) — and the **abundance of fish observed within the water column**, as recorded by sonar.\n",
    "\n",
    "The **sonar dataset** was collected by **NOAA Fisheries** during a research cruise conducted aboard the vessel *H.B. Bigelow* between **October and November 2019**. To facilitate analysis, we integrate this dataset with two NCAR environmental datasets (*[Dataset Name 1]* and *[Dataset Name 2]*), aligning them both **spatially** and **temporally**.\n",
    "\n",
    "This integration enables us to examine statistical relationships between physical oceanographic conditions and fish distribution patterns.\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "1. **Imports**\n",
    "2. **Data Loading**\n",
    "2. **Data Preprocessing**\n",
    "3. **Data Visualization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "If you have an introductory paragraph, lead with it here! Keep it short and tied to your material, then be sure to continue into the required list of topics below,\n",
    "\n",
    "1. This is a numbered list of the specific topics\n",
    "1. These should map approximately to your main sections of content\n",
    "1. Or each second-level, `##`, header in your notebook\n",
    "1. Keep the size and scope of your notebook in check\n",
    "1. And be sure to let the reader know up front the important concepts they'll be leaving with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This section was inspired by [this template](https://github.com/alan-turing-institute/the-turing-way/blob/master/book/templates/chapter-template/chapter-landing-page.md) of the wonderful [The Turing Way](https://the-turing-way.netlify.app) Jupyter Book.\n",
    "\n",
    "Following your overview, tell your reader what concepts, packages, or other background information they'll **need** before learning your material. Tie this explicitly with links to other pages here in Foundations or to relevant external resources. Remove this body text, then populate the Markdown table, denoted in this cell with `|` vertical brackets, below, and fill out the information following. In this table, lay out prerequisite concepts by explicitly linking to other Foundations material or external resources, or describe generally helpful concepts.\n",
    "\n",
    "Label the importance of each concept explicitly as **helpful/necessary**.\n",
    "\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Intro to Cartopy](https://foundations.projectpythia.org/core/cartopy/cartopy) | Necessary | |\n",
    "| [Understanding of NetCDF](https://foundations.projectpythia.org/core/data-formats/netcdf-cf) | Helpful | Familiarity with metadata structure |\n",
    "| Project management | Helpful | |\n",
    "\n",
    "- **Time to learn**: estimate in minutes. For a rough idea, use 5 mins per subsection, 10 if longer; add these up for a total. Safer to round up and overestimate.\n",
    "- **System requirements**:\n",
    "    - Populate with any system, version, or non-Python software requirements if necessary\n",
    "    - Otherwise use the concepts table above and the Imports section below to describe required packages as necessary\n",
    "    - If no extra requirements, remove the **System requirements** point altogether"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Begin your body of content with another `---` divider before continuing into this section, then remove this body text and populate the following code cell with all necessary Python imports **up-front**:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import xarray as xr\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Initializing the datasets"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "All datasets are accessed using the OSDF infrastructure"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "bucket_name = 'noaa-wcsd-zarr-pds'\n",
    "ship_name = \"Henry_B._Bigelow\"\n",
    "cruise_name = \"HB1906\"\n",
    "sensor_name = \"EK60\"\n",
    "\n",
    "# Accessing the NOAA HB1906 dataset using OSDF\n",
    "s3_file_system = s3fs.S3FileSystem(anon=True)\n",
    "zarr_store = f'{cruise_name}.zarr'\n",
    "s3_zarr_store_path = f\"{bucket_name}/level_2/{ship_name}/{cruise_name}/{sensor_name}/{zarr_store}\"\n",
    "store = s3fs.S3Map(root=s3_zarr_store_path, s3=s3_file_system, check=False)\n",
    "cruise = xr.open_zarr(store=store, consolidated=None)\n",
    "start_time = \"2019-10-16T15:00:00\"\n",
    "end_time = \"2019-10-16T23:30:00\"\n",
    "timeslice = slice(start_time, end_time)\n",
    "depths=slice(10, 250)\n",
    "cruise = cruise.sel(time=timeslice, depth=depths, drop=False)\n",
    "cruise = cruise.sel(frequency=38000, method='nearest')\n",
    "hm_timestamps = cruise.time.values.tolist()\n",
    "\n",
    "# location of one specific buoy located on Georges Bank\n",
    "target_lon = 360 - 66.546\n",
    "target_lat = 41.088\n",
    "print(f\"Target coordinates: Longitude: {target_lon}, Latitude: {target_lat}\")\n",
    "# Accessing stationary buoy data from a specific buoy located on Georges Bank, sampled daily\n",
    "bouy_data_day_before = xr.open_dataset(\n",
    "    'https://data.rda.ucar.edu/d277007/avhrr_v2.1/2019/oisst-avhrr-v02r01.20191015.nc#mode=bytes', engine='netcdf4')\n",
    "buoy_data_actual_day = xr.open_dataset(\n",
    "    'https://data.rda.ucar.edu/d277007/avhrr_v2.1/2019/oisst-avhrr-v02r01.20191016.nc#mode=bytes',\n",
    "    engine='netcdf4')\n",
    "buoy_data_day_after = xr.open_dataset(\n",
    "    'https://data.rda.ucar.edu/d277007/avhrr_v2.1/2019/oisst-avhrr-v02r01.20191017.nc#mode=bytes',\n",
    "    engine='netcdf4')\n",
    "\n",
    "sst_day_before = bouy_data_day_before['sst'].sel(lon=target_lon, lat=target_lat, method='nearest').values[0][0]\n",
    "sst_actual_day = buoy_data_actual_day['sst'].sel(lon=target_lon, lat=target_lat, method='nearest').values[0][0]\n",
    "sst_day_after = buoy_data_day_after['sst'].sel(lon=target_lon, lat=target_lat, method='nearest').values[0][0]\n",
    "# Accessing a second dataset from NCAR, gridded satellite data, sampled hourly\n",
    "url = 'https://data-osdf.rda.ucar.edu/ncar/rda/d633000/kerchunk/meanflux/Mean_evaporation_rate-osdf.json'\n",
    "ds = xr.open_dataset(url, engine='kerchunk')\n",
    "\n",
    "response = requests.get('https://data-osdf.rda.ucar.edu/ncar/rda/pythia_2025/osdf-cookbook/mae_error_map.npy')\n",
    "response.raise_for_status()\n",
    "error_map = np.load(io.BytesIO(response.content))\n",
    "# return sst_day_before, sst_actual_day, sst_day_after, cruise"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mer = ds.sel(longitude=target_lon, latitude=target_lat, method='nearest')\n",
    "# print(subset.forecast_initial_time[:5].values)\n",
    "mer = mer.sel(forecast_initial_time=slice(start_time, end_time))\n",
    "mer = mer.MER.head(9).values.tolist()[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_sv_mean(input_sv):\n",
    "    sv = 10. ** (input_sv / 10.)\n",
    "    return 10 * np.log10(np.mean(sv))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cruise['time_hour'] = cruise['time'].dt.floor('1h')\n",
    "\n",
    "# Step 2: Group by each hour\n",
    "grouped = cruise.groupby('time_hour')\n",
    "\n",
    "# Step 3: Extract each 1-hour Dataset as a chunk\n",
    "chunks = [group.drop_vars('time_hour') for _, group in grouped]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sv_hourly = []\n",
    "timestamps = []\n",
    "\n",
    "for i in range(0,len(chunks)):\n",
    "    sv_data = chunks[i]['Sv']\n",
    "    result = calculate_sv_mean(sv_data)\n",
    "\n",
    "    # Optional: get the first timestamp in the chunk as label\n",
    "    timestamps.append(pd.to_datetime(chunks[i]['time'].values[0]))\n",
    "    result = result.compute()\n",
    "    result = float(result.values)\n",
    "    # print(result)\n",
    "\n",
    "    sv_hourly.append(result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualization"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Decription"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_synchronized_heatmaps(\n",
    "        # matrix1: np.ndarray,\n",
    "        heatmap_timestamps: np.ndarray,\n",
    "        anomalies: np.ndarray = None,\n",
    "        correlated_variables: list = None,\n",
    "        depths: np.ndarray = None,\n",
    "        timestamps: np.ndarray = None,\n",
    "        # title1: str = \"Input Signal\",\n",
    "        # title2: str = \"Reconstructed Signal\",\n",
    "        # title3: str = \"Anomaly Heatmap\",\n",
    "        colorscale: str = 'Reds',\n",
    "):\n",
    "\n",
    "    # --- Create Subplots ---\n",
    "    import os\n",
    "    print (heatmap_timestamps)\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        shared_xaxes=True,\n",
    "        shared_yaxes=True,\n",
    "        vertical_spacing=0.03\n",
    "    )\n",
    "\n",
    "\n",
    "    cb_len1, cb_y1 = 0.28, 0.86\n",
    "    cb_len3, cb_y3 = 0.28, 0.14\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=correlated_variables[0], y=[correlated_variables[1], correlated_variables[2]]),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    # --- Add Heatmaps ---\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=anomalies.astype(int),\n",
    "            colorscale=colorscale,\n",
    "            zmin=anomalies.min(),\n",
    "            zmax=anomalies.max(),\n",
    "            colorbar_len=cb_len3,\n",
    "            colorbar_y=cb_y3,\n",
    "            # name=\"Correlations\",\n",
    "            showscale=True,\n",
    "            x=heatmap_timestamps,\n",
    "            y=depths,\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    # --- Layout Titles ---\n",
    "    # fig.update_layout(\n",
    "    #     title_text=\"Signal Comparison\",\n",
    "    #     title_x=0.5,\n",
    "    #     yaxis_title=title1,\n",
    "    #     yaxis2_title=title2,\n",
    "    #     yaxis_autorange='reversed',\n",
    "    #     yaxis2_autorange='reversed'\n",
    "    # )\n",
    "\n",
    "    # fig.update_layout(\n",
    "    #     yaxis3_title=title3,\n",
    "    #     yaxis3_autorange='reversed'\n",
    "    # )\n",
    "\n",
    "    # # --- Explicit Axis Matching for Zoom Synchronization ---\n",
    "    # fig.update_layout(\n",
    "    #     xaxis=dict(matches='x'),\n",
    "    #     xaxis2=dict(matches='x'),\n",
    "    #     yaxis=dict(matches='y'),\n",
    "    #     yaxis2=dict(matches='y'),\n",
    "    # )\n",
    "\n",
    "    # --- Show or Save ---\n",
    "    # if save_path:\n",
    "    try:\n",
    "        save_dir = os.path.dirname(os.getcwd())\n",
    "        if save_dir and not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        fig.write_html(save_dir + '/out.html')\n",
    "        print(\"Plot saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving plot: {e}\")\n",
    "    # else:\n",
    "    fig.show()\n",
    "# plot_synchronized_heatmaps(heatmap_timestamps=hm_timestamps, anomalies=error_map, correlated_variables=[timestamps, sv_hourly, mer], depths=depths)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ]
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
