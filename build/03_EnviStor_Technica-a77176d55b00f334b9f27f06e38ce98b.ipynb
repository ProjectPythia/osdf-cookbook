{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff23a7f0",
   "metadata": {},
   "source": [
    "# Analyzing Salinity Patterns in South Florida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a115a064-cd43-4a83-8eb3-bd47dbc0b4b2",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to analyze salinity patterns in South Florida using curated buoy data processed by the **EnviStor smart data pipeline** and made available through **PelicanFS**, a high-performance file system interface for the Open Science Data Federation (OSDF).\n",
    "\n",
    "The data used here comes from three monitoring stations managed by FIU, each located in a distinct part of South Floridaâ€™s coastal waters. These datasets were curated and made analysis-ready by the EnviStor pipeline.\n",
    "\n",
    "The central question is: \n",
    "> **What are the salinity patterns in South Florida?**\n",
    "\n",
    "We'll answer this by loading the data, cleaning and merging it, and visualizing salinity trends over time across multiple locations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ea503-20f8-4e92-80be-dc8d8a3137f2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1177068-dad4-4a1b-962c-ec1d31d287e2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a239e37-d91b-433f-8715-b684fc9f1fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pelicanfs.core import OSDFFileSystem\n",
    "from io import BytesIO\n",
    "\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f8dea-67e3-4782-b667-e02cc190c969",
   "metadata": {},
   "source": [
    "## Load the Curated Salinity Datasets\n",
    "\n",
    "Weâ€™ll load data from three buoy stations directly from PelicanFS using the `OSDFFileSystem()` interface. These Excel files have already been curated and prepared by the EnviStor smart pipeline, and are stored in the `/envistor` namespace in OSDF.\n",
    "\n",
    "Each file corresponds to a different buoy location:\n",
    "- **Biscayne Bay**\n",
    "- **Haulover Inlet**\n",
    "- **Little River**\n",
    "\n",
    "We use `BytesIO` to read the content as a stream before passing it to `pandas.read_excel()`. Each resulting DataFrame includes a `\"Station\"` column to identify its source location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54f1647-3ba6-48bf-9c62-9edc966fe7d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pelfs = OSDFFileSystem()\n",
    "file_buoy1 = pelfs.cat('/envistor/CREST_Buoy_2_NW_Biscayne_Bay_-_S_of_Biscayne_Canal_082720-112221.xlsx')\n",
    "file_buoy2 = pelfs.cat('/envistor/CREST_Buoy_3_Haulover_Inlet_100518_-_073020_updated.xlsx')\n",
    "file_buoy3 = pelfs.cat('/envistor/CREST_Buoy_3-2_Little_River_042121-050624.xlsx')\n",
    "\n",
    "excel_file1 = BytesIO(file_buoy1)\n",
    "df_file_buoy1 = pd.read_excel(excel_file1)\n",
    "df_file_buoy1['Station'] = 'Buoy - Biscayne Bay'\n",
    "\n",
    "excel_file2 = BytesIO(file_buoy2)\n",
    "df_file_buoy2 = pd.read_excel(excel_file2)\n",
    "df_file_buoy2['Station'] = 'Buoy - Haulover Inlet'\n",
    "\n",
    "excel_file3 = BytesIO(file_buoy3)\n",
    "df_file_buoy3 = pd.read_excel(excel_file3)\n",
    "df_file_buoy3['Station'] = 'Little River'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a03b5a-bdc5-4aab-a7f6-92ccc0c6f58a",
   "metadata": {},
   "source": [
    "## Clean and Combine the Data\n",
    "\n",
    "Now that weâ€™ve loaded the individual DataFrames, weâ€™ll prepare them for analysis.\n",
    "\n",
    "Hereâ€™s what we do:\n",
    "1. **Create a timestamp**: Combine the `\"Date\"` and `\"Time\"` columns into a single `datetime` column.\n",
    "2. **Standardize salinity**: Rename the `\"Sal_psu\"` column to `\"Salinity\"` and convert its values to numeric (in case of string or error-prone entries).\n",
    "3. **Merge datasets**: Concatenate the three cleaned DataFrames into one (`df_all`) so we can analyze salinity trends across all buoy stations together. We also drop any rows with missing salinity values and set the `datetime` column as the index to enable time-based operations later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4afbed-8594-4a41-9314-e5a61dc92960",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_file_buoy1, df_file_buoy2, df_file_buoy3]:\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"Date\"].astype(str) + \" \" + df[\"Time\"].astype(str))\n",
    "\n",
    "    df.rename(columns={\"Sal_psu\": \"Salinity\"}, inplace=True)\n",
    "    df[\"Salinity\"] = pd.to_numeric(df[\"Salinity\"], errors=\"coerce\")\n",
    "\n",
    "df_all = pd.concat([df_file_buoy1, df_file_buoy2, df_file_buoy3], ignore_index=True)\n",
    "df_all.dropna(subset=[\"Salinity\"], inplace=True)\n",
    "df_all.set_index(\"datetime\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646a82f-e03a-408c-a841-89f03583a189",
   "metadata": {},
   "source": [
    "## Resample and Aggregate\n",
    "\n",
    "Salinity readings are taken multiple times per day. To reveal broader trends, we resample the data to **daily averages**.\n",
    "\n",
    "This step:\n",
    "- Reduces noise\n",
    "- Makes it easier to compare across time\n",
    "- Prepares the data for visualization\n",
    "\n",
    "We group by station and resample by day (`'1D'`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c863d-7a30-42c2-9712-c1269c6aa9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily = (\n",
    "    df_all\n",
    "    .groupby(\"Station\")\n",
    "    .resample(\"1D\")\n",
    "    [\"Salinity\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e6469a-bf12-4613-86bc-0bbb8799904d",
   "metadata": {},
   "source": [
    "## Visualize the Salinity Patterns\n",
    "\n",
    "Now we can plot daily salinity patterns to compare how they evolve over time across the three locations.\n",
    "\n",
    "Weâ€™ll use Seaborn for a clean, readable line chart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf4f4e-e1f8-4e33-8ebb-c3414743561e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(data=df_daily, x=\"datetime\", y=\"Salinity\", hue=\"Station\")\n",
    "plt.title(\"Daily Average Salinity in South Florida (by Buoy Station)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Salinity (psu)\")\n",
    "plt.legend(title=\"Station\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af4003b-79b8-4b06-b3c5-a54c05634ed1",
   "metadata": {},
   "source": [
    "```{note}\n",
    "ðŸ’¡ **Why this is possible**\n",
    "\n",
    "This visualization is only possible thanks to the **EnviStor Smart Pipeline**, which curated, cleaned, and enriched the buoy data, and published it to OSDF.\n",
    "\n",
    "Additionally, the **Pelican platform** allowed us to access the data on-demand using `PelicanFS` â€” no local downloads or manual data wrangling required. This is a great example of how data infrastructure can directly support scientific insight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85146342-a8fd-4aec-864b-a2495d0e4d19",
   "metadata": {},
   "source": [
    "## Interpret the Results\n",
    "\n",
    "With the visualization in hand, we can start identifying key salinity patterns in South Florida:\n",
    "\n",
    "- **Biscayne Bay** (blue) shows a clear **seasonal fluctuation**, with higher salinity in dry months and noticeable drops likely linked to storm events or freshwater inflow.\n",
    "- **Haulover Inlet** (orange) tends to have **consistently higher salinity** levels, suggesting stronger tidal mixing and less influence from freshwater discharge.\n",
    "- **Little River** (green) displays **the most variability** â€” sharp dips and spikes in salinity hint at frequent freshwater input, possibly from canals, rain events, or upstream runoff.\n",
    "\n",
    "These differences illustrate how **geographic location and local hydrology** impact salinity levels. By comparing trends across stations, we gain insight into how dynamic and localized coastal salinity can be.\n",
    "\n",
    "```{note}\n",
    "This type of analysis can support environmental monitoring, resource management, and research on saltwater intrusion and estuarine health.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92163bc-20cf-4fdf-b761-056bf6927b13",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This notebook provided a foundation for analyzing coastal salinity patterns using curated data from the EnviStor pipeline. If you're interested in extending this work, here are a few ideas:\n",
    "\n",
    "- **Incorporate other environmental variables**: Analyze how temperature, turbidity, or dissolved oxygen vary alongside salinity to build a more holistic view of water quality.\n",
    "- **Add spatial analysis**: Use GIS tools or libraries (e.g., Cartopy or Folium) to visualize station locations and explore spatial gradients in salinity.\n",
    "- **Compare across years**: Investigate long-term salinity trends and identify anomalies across different seasons or years.\n",
    "- **Include other datasets from EnviStor**: Expand the workflow by pulling additional datasets from OSDF, such as ocean currents, rainfall, or metadata-enriched observations.\n",
    "- **Develop alert thresholds**: Identify salinity levels that may signal ecological stress or risk, potentially integrating this with decision-making tools.\n",
    "\n",
    "```{hint}\n",
    "Curious about how the data got so clean? Check out the [EnviStor smart pipelineâ€™s](https://envistorhome.fiu.edu/envistor/) role in preparing these files â€” from metadata tagging to anomaly filtering â€” to better understand the power of backend automation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
