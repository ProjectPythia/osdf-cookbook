{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73280389-1c76-4f23-b401-254255626ca3",
   "metadata": {},
   "source": [
    "# PyCoGSS: Spectral Change Analysis of Arctic Water Tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba76106-01f9-4716-8fe1-a5cf1f22db77",
   "metadata": {},
   "source": [
    "```{image} ../thumbnails/pycogss-logo.png\n",
    ":alt: PyCoGSS logo\n",
    ":width: 300px\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe4cc4-2bc8-4ff0-9d6d-a6120ccf9af0",
   "metadata": {},
   "source": [
    "## Section 1: Data access through PelicanFS and OSDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c78037-67ee-48e0-ae4b-8e8b8841f7a4",
   "metadata": {},
   "source": [
    "### Overview\n",
    "This notebook provides an example of a scientific use-case of the PelicanFS framework by accessing data to be analyzed through federated OSDF caches. We use this data to analyze spectral changes of water tracks in a small area of interest in the Arctic. In this analysis we calculate NDVI index which we use to infer the greening and browning trends of the water tracks in the summer months as permafrost thaws. \n",
    "\n",
    "We build a dataset from a catalog of open-access Sentinel-2 data from the Amazon Web Service (AWS). Normally, we would access this open data from AWS each time we need to run or re-run our analysis, and each time we would be making requests to the AWS servers. Given the vast spatial, temporal and spectral scale of the satellite data  we use for our analysis, retrieving data can be time and resource costly depending on the infrastructure or network context from which we are performing our computation.  With PelicanFS, we can reduce data acquisition times by performing our catalog search on OSDF federated caches. Additionally, when possible PelicanFS caches data that was previously unavailable in the OSDF cache so that next time we would be able to retrieve it from the cache. Ultimately, we expect that accessing data through PelicanFS will improve the overall time complexity of our analysis workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c0dd3e-482f-4d57-9daa-8f66cc90fe98",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "To better understand this notebook, please familiarize yourself with the following concepts:\n",
    "\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Intro to OSDF](http://projectpythia.org/osdf-cookbook/notebooks/osdf-intro/) | Recommended | Overview of OSDF |\n",
    "| [Overview of FSSpec](https://filesystem-spec.readthedocs.io/en/latest/) | Necessary | To better understand the FSSpec library |\n",
    "| [Overview of Python xarrays](https://foundations.projectpythia.org/core/xarray/) | Necessary | An introduction to data manipulation using Xarray DataArrays and Datasets |\n",
    "| [Working with STAC catalogs](https://stacspec.org/en) | Necessary | An overview of SpatioTemporal Asset Catalog (STAC) catalogs for spatial data |\n",
    "| [Spatial STAC catalogs as xarray data structures](https://stackstac.readthedocs.io/en/latest/); [Working with STAC catalogs in Python](https://pystac-client.readthedocs.io/en/stable/) | Necessary | Efficient computation of spatial raster data as STAC catalogs and xarray data structure in Python |\n",
    "\n",
    "- **Time to learn**: 30-45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5936d47-a395-45c9-b8cb-69f0c04fd4dc",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99a7733-308e-4fab-b77d-e670bc14a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pelicanfs.core import PelicanFileSystem, PelicanMap,OSDFFileSystem \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import urllib\n",
    "\n",
    "import geopandas as gpd\n",
    "import pystac_client \n",
    "import stackstac\n",
    "import rasterio\n",
    "import shapely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23666e8",
   "metadata": {},
   "source": [
    "### Build a STAC catalog of Sentinel-2 data from AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553388b9",
   "metadata": {},
   "source": [
    "Here, we begin by building a catalog of Sentinel-2 data that we will access from AWS. The data will be in form of  When we query our catalog, we obtain SPEC metadata objects which we will process further through the PySTAC library. As we see below, our catalog search returned 104 items, where item is a Sentinel-2 scene and take [cite, provide more info for S2 scenes, takes?] matching our AOI for June-October 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95fcb18-e781-40b6-b4a7-c53c200d3a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build a geometry of AOI from these coordinate bounds\n",
    "aoiBounds = (134.66615966473387,  66.82737559988661, 134.72162967387277, 66.85380494758718)\n",
    "aoiGeom = shapely.geometry.box(*aoiBounds)\n",
    "lon, lat = 134.70071475239416, 66.84143426792251\n",
    "\n",
    "startDate      = '2020-06'\n",
    "endDate       = '2020-08'\n",
    "# cloudCovMaxPct = 5\n",
    "\n",
    "catalogURL = 'https://earth-search.aws.element84.com/v1'\n",
    "search = pystac_client.Client.open(catalogURL).search(collections=['sentinel-2-l2a'],\n",
    "                                                      # bbox=aoiGeom.bounds,\n",
    "                                                      datetime=f'{startDate}/{endDate}',\n",
    "                                                      intersects=dict(type=\"Point\", coordinates=(lon, lat)),\n",
    "                                                      # query={'eo:cloud_cover': {'lt': cloudCovMaxPct}}\n",
    "                                                        )\n",
    "\n",
    "\n",
    "# Get all matching items\n",
    "items = list(search.items())\n",
    "print(f'Found {len(items)} matching items.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f24bc55-6776-47da-bb5e-6740cc7b0edf",
   "metadata": {},
   "source": [
    "### Pointing STAC catalog to OSDF caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a557ead-911b-4164-92cf-170c2e1859fc",
   "metadata": {},
   "source": [
    "Now is the time to utilize the capabilities of Pelican File System (PelicanFS) and OSDF caches. Remember that our STAC catalog contains metadata of our data. As mentioned above, each of the 104 items in our catalog is a Sentinel-2 scene for a particular timestamp matching our AOI. We can look up one of the items from the catalog to see its metadata. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35badc-698f-4927-942a-34c7a697c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(items[0].properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94957bf-fe55-4c89-837c-4cf65a5cb6df",
   "metadata": {},
   "source": [
    "We can peek further to see where the \"assets\" of this particular item is store by looking for its URL. As expected the URL points to a AWS bucket somewhere. This checks out, because out STAC catalog is build from Sentinel-2 data store in AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcbabdc-d580-4a67-96d9-d3c9989358ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "items[0].assets['nir'].href"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ddd076-3105-46cf-9900-2d8d6e9dbe66",
   "metadata": {},
   "source": [
    "Remember we are trying to access our data through PelicanFS, which will hopefully point us to an OSDF cache instead of AWS. If PelicanFS were able to do that, then the URL above would point to an OSDF network of cache resource in some non-AWS server. Now we will prepare to access our data through PelicanFS by first telling PelicanFS where to find Sentinel-2 data in AWS, i.e. pointing it to Sentinel-2 data namespace in AWS. This creates a kind of file path to where OSDF caches data. We will have to do this for each asset URL as seen in the example above. Here is an example of how a new constructed OSDF path from an Asset URL looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7f3f5-4d24-4ed7-b984-daf8ed457949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOSDFPath(url,AWSRegion='us-west-2'):\n",
    "    \"\"\"\n",
    "    Constructs an OSDF path from an asset's original URL.\n",
    "\n",
    "    Parameters:\n",
    "    - url: URL to convert.\n",
    "\n",
    "    Returns:\n",
    "    - OSDF path.\n",
    "    \"\"\"\n",
    "    \n",
    "    return f'/aws-opendata/{AWSRegion}/sentinel-cogs{urllib.parse.urlparse(url).path}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fccdea-c808-451c-aa45-b09b3681d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "getOSDFPath(items[0].assets['nir'].href)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72217f6-a19b-4d08-847b-d6612cad1710",
   "metadata": {},
   "source": [
    "Now let's try accessing this asset through PelicanFS, and opening the retrieved GeoTIFF file using Rasterio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524048fc-1d05-4aff-810b-3c0a0b07d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pelFS = PelicanFileSystem('pelican://osg-htc.org')\n",
    "\n",
    "AWSRegionLst = ['us-east-1','us-east-2','us-central-1','us-central-2','us-west-1','us-west-2']\n",
    "bandUrl = getOSDFPath(items[1].assets['nir'].href, 'us-west-2')\n",
    "bandDS = rasterio.open(bandUrl, opener=pelFS)\n",
    "bandDS.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70abeee8-4a55-4987-a9d5-82e036969ac1",
   "metadata": {},
   "source": [
    "So we are able to read the raster through the PelicanFS using Rasterio. Let's probe further to see which OSDF caches PelicanFS is routing us to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b0bff-8ea3-4e6f-acc1-9e787a5cb600",
   "metadata": {},
   "outputs": [],
   "source": [
    "pelFS._access_stats.get_responses(bandUrl)[0][-1].access_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234d8cb8-8629-4198-a849-9ee67f14f04b",
   "metadata": {},
   "source": [
    "Now we'll pull a small trick and try to find out which OSDF resource PelicanFS found the cache, i.e. our raster file. This is helpful for us, because now we can easily point PelicanFS to this OSDF resource for all items in our catalog. Why are we doing this? We see from the two cells above that reading a single item's asset takes ~8 second. Since we have 104 items each with 3 assets (the Red, NIR and SCL bands), that means at least ~24 seconds for each item, and $104 \\times 24 = 2496$ for all items in our catalog. About 42 minutes in total, with potential additional network overhead time costs!\n",
    "\n",
    "Now we will point all the URLs in our cache to ``\n",
    "https://osdf1.newy32aoa.nrp.internet2.edu:8443``. We will also make a HTTP request to the new OSDF Cache URL and only point update our catalog with this new URL if the HTTP request is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd225eb0-fa75-44de-87f6-e4c58c91868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pelFS = PelicanFileSystem('pelican://osg-htc.org')\n",
    "\n",
    "itemsOSDFCache = items\n",
    "for idx, item in enumerate(items): # start=1):\n",
    "    # print(f'Processing dataset #{idx}')\n",
    "\n",
    "    cacheCount = 0\n",
    "    for band in ['red','nir','scl']:\n",
    "        bandUrl = getOSDFPath(items[idx].assets[band].href)\n",
    "        \n",
    "        # takes too long to access cache through PelicanFS unfortunately\n",
    "        # we can do a sneak peak of one of the items to see the OSDF location \n",
    "        # from which PelicanFS finds its cache then point the rest of our catalog to it!\n",
    "        '''\n",
    "        bandDS = rasterio.open(bandUrl, opener=pelFS)\n",
    "        if pelFS._access_stats.get_responses(bandUrl)[1]:\n",
    "            osdfCachePath = pelFS._access_stats.get_responses(bandUrl)[0][-1].access_path\n",
    "            # print(f'cache for dataset #{idx} found in {osdfCachePath} \\n')\n",
    "            items[idx].assets[band].href = pelFS._access_stats.get_responses(bandUrl)[0][-1].access_path\n",
    "            cacheCount += 1\n",
    "        # close dataset!\n",
    "        bandDS.close()\n",
    "        '''\n",
    "        cacheOSDF = 'https://osdf1.newy32aoa.nrp.internet2.edu:8443'+bandUrl\n",
    "        with urllib.request.urlopen(cacheOSDF) as res:\n",
    "            if res.status == 200:\n",
    "                itemsOSDFCache[idx].assets[band].href = cacheOSDF\n",
    "            \n",
    "    \n",
    "    # print(f'{cacheCount} caches found for dataset #{idx} \\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad985d0-ed55-41c8-8b4d-6b9045fc0045",
   "metadata": {},
   "source": [
    "Inspecting one of the items shows the that its assets point to the new OSDF URL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132a158-6959-4596-a169-7e3a7c7fb6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "items[23].assets['nir'].href"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07258f26-a0fc-4206-976e-7365aff560e3",
   "metadata": {},
   "source": [
    "### Stacking STAC into lazy xarray objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592113c9-5fe3-4361-890f-c472d3fa2d00",
   "metadata": {},
   "source": [
    "Now is the time to turn our \"OSDF catalog\" into data that we can analyze. We'll do so by using the ``stackstac`` library to convert the STAC catalog to xarray data structures, and makes it possible to perform distributed computing with the help of dask. In any case, the resulting xarray data structures will be lazy, meaning that data is not loaded upfront, but only when needed. Because we have a large dataset [cite: compute size], lazy loading will be really helpful to optimize our resource usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7435c-ccf7-42b9-b5fd-2cac26fbefa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "s2Stack = stackstac.stack(itemsOSDFCache, assets=['red', 'nir', 'scl'],\n",
    "                         bounds = aoiGeom.bounds,\n",
    "                          gdal_env=stackstac.DEFAULT_GDAL_ENV.updated(\n",
    "                               {'GDAL_HTTP_MAX_RETRY': 3,\n",
    "                                'GDAL_HTTP_RETRY_DELAY': 5,\n",
    "                               }),\n",
    "                          epsg=4326,\n",
    "                              #    chunksize=(1, 1, 50, 50) # Original - many small chunks bad for plotting\n",
    "                                 chunksize=(1, -1, 100, 100)\n",
    "                                ).rename(\n",
    "       {'x': 'lon', 'y': 'lat'}).to_dataset(dim='band')\n",
    "s2Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecef2f5-b132-4b0a-bb13-56ddcaccf5b6",
   "metadata": {},
   "source": [
    "## Section 2: NDVI analysis of water tracks and inter-tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3776ad7c-4952-4ed0-b112-d22bb2acbfee",
   "metadata": {},
   "source": [
    "Now we can continue our analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154609e6",
   "metadata": {},
   "source": [
    "### Calculate NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44694bd0-e5f4-40dd-ba81-6475325197e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroMask = s2Stack['nir'] + s2Stack['red']\n",
    "\n",
    "s2Stack['ndvi'] = (s2Stack.where(zeroMask != 0, np.nan)['nir'] - s2Stack.where(zeroMask != 0, np.nan)['red'])/\\\n",
    "                        (s2Stack.where(zeroMask != 0, np.nan)['nir'] + s2Stack.where(zeroMask != 0, np.nan)['red'])\n",
    "\n",
    "# # Only keep ndvi and classification, but know you can save things like 'visible' or other fun rasters!\n",
    "s2Stack = s2Stack[['ndvi', 'scl']]\n",
    "s2Stack = s2Stack.drop_vars([c for c in s2Stack.coords if not (c in ['time', 'lat', 'lon'])])\n",
    "s2Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8d0e8",
   "metadata": {},
   "source": [
    "### Test on the centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796817da-dc4f-4800-98df-898280db8c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "s2Point = s2Stack.interp(lat=lat, lon=lon,method='nearest')\n",
    "\n",
    "s2Df = s2Point.to_dataframe()\n",
    "s2DfFilt = s2Df[(s2Df['scl'] == 4) | (s2Df['scl'] == 5)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "s2DfFilt['ndvi'].plot(label='unfiltered',\n",
    "                      marker='o', \n",
    "                      # linestyle='--',\n",
    "                      markersize=2, ax=ax)\n",
    "ax.set_ylim(-1.0,5.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472362c3",
   "metadata": {},
   "source": [
    "Fascinating. What happened in late July, early August 2020? Let's take a look at an asset from the collection during that time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e952280",
   "metadata": {},
   "outputs": [],
   "source": [
    "pics = {}\n",
    "from datetime import datetime\n",
    "for item in items:\n",
    "    item_dict = {}\n",
    "    item_dict['date'] = item.properties['datetime']\n",
    "    item_dict['pic'] = item.assets['visual'].href\n",
    "    item_dict['thumb'] = item.assets['thumbnail'].href\n",
    "    pics[item.id] = item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ef5680",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from rasterio.plot import show\n",
    "with rasterio.open(pics['S2A_53WMQ_20200804_1_L2A']['pic']) as dataset:\n",
    "    show(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f2477",
   "metadata": {},
   "source": [
    "That'll do it! (this is a fire burning the larch forests of the eastern Siberia taiga)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b1e2e",
   "metadata": {},
   "source": [
    "### Plot the whole stack?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b442c6d-6294-4028-85dc-60230b4ceed3",
   "metadata": {},
   "source": [
    "#### Mosaic by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e3e369-3a5a-45e5-9c06-aad507f5c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2StackMosaic = s2Stack.groupby('time.date').median(dim='time')\n",
    "# s2StackMosaic = sentinel_stack_mosaicked.rename({'date': 'time'})\n",
    "# sentinel_stack_mosaicked['time'] = sentinel_stack_mosaicked['time'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676e8e84-9d73-47ab-99df-b7681b65b8ce",
   "metadata": {},
   "source": [
    "Day of year makes it easier for a linear trend analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04b5d3-3814-4f59-84f7-36b2eee4fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_year = xr.DataArray(\n",
    "    s2StackMosaic['date'].astype('datetime64[ns]').dt.dayofyear,\n",
    "    coords={'date': s2StackMosaic['date']},\n",
    "    dims='date',\n",
    "    name='day_of_year'\n",
    ")\n",
    "\n",
    "s2StackMosaic = s2StackMosaic.assign_coords({'day_of_year': day_of_year})\n",
    "s2StackMosaic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9ef86d-d54a-4cb7-9dec-e1e7df3caadd",
   "metadata": {},
   "source": [
    "Now we can look at cool pictures by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b4ad0d-190a-4431-9766-c4dffb591849",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2StackMosaic['ndvi'][1].plot.imshow(vmin=-1.0, vmax=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5812ced8-c83a-444e-90ed-282590688d60",
   "metadata": {},
   "source": [
    "#### Mask out undesireable pixels in the stack according to the scene classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2cd779-aaba-4ba0-863d-45e0cced591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2StackMosaic['scl'][0].plot.imshow(cmap='Set1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff90f31-89aa-4a56-b1e9-5f5be2f23220",
   "metadata": {},
   "source": [
    "Google 'sentinel 2 scene classification layer'. You'll see that 4 and 5 are coded for vegetated and not vegetated. Conservatively, everything else is trash for interpreting NDVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0cd251-a461-42eb-86d8-b0a58038ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndviMasked = s2StackMosaic['ndvi'].where(s2StackMosaic['scl'].isin([4, 5]))\n",
    "\n",
    "ndviMasked = ndviMasked.where(ndviMasked >= -1.0)\n",
    "\n",
    "ndviMasked = ndviMasked.where(ndviMasked <= 1.0)\n",
    "\n",
    "ndviMasked[0].plot.imshow(\n",
    "    # vmin=-1.0, vmax=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fac478-130e-4bba-9544-c63c6818137e",
   "metadata": {},
   "source": [
    "Ah yes, that's better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bdc801-f62a-4659-80a1-9c7887038b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "meanNDVIDoY = ndviMasked.mean(dim=['lat', 'lon'], skipna=True).to_dataframe(name='mean_ndvi').reset_index()\n",
    "meanNDVIDoY.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db2ec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanNDVIDoY.plot.scatter(x='day_of_year', y='mean_ndvi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f837176f-e69a-4453-8fac-b0437b006c31",
   "metadata": {},
   "source": [
    "Ok, so now we're able to detect this somehow automatically. It would be helpful to cross-referencing this against some climate (in this case it's burned, not climate, which are related but also maybe checking against some fire database or product too)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171dfccd-a0da-4248-8d7d-68b77380188b",
   "metadata": {},
   "source": [
    "### Do a trendline fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a345de",
   "metadata": {},
   "source": [
    "What if we could visualize the pixelwise trend in greening and browning over a season? Let's take a linear trend for each pixel's NDVI across the year (hence why day of year is a useful variable here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dbd208-ad35-4578-8eee-742959ca7068",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit = ndviMasked.polyfit(dim='day_of_year', deg=1)\n",
    "slopes = fit['polyfit_coefficients'].sel(degree=1)\n",
    "\n",
    "slopes.plot.imshow(\n",
    "                cmap='coolwarm_r',\n",
    "                vmin=-.01,\n",
    "                vmax=.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc3afcb",
   "metadata": {},
   "source": [
    "This is now a great raster to play around with in GIS if you choose. A lot of those red streaks are the flowpaths we're intersted in (water tracks) so it's interesting that they got browner over the growing season compared to the intertrack areas which are generally positive (got greener over the growing season). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3de0c4-3f1b-4ed7-a3e1-0ab0c5bb5368",
   "metadata": {},
   "source": [
    "### What's Next?\n",
    "\n",
    "In the near future, this notebook will:\n",
    "\n",
    "- Address PelicanFS bottlenecks during reading of cache metadata.\n",
    "- Use a larger AOI and longer time period to see this anylysis over multiple years.\n",
    "- With a larger AOI and longer time series, we'll have more data, wo we'll experiment with distributed computing with Dask."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
