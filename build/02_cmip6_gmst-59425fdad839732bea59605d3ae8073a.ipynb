{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this notebook we will compute the Global Mean Surface Temperature Anomalies (GMSTA) from CMIP6 data and compare it with observations. This notebook is heavily inspired by the [GMST example](https://projectpythia.org/cmip6-cookbook/notebooks/example-workflows/gmst/) in the CMIP6 cookbook and we thank the authors for their workflow.\n",
    "\n",
    "1. We will get the CMIP6 temperature data from the AWS open data program via the us-west-2 origin\n",
    "2. In order to do this, we will use an intake-ESM catalog (hosted on NCAR's RDA) that uses pelicanFS backed links instead of https or s3 links\n",
    "3. We will grab observational data hosted on NCAR's RDA, which is accessible via the NCAR origin\n",
    "4. Please refer to the first chapter of this cookbook to learn more about OSDF, pelican or pelicanFS\n",
    "5. This notebook demonstrates that you can seamlessly stream data from multiple OSDF origins in your workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Intro to Intake-ESM](https://foundations.projectpythia.org/core/cartopy/cartopy) | Necessary | Used for searching CMIP6 data |\n",
    "| [Understanding of Zarr](https://zarr.dev/) | Helpful | Familiarity with metadata structure |\n",
    "| [Seaborn](https://seaborn.pydata.org/) | Helpful | Used for plotting|\n",
    "| [PelicanFS](https://projectpythia.org/osdf-cookbook/notebooks/pelicanfs/) | Necessary | The python package used to stream data in this notebook |\n",
    "| OSDF| Helpful | OSDF is used to stream data in this notebook |\n",
    "\n",
    "- **Time to learn**: 20 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from dask.diagnostics import progress\n",
    "from tqdm.autonotebook import tqdm\n",
    "import intake\n",
    "import fsspec\n",
    "import seaborn as sns\n",
    "import aiohttp\n",
    "import dask\n",
    "from dask.distributed import LocalCluster\n",
    "import pelicanfs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use an intake-ESM catalog hosted on NCAR's Research Data Archive. This is nothing but the AWS cmip6 catalog modified to use OSDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load catalog URL\n",
    "rda_url     =  'https://data.rda.ucar.edu/'\n",
    "cat_url     = rda_url +  'd850001/catalogs/osdf/cmip6-aws/cmip6-osdf-zarr.json'\n",
    "print(cat_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up local dask cluster "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do any computation let us first set up a local cluster using dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster()          \n",
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the cluster\n",
    "n_workers = 5\n",
    "cluster.scale(n_workers)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "### Load CMIP6 data from AWS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = intake.open_esm_datastore(cat_url)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is currently a significant amount of data for these runs\n",
    "expts = ['historical', 'ssp245', 'ssp370']\n",
    "\n",
    "query = dict(\n",
    "    experiment_id=expts,\n",
    "    table_id='Amon',\n",
    "    variable_id=['tas'],\n",
    "    member_id = 'r1i1p1f1',\n",
    "    #activity_id = 'CMIP',\n",
    ")\n",
    "\n",
    "col_subset = col.search(require_all_on=[\"source_id\"], **query)\n",
    "col_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let us inspect the zarr store paths to see if we are using the pelican protocol.\n",
    "- We see that zstore column has paths that start with 'osdf:///' instead of 'https://' which tells us that we are not using a simple 'https' GET request to fetch the data.\n",
    "- In order to know more about the pelican protocol, please refer to the first chapter of this cookbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_subset.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab some Observational time series data for comparison with ensemble spread\n",
    "- The observational data we will use is the HadCRUT5 dataset from the UK Met Office\n",
    "- The data has been downloaded to NCAR's Research Data Archive (RDA) from https://www.metoffice.gov.uk/hadobs/hadcrut5/\n",
    "- We will use an OSDF to access this copy from the RDA. Again the links will start with 'osdf:///'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "obs_url    = 'osdf:///ncar/rda/d850001/HadCRUT.5.0.2.0.analysis.summary_series.global.monthly.nc'\n",
    "#\n",
    "obs_ds = xr.open_dataset(obs_url, engine='h5netcdf').tas_mean\n",
    "obs_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some helpful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_all_bounds(ds):\n",
    "    drop_vars = [vname for vname in ds.coords\n",
    "                 if (('_bounds') in vname ) or ('_bnds') in vname]\n",
    "    return ds.drop_vars(drop_vars)\n",
    "\n",
    "def open_dset(df):\n",
    "    assert len(df) == 1\n",
    "    mapper = fsspec.get_mapper(df.zstore.values[0])\n",
    "    #path = df.zstore.values[0][7:]+\".zmetadata\"\n",
    "    ds = xr.open_zarr(mapper, consolidated=True)\n",
    "    return drop_all_bounds(ds)\n",
    "\n",
    "def open_delayed(df):\n",
    "    return dask.delayed(open_dset)(df)\n",
    "\n",
    "from collections import defaultdict\n",
    "dsets = defaultdict(dict)\n",
    "\n",
    "for group, df in col_subset.df.groupby(by=['source_id', 'experiment_id']):\n",
    "    dsets[group[0]][group[1]] = open_delayed(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets_ = dask.compute(dict(dsets))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate global means\n",
    "def get_lat_name(ds):\n",
    "    for lat_name in ['lat', 'latitude']:\n",
    "        if lat_name in ds.coords:\n",
    "            return lat_name\n",
    "    raise RuntimeError(\"Couldn't find a latitude coordinate\")\n",
    "\n",
    "def global_mean(ds):\n",
    "    lat = ds[get_lat_name(ds)]\n",
    "    weight = np.cos(np.deg2rad(lat))\n",
    "    weight /= weight.mean()\n",
    "    other_dims = set(ds.dims) - {'time'}\n",
    "    return (ds * weight).mean(other_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate global means\n",
    "def get_lat_name(ds):\n",
    "    for lat_name in ['lat', 'latitude']:\n",
    "        if lat_name in ds.coords:\n",
    "            return lat_name\n",
    "    raise RuntimeError(\"Couldn't find a latitude coordinate\")\n",
    "\n",
    "def global_mean(ds):\n",
    "    lat = ds[get_lat_name(ds)]\n",
    "    weight = np.cos(np.deg2rad(lat))\n",
    "    weight /= weight.mean()\n",
    "    other_dims = set(ds.dims) - {'time'}\n",
    "    return (ds * weight).mean(other_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMST computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_da = xr.DataArray(expts, dims='experiment_id', name='experiment_id',\n",
    "                       coords={'experiment_id': expts})\n",
    "\n",
    "dsets_aligned = {}\n",
    "\n",
    "for k, v in tqdm(dsets_.items()):\n",
    "    expt_dsets = v.values()\n",
    "    if any([d is None for d in expt_dsets]):\n",
    "        print(f\"Missing experiment for {k}\")\n",
    "        continue\n",
    "\n",
    "    for ds in expt_dsets:\n",
    "        ds.coords['year'] = ds.time.dt.year\n",
    "\n",
    "    # workaround for\n",
    "    # https://github.com/pydata/xarray/issues/2237#issuecomment-620961663\n",
    "    dsets_ann_mean = [v[expt].pipe(global_mean).swap_dims({'time': 'year'})\n",
    "                             .drop_vars('time').coarsen(year=12).mean()\n",
    "                      for expt in expts]\n",
    "\n",
    "    # align everything with the 4xCO2 experiment\n",
    "    dsets_aligned[k] = xr.concat(dsets_ann_mean, join='outer',dim=expt_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with progress.ProgressBar():\n",
    "    dsets_aligned_ = dask.compute(dsets_aligned)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ids = list(dsets_aligned_.keys())\n",
    "source_da = xr.DataArray(source_ids, dims='source_id', name='source_id',\n",
    "                         coords={'source_id': source_ids})\n",
    "\n",
    "big_ds = xr.concat([ds.reset_coords(drop=True)\n",
    "                    for ds in dsets_aligned_.values()],\n",
    "                    dim=source_da)\n",
    "\n",
    "big_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{hint}\n",
    "Note that even though the variable is called tas, the DataArray big_ds actually has the global and annual mean of surface temperatures! If you are wondering why this is the case, take a look at all the functions that were applied to obtain dsets_ann_mean!\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute annual mean temperatures anomalies of observational data\n",
    "obs_gmsta = obs_ds.resample(time='YS').mean(dim='time')\n",
    "# obs_gmsta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute anomlaies and plot\n",
    "- We will compute the temperature anomalies w.r.t 1960-1990 baseline period\n",
    "- Convert xarray datasets to pandas dataframes\n",
    "- Use Seaborn to plot GMSTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = big_ds.to_dataframe().reset_index()\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the baseline period\n",
    "baseline_df = df_all[(df_all[\"year\"] >= 1960) & (df_all[\"year\"] <= 1990)]\n",
    "\n",
    "# Compute the baseline mean\n",
    "baseline_mean = baseline_df[\"tas\"].mean()\n",
    "\n",
    "# Compute anomalies\n",
    "df_all[\"tas_anomaly\"] = df_all[\"tas\"] - baseline_mean\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = obs_gmsta.to_dataframe(name='tas_anomaly').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'time' to 'year' (keeping only the year)\n",
    "obs_df['year'] = obs_df['time'].dt.year\n",
    "\n",
    "# Drop the original 'time' column since we extracted 'year'\n",
    "obs_df = obs_df[['year', 'tas_anomaly']]\n",
    "obs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there! Let us now use seaborn to plot all the anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(data=df_all, x=\"year\", y=\"tas_anomaly\",\n",
    "                hue='experiment_id', kind=\"line\", errorbar=\"sd\", aspect=2, palette=\"Set2\")  # Adjust the color palette)\n",
    "\n",
    "# Get the current axis from the FacetGrid\n",
    "ax = g.ax\n",
    "\n",
    "# Overlay the observational data in red\n",
    "sns.lineplot(data=obs_df, x=\"year\", y=\"tas_anomaly\",color=\"red\", \n",
    "             linestyle=\"dashed\", linewidth=2,label=\"Observations\", ax=ax)\n",
    "\n",
    "# Adjust the legend to include observations\n",
    "ax.legend(title=\"Experiment ID + Observations\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook, we used surface air temperature data from several CMIP6 models for the 'historical', 'SSP245' and 'SSP370' runs to compute Global Mean Surface Temperature Anomaly (GMSTA) relative to the 1960-1990 baseline period and compare it with anomalies computed from the HadCRUT monthly surface temperature dataset. We used a modified intake-ESM catalog and pelicanFS to 'stream/download' temperature data from two different OSDF origins. The CMIP6 model data was streamed from the AWS OpenData origin in the us-west-2 region and the observational data was streamed from NCAR's OSDF origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources and references\n",
    "1. [Original notebook](https://gallery.pangeo.io/repos/pangeo-gallery/cmip6/global_mean_surface_temp.html) in the Pangeo Gallery by Henri Drake and Ryan Abernathey\n",
    "2. [CMIP6 cookbook](https://projectpythia.org/cmip6-cookbook/) by Ryan Abernathey, Henri Drake, Robert Ford and Max Grover\n",
    "3. Coupled Model Intercomparison Project 6 was accessed from https://registry.opendata.aws/cmip6 using a modified intake-ESM catalog hosted on NCAR's RDA\n",
    "4. We thank the UK Met Office Hadley Center for providing the observational data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ]
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
