{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Integration of NCAR/NOAA Environmental Data with the NOAA Water Column Sonar Archive\n",
    "\n",
    "This notebook walks through an end-to-end workflow to relate shipboard sonar backscatter (Sv) to local environmental conditions. We (1) open EK60 data from a public NOAA S3 Zarr, (2) gather co-located environmental variables from OISST and IOOS ERDDAP, (3) compute hourly mean Sv, (4) assemble a depth×time error map for reference, and (5) synchronize timestamps to produce an interactive line-plus-heatmap visualization. All selections (time/depth/frequency) and conversions are kept explicit for reproducibility.\n",
    "\n",
    "1. **Imports**\n",
    "   Load core libraries for data access (xarray, s3fs), analysis (numpy, pandas), plotting (plotly), and I/O.\n",
    "\n",
    "2. **Initializing the datasets**\n",
    "   Access HB1906 EK60 Zarr data from public S3; subset by time/depth, select 38 kHz, and mask bins below bottom.\n",
    "\n",
    "3. **Access buoy data**\n",
    "   Define Georges Bank buoy coordinates, sample daily OISST SST at the nearest grid cell (±1 day), and download the model error map (`.npy`).\n",
    "\n",
    "4. **Calculate the temperature anomaly, sun elevation in degree and azimuth**\n",
    "\n",
    "5. **Downloading external error map for the specific location**\n",
    "   Downloading the error map comes from a fixed file\n",
    "\n",
    "6. **Helper Function: Mean Sv (dB)**\n",
    "   Convert Sv from dB→linear, compute mean, convert back to dB.\n",
    "\n",
    "7. **Group Cruise Data into Hourly Chunks**\n",
    "   Add an hourly label and split the EK60 dataset into per-hour `xarray.Dataset` chunks.\n",
    "\n",
    "8. **Compute Hourly Mean Sv & Attach to `env_df`**\n",
    "   Aggregate Sv per hour and append results as a new column in the environmental dataframe.\n",
    "\n",
    "9. **Build Depth×Time Error-Map DataFrame & Align Timestamps**\n",
    "   Construct a depth-by-time matrix from the error map, guard for size mismatches, and align `env_df` endpoints to the heatmap timestamps.\n",
    "\n",
    "10. **Data Visualization: Synchronized Lines + Heatmap**\n",
    "   Plot environmental time series above a depth×time heatmap with shared x-axis; save interactive HTML output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This section was inspired by [this template](https://github.com/alan-turing-institute/the-turing-way/blob/master/book/templates/chapter-template/chapter-landing-page.md) of the wonderful [The Turing Way](https://the-turing-way.netlify.app) Jupyter Book.\n",
    "\n",
    "This notebook opens public NOAA EK60 Zarr data from S3, subsets by time/depth, reads daily OISST SST near a buoy, fetches ERDDAP environmental variables, computes hourly mean **Sv** (dB), aligns with a depth×time error map, and renders synchronized line/heatmap plots.\n",
    "\n",
    "Label the importance of each concept explicitly as **helpful/necessary**.\n",
    "\n",
    "| Concepts | Importance    | Notes                                                                 |\n",
    "| --- |---------------|-----------------------------------------------------------------------|\n",
    "| Xarray + Zarr basics | **Necessary** | Opening Zarr stores, selecting by coords/dims, `.compute()` semantics |\n",
    "| s3fs & public S3 access | **Necessary** | Anonymous reads from AWS S3 (`anon=True`)                             |\n",
    "| Pandas time series | **Necessary** | `DatetimeIndex`, sorting, filtering, timezone-naive vs. aware         |\n",
    "| NumPy fundamentals | **Necessary** | Array slicing, stats, type conversion                                 |\n",
    "| Acoustic backscatter (Sv) & dB averaging | **Necessary** | Convert dB→linear, mean, then linear→dB, Understanding results        |\n",
    "| ERDDAP tabledap & info endpoints | **Helpful**   | Reading CSV responses; unit metadata lookup                           |\n",
    "| Plotly fundamentals | **Helpful**   | Subplots, heatmaps, interactive HTML export                           |\n",
    "| Understanding of NetCDF/CF | **Helpful**   | Variable metadata and geospatial conventions                          |\n",
    "| Dask awareness | **Helpful**   | Lazy arrays; when/why to call `.compute()`                            |\n",
    "| Geographic coordinates | **Helpful**   | 0–360 vs. −180–180 longitude handling                                 |\n",
    "| HTTP/IO with `requests` | **Helpful**   | Downloading `.npy` assets for local use                               |\n",
    "\n",
    "- **Time to learn**: ~75 minutes\n",
    "\n",
    "- **System requirements**:\n",
    "  - Python 3.9+ with Jupyter Notebook/Lab\n",
    "  - Required packages: `xarray`, `s3fs`, `numpy`, `pandas`, `plotly`, `requests`, `netCDF4` *(optional but helpful: `dask`)*\n",
    "\n",
    "**Note:** Run the cell below **only** in a local environment to install the required packages. If you’re using Binder, skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: /glade/u/apps/jupyterhub/jh-23.11/bin/mamba install -y -p /glade/u/apps/jupyterhub/jh-23.11 -c conda-forge jupyterlab mystmd jupyterlab-myst aiohttp bash_kernel intake intake-esm=2025.2.3 pelicanplatform panel bokeh xmltodict cartopy colorcet cf-units boto3 basemap pelicanfs=1.2.1 matplotlib metpy pandas xarray geopandas pystac-client rasterio dask dask-jobqueue openpyxl hvplot plotly zarr=2.18.1 s3fs seaborn tqdm h5netcdf igwn-auth-utils=1.4.0\n",
      "\n",
      "Looking for: ['jupyterlab', 'mystmd', 'jupyterlab-myst', 'aiohttp', 'bash_kernel', 'intake', 'intake-esm=2025.2.3', 'pelicanplatform', 'panel', 'bokeh', 'xmltodict', 'cartopy', 'colorcet', 'cf-units', 'boto3', 'basemap', 'pelicanfs=1.2.1', 'matplotlib', 'metpy', 'pandas', 'xarray', 'geopandas', 'pystac-client', 'rasterio', 'dask', 'dask-jobqueue', 'openpyxl', 'hvplot', 'plotly', 'zarr=2.18.1', 's3fs', 'seaborn', 'tqdm', 'h5netcdf', 'igwn-auth-utils=1.4.0']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error    libmamba Could not open lockfile '/glade/u/apps/jupyterhub/jh-23.11/pkgs/cache/cache.lock'\n",
      "warning  libmamba Could not parse mod/etag header\n",
      "error    libmamba Could not open lockfile '/glade/u/apps/jupyterhub/jh-23.11/pkgs/cache/cache.lock'\n",
      "warning  libmamba Could not parse mod/etag header\n",
      "error    libmamba Could not open lockfile '/glade/u/apps/jupyterhub/jh-23.11/pkgs/cache/cache.lock'\n",
      "warning  libmamba Could not parse mod/etag header\n",
      "warning  libmamba Could not parse mod/etag header\n",
      "error    libmamba Could not open lockfile '/glade/u/apps/jupyterhub/jh-23.11/pkgs/cache/cache.lock'\n",
      "warning  libmamba Could not parse mod/etag header\n",
      "warning  libmamba Could not parse mod/etag header\n",
      "error    libmamba Could not open lockfile '/glade/u/apps/jupyterhub/jh-23.11/pkgs/cache/cache.lock'\n",
      "warning  libmamba Could not parse mod/etag header\n",
      "warning  libmamba Could not parse mod/etag header\n",
      "error    libmamba Could not open lockfile '/glade/u/apps/jupyterhub/jh-23.11/pkgs/cache/cache.lock'\n",
      "warning  libmamba Could not parse mod/etag header\n",
      "warning  libmamba Could not parse mod/etag header\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pinned packages:\n",
      "  - python 3.10.*\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "ENV_PATH = \"../environment.yml\"\n",
    "\n",
    "with open(ENV_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    env = yaml.safe_load(f)\n",
    "\n",
    "channels = env.get(\"channels\", [])\n",
    "deps = env.get(\"dependencies\", [])\n",
    "\n",
    "conda_pkgs = []\n",
    "pip_pkgs = []\n",
    "\n",
    "for dep in deps:\n",
    "    if isinstance(dep, str):\n",
    "        # Skip python pin and the literal 'pip' meta-package entry\n",
    "        name = dep.split(\"=\")[0].strip().lower()\n",
    "        if name in {\"python\", \"pip\"}:\n",
    "            continue\n",
    "        conda_pkgs.append(dep)\n",
    "    elif isinstance(dep, dict) and \"pip\" in dep:\n",
    "        pip_pkgs.extend(dep[\"pip\"])\n",
    "\n",
    "# Prefer mamba if present; fallback to conda\n",
    "conda_exe = shutil.which(\"mamba\") or shutil.which(\"conda\")\n",
    "\n",
    "# Install Conda packages\n",
    "if conda_pkgs:\n",
    "    if not conda_exe:\n",
    "        raise RuntimeError(\"Conda/mamba not found in PATH. Run this inside a Conda environment.\")\n",
    "    # Install into the current environment prefix\n",
    "    env_prefix = os.environ.get(\"CONDA_PREFIX\", sys.prefix)\n",
    "    cmd = [conda_exe, \"install\", \"-y\", \"-p\", env_prefix]\n",
    "    for ch in channels:\n",
    "        cmd += [\"-c\", ch]\n",
    "    cmd += conda_pkgs\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "else:\n",
    "    print(\"No Conda packages to install.\")\n",
    "\n",
    "# Install pip packages into the current kernel's Python\n",
    "if pip_pkgs:\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", *pip_pkgs]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "else:\n",
    "    print(\"No pip packages to install.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Imports\n",
    "Core libraries used throughout the notebook.\n",
    "\n",
    "Key roles:\n",
    "\n",
    "    xarray/s3fs for reading NOAA Zarr data from S3\n",
    "    numpy/pandas for arrays & tables\n",
    "    plotly for interactive plotting\n",
    "    requests/io/os for file I/O and downloads\n",
    "    datetime for time calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import os\n",
    "import rioxarray\n",
    "from datetime import datetime, timedelta\n",
    "import pvlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Initializing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Builds the S3 path to the HB1906 EK60 Zarr dataset and opens it anonymously.\n",
    "Subsets by time window and depth range, selects the 38 kHz channel, and masks samples below the estimated bottom.\n",
    ".compute() materializes the selection; hm_timestamps will be reused for time alignment later. All datasets are accessed\n",
    "using the OSDF infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'noaa-wcsd-zarr-pds'\n",
    "ship_name = \"Henry_B._Bigelow\"\n",
    "cruise_name = \"HB1906\"\n",
    "sensor_name = \"EK60\"\n",
    "\n",
    "# Accessing the NOAA HB1906 dataset using OSDF (anonymous S3)\n",
    "s3_file_system = s3fs.S3FileSystem(anon=True)\n",
    "zarr_store = f'{cruise_name}.zarr'\n",
    "s3_zarr_store_path = f\"{bucket_name}/level_2/{ship_name}/{cruise_name}/{sensor_name}/{zarr_store}\"\n",
    "\n",
    "# Map S3 path to a zarr store and open (consolidated=None to let xarray infer metadata)\n",
    "store = s3fs.S3Map(root=s3_zarr_store_path, s3=s3_file_system, check=False)\n",
    "cruise = xr.open_zarr(store=store, consolidated=None)\n",
    "\n",
    "# Time/depth subset and single-frequency selection\n",
    "start_time = \"2019-10-16T15:00:00\"\n",
    "end_time = \"2019-10-16T23:11:09\"\n",
    "timeslice = slice(start_time, end_time)\n",
    "depths = slice(10, 250)\n",
    "cruise = cruise.sel(time=timeslice, depth=depths, drop=False)\n",
    "cruise = cruise.sel(frequency=38000, method='nearest').compute()  # materialize after selection\n",
    "cruise = cruise.where(cruise.depth < cruise.bottom + 2, drop=True)  # remove bins below bottom\n",
    "\n",
    "# Timestamps for later alignment\n",
    "hm_timestamps = cruise.time.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Accessing buoy data\n",
    "Defines a buoy location on Georges Bank (longitude converted to 0–360).\n",
    "1) Optional: Loads three daily OISST files and samples SST at the nearest grid point (day before, day of, day after).\n",
    "2) ERDDAP buoy environmental data.\n",
    "\n",
    "Sets ERDDAP dataset parameters and enforces a max_days cap by adjusting end_date_time if needed.\n",
    "Reads station metadata to extract lon/lat and wind-speed units; prepares a conversion to knots.\n",
    "Pulls a table of time, wind_speed, SST, significant wave height, converts wind speed to knots, indexes by time.\n",
    "Filters to the requested window and keeps the first nine rows (intentional truncation for later alignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of one specific buoy located on Georges Bank\n",
    "target_lon = 360 - 66.546  # convert from -180..180 to 0..360\n",
    "target_lat = 41.088\n",
    "\n",
    "# ______________OPTIONAL BUOY DATA FROM NCAR/UCAR______________\n",
    "# print(f\"Target coordinates: Longitude: {target_lon}, Latitude: {target_lat}\")\n",
    "#\n",
    "# # Accessing stationary buoy data (daily OISST files); select nearest grid cell\n",
    "# buoy_data_day_before = xr.open_dataset(\n",
    "#     'https://data.gdex.ucar.edu/d277007/avhrr_v2.1/2019/oisst-avhrr-v02r01.20191015.nc#mode=bytes', engine='netcdf4')\n",
    "# buoy_data_actual_day = xr.open_dataset(\n",
    "#     'https://data.gdex.ucar.edu/d277007/avhrr_v2.1/2019/oisst-avhrr-v02r01.20191016.nc#mode=bytes',\n",
    "#     engine='netcdf4')\n",
    "# buoy_data_day_after = xr.open_dataset(\n",
    "#     'https://data.gdex.ucar.edu/d277007/avhrr_v2.1/2019/oisst-avhrr-v02r01.20191017.nc#mode=bytes',\n",
    "#     engine='netcdf4')\n",
    "#\n",
    "# sst_day_before = buoy_data_day_before['sst'].sel(lon=target_lon, lat=target_lat, method='nearest').values[0][0]\n",
    "# sst_actual_day = buoy_data_actual_day['sst'].sel(lon=target_lon, lat=target_lat, method='nearest').values[0][0]\n",
    "# sst_day_after = buoy_data_day_after['sst'].sel(lon=target_lon, lat=target_lat, method='nearest').values[0][0]\n",
    "# ______________________________________________________________\n",
    "\n",
    "# The following code has been copied from https://github.com/ioos/soundcoop/blob/main/3_HMD_environmental_data/plot_sound_environmental_and_climatology_data.ipynb in order to access buoy environmental data.\n",
    "\n",
    "erddap_dataset = 'gov-ndbc-44005'\n",
    "sound_dataset = 'Monh'\n",
    "max_days = 25\n",
    "start_date_time = '2019-10-16T14:00:00.000'\n",
    "end_date_time = '2021-10-16T23:30:00.000'\n",
    "min_frequency = 21\n",
    "max_frequency = 24000\n",
    "\n",
    "erddap_base_url = 'https://erddap.sensors.ioos.us/erddap'\n",
    "\n",
    "# Cap the end date if requested range exceeds max_days\n",
    "time_delta = datetime.fromisoformat(end_date_time) - datetime.fromisoformat(start_date_time)\n",
    "if time_delta.days > max_days:\n",
    "    end_date_time = str(datetime.fromisoformat(start_date_time) + timedelta(days=max_days))\n",
    "    print(f'end_date_time updated to {end_date_time}')\n",
    "\n",
    "# Get station lon/lat and units from ERDDAP metadata (CSV)\n",
    "erddap_metadata_url = f'{erddap_base_url}/info/{erddap_dataset}/index.csv'\n",
    "env_metadata_df = pd.read_csv(erddap_metadata_url)\n",
    "\n",
    "env_station_x = env_metadata_df.loc[env_metadata_df['Attribute Name'] == 'geospatial_lon_min']['Value'].item()\n",
    "env_station_y = env_metadata_df.loc[env_metadata_df['Attribute Name'] == 'geospatial_lat_min']['Value'].item()\n",
    "swt_var = 'sea_surface_temperature'\n",
    "\n",
    "# __________OPTIONAL: Add Wind__________\n",
    "# Determine wind_speed units to convert to knots\n",
    "# wind_speed_units_row = env_metadata_df[\n",
    "#     (env_metadata_df['Row Type'] == 'attribute') &\n",
    "#     (env_metadata_df['Attribute Name'] == 'units') &\n",
    "#     # (env_metadata_df['Variable Name'] == 'wind_speed')\n",
    "#     ]\n",
    "# wind_speed_units = wind_speed_units_row.iloc[0]['Value']\n",
    "# print(wind_speed_units)\n",
    "\n",
    "# wind_speed_to_kts_factors = {\n",
    "#     \"m.s-1\": 1.94384,\n",
    "#     \"mph\": 0.86897423357831,\n",
    "#     \"kmh\": 0.53995555554212126825,\n",
    "#     \"ft.s-1\": 0.59248243198521155506\n",
    "# }\n",
    "\n",
    "# if wind_speed_units in wind_speed_to_kts_factors:\n",
    "#     print(\"Success! Units can be converted from\", wind_speed_units, 'to', 'kts')\n",
    "# else:\n",
    "#     print(\"Error! Wind speed cannot be converted from\", wind_speed_units, 'to', 'kts')\n",
    "\n",
    "# wind_var = 'wind_speed'\n",
    "# wave_var = 'sea_surface_wave_significant_height'\n",
    "# anomaly_var = 'swt_anomaly'\n",
    "# wind_var_kts = 'wind_speed_kts'\n",
    "# ________________________________________\n",
    "\n",
    "# Build ERDDAP tabledap query URL\n",
    "erddap_dataset_url = (\n",
    "    f'{erddap_base_url}/tabledap/{erddap_dataset}.csv'\n",
    "    f'?time,{swt_var}'\n",
    ")\n",
    "\n",
    "# Read dataset (skip the second row of units)\n",
    "env_df = pd.read_csv(\n",
    "    erddap_dataset_url,\n",
    "    skiprows=[1]  # The second row (index 1) are the column units, which we don't need\n",
    ")\n",
    "\n",
    "# Format time, convert wind speed to knots, index by time\n",
    "env_df['time'] = pd.to_datetime(env_df['time'])\n",
    "# env_df['wind_speed_kts'] = env_df['wind_speed'].apply(lambda x: x * wind_speed_to_kts_factors[wind_speed_units])\n",
    "# del env_df['wind_speed']\n",
    "env_df = env_df.set_index('time').sort_index()\n",
    "\n",
    "# Filter by requested time window and keep first 9 rows (drops the rest)\n",
    "env_df = env_df[(env_df.index > start_date_time) & (env_df.index < end_date_time)]\n",
    "env_df.drop(env_df.tail(-9).index, inplace=True)\n",
    "# env_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Calculate the temperature anomaly, sun elevation in degree and azimuth\n",
    "Extracts World Ocean Atlas 2023 temperature data for a specific location and month and calculates temperature anomaly (optional), sun elevation in degree and azimuth (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_woa23_temp_at_xy(x, y, month, var='t_mn', depth=0):\n",
    "    \"\"\"\n",
    "    Get 1-degree WOA 2023 temperature values for a given point and month.\n",
    "\n",
    "    Args:\n",
    "        x: A longitude value given in decimal degrees\n",
    "        y: A latitude value given in decimal degrees\n",
    "        month: The month asn integer from which to extract the value\n",
    "        var (optional): The temperature variable to use. Defaults to the statistical mean.\n",
    "        depth (optional): The depth at which to extract the value. Defaults to the surface.\n",
    "    \"\"\"\n",
    "    url = (\n",
    "        'https://www.ncei.noaa.gov/thredds-ocean/dodsC/woa23/DATA/'\n",
    "        f'temperature/netcdf/decav/1.00/woa23_decav_t{month:02}_01.nc'\n",
    "    )\n",
    "    ds = xr.open_dataset(\n",
    "        url,\n",
    "        decode_times=False  # xarray can't handle times defined as \"months since ...\"\n",
    "    )\n",
    "\n",
    "    da = ds.isel(depth=depth)[var]  # Pull out just the variable we're interested in\n",
    "\n",
    "    # Because nearshore locations are often NaN due to the grid's low resolution\n",
    "    # we need to interpolate the NaNs to the nearest non-NaN before extracting our value.\n",
    "    # We use rioxarray to do the interpolations in two dimensions because plain vanilla xarray\n",
    "    # can only interpolate in one dimension.\n",
    "    da = da.rio.write_crs(4326)\n",
    "    da = da.rio.interpolate_na(method='nearest')\n",
    "\n",
    "    # Then we extract the value, also using the nearest neighbor method because the given\n",
    "    # x and y values are unlikely to fall exactly on one of the grid's lat/lon coordinate pairs\n",
    "    val = da.sel(lon=x, lat=y, method='nearest').item()\n",
    "\n",
    "    return val\n",
    "\n",
    "\n",
    "# Define the location of our selected ERDDAP dataset\n",
    "# Override here if needed\n",
    "x = env_station_x\n",
    "y = env_station_y\n",
    "\n",
    "url = (\n",
    "    'https://www.ncei.noaa.gov/thredds-ocean/dodsC/woa23/DATA/'\n",
    "    f'temperature/netcdf/decav/1.00/woa23_decav_t07_01.nc'\n",
    ")\n",
    "da = xr.open_dataset(\n",
    "    url,\n",
    "    decode_times=False  # xarray can't handle times defined as \"months since ...\"\n",
    ").isel(depth=0)['t_mn']  # Pull out just the variable we're interested in\n",
    "\n",
    "# Because nearshore locations are often NaN due to the grid's low resolution\n",
    "# we need to interpolate the NaNs to the nearest non-NaN before extracting our value.\n",
    "# We use rioxarray to do the interpolations in two dimensions because plain vanilla xarray\n",
    "# can only interpolate in one dimension.\n",
    "da = da.rio.write_crs(4326)\n",
    "da = da.rio.interpolate_na(method='nearest')\n",
    "\n",
    "# Then we extract the value, also using the nearest neighbor method because the given\n",
    "# x and y values are unlikely to fall exactly on one of the grid's lat/lon coordinate pairs\n",
    "val = da.sel(lon=x, lat=y, method='nearest').item()\n",
    "\n",
    "# Assemble a mapping between months and WOA 2023 temperature values\n",
    "months = list(range(1, 13))\n",
    "temps = [get_woa23_temp_at_xy(x, y, m) for m in months]\n",
    "clim_dict = {m: t for m, t in zip(months, temps)}\n",
    "\n",
    "# Calculate the sea water temperature anomaly by subtracting the monthly WOA 2023 temperature value\n",
    "# from each measured sea water temperature value and store it as a new variable\n",
    "anomaly_var = env_df[swt_var] - [clim_dict[10]]\n",
    "# We are not adding the temperature_anomaly variable to our dataset, because we were able to see that it follows the sea surface temperature.\n",
    "# env_df[\"temperature_anomaly\"] = anomaly_var\n",
    "\n",
    "# ---- Time range in UTC ----\n",
    "times_utc = pd.date_range(\n",
    "    start=start_date_time,\n",
    "    end=end_date_time,\n",
    "    freq=\"1h\",\n",
    "    tz=\"UTC\"  # <-- key: set timezone to UTC\n",
    ")\n",
    "\n",
    "# ---- Calculate solar position ----\n",
    "solpos = pvlib.solarposition.get_solarposition(times_utc, target_lat, target_lon)\n",
    "\n",
    "# ---- Extract elevation ----\n",
    "df = pd.DataFrame({\n",
    "    \"time_utc\": times_utc,\n",
    "    \"elevation_deg\": solpos[\"elevation\"],\n",
    "    \"azimuth_deg\": solpos[\"azimuth\"]\n",
    "})\n",
    "\n",
    "env_df[\"elevation_deg\"] = solpos[\"elevation\"].tolist()[1:10]\n",
    "# env_df[\"azimuth_deg\"] = solpos[\"azimuth\"].tolist()[:9]\n",
    "env_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Downloading external error map for the specific location.\n",
    "Currently the error map comes from a fixed file; our plan is to switch to a dynamic AWS download that accepts location parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading anomaly detection model error map from NCAR via OSDF\n",
    "response = requests.get('https://osdf-data.gdex.ucar.edu/special_projects/pythia_2025/osdf-cookbook/mae_error_map.npy')\n",
    "response.raise_for_status()\n",
    "sonar_clusters = np.load(io.BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Helper: mean Sv in dB\n",
    "Computes the mean of Sv correctly by converting dB → linear, averaging, then linear → dB.\n",
    "Accepts array-like input (NumPy/xarray/dask); returns a scalar in dB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sv_mean(input_sv):\n",
    "    # Convert dB to linear, mean in linear space, convert back to dB\n",
    "    sv = 10. ** (input_sv / 10.)\n",
    "    return 10 * np.log10(np.mean(sv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Group cruise data into hourly chunks\n",
    "Adds an hourly label and groups the cruise data by hour.\n",
    "Produces a list of per-hour xarray.Dataset chunks for downstream aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cruise['time_hour'] = cruise['time'].dt.floor('1h')  # hourly bin label\n",
    "\n",
    "# Group by each hour\n",
    "grouped = cruise.groupby('time_hour')\n",
    "\n",
    "# Extract each 1-hour Dataset as a chunk (drop helper label)\n",
    "chunks = [group.drop_vars('time_hour') for _, group in grouped]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Compute hourly mean Sv and attach to env_df\n",
    "Iterates over hourly chunks, computes mean Sv per hour using calculate_sv_mean.\n",
    "Converts dask→NumPy→Python float and appends to a list.\n",
    "Assigns the resulting hourly series to env_df[\"sv_hourly\"].\n",
    "Assumes the number/order of hours matches rows retained in env_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_hourly = []\n",
    "timestamps = []\n",
    "\n",
    "for i in range(0, len(chunks)):\n",
    "    sv_data = chunks[i]['Sv']\n",
    "    result = calculate_sv_mean(sv_data)\n",
    "\n",
    "    # Use first time in hour as representative timestamp\n",
    "    ts = pd.to_datetime(chunks[i]['time'].values[0])\n",
    "    result = result.compute()  # dask -> numpy\n",
    "    result = float(result.values)  # numpy -> Python float\n",
    "\n",
    "    sv_hourly.append(result)\n",
    "\n",
    "env_df[\"sv_hourly\"] = sv_hourly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Build (depth × time) error-map DataFrame and align timestamps\n",
    "Extracts one channel from sonar_clusters and pairs it with cruise depths and timestamps to form a DataFrame.\n",
    "Uses min(...) to guard against size mismatches in depth/time dimensions.\n",
    "Aligns only the first and last timestamps in env_df to the heatmap’s time range (keeps interior indices unchanged, sets UTC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare axes\n",
    "depths = np.asarray(cruise.depth.values)\n",
    "times = pd.to_datetime(hm_timestamps)\n",
    "\n",
    "# Select channel/slice from sonar_clusters\n",
    "vals = sonar_clusters[:, :, 1]  # (1088, 28096)\n",
    "\n",
    "# Guard against mismatched sizes\n",
    "n_depth = min(len(depths), vals.shape[0])\n",
    "n_time = min(len(times), vals.shape[1])\n",
    "\n",
    "# DataFrame: rows=depths, cols=timestamps\n",
    "df = pd.DataFrame(\n",
    "    data=vals[:n_depth, :n_time],\n",
    "    index=depths[:n_depth],\n",
    "    columns=times[:n_time]\n",
    ")\n",
    "\n",
    "# Align env_df index endpoints to heatmap timestamps (keeps interior unchanged)\n",
    "idx = env_df.index.tolist()\n",
    "df_timestamps = pd.to_datetime(df.columns).tz_localize(None)\n",
    "idx[0] = pd.Timestamp(df_timestamps.values[0], tz='UTC').floor(\"s\")\n",
    "idx[-1] = pd.Timestamp(df_timestamps.values[-1], tz='UTC').floor(\"s\")\n",
    "env_df.index = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Data Visualization: Synchronized Lines + Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots synchronized data: top = time series from line_df; bottom = depth×time heatmap from heatmap_df.\n",
    "Expects line_df to have a DatetimeIndex (timezone-naive or converted).\n",
    "Depth axis is reversed (surface at top).\n",
    "Saves an interactive HTML file to the parent directory (correlations.html) and shows the figure if show=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_synchronized_heatmaps_from_df(\n",
    "        heatmap_df: pd.DataFrame,\n",
    "        line_df: pd.DataFrame,\n",
    "        colorscale: str = \"Reds\",\n",
    "        show_markers: bool = False,\n",
    "        show: bool = False,\n",
    "):\n",
    "    if not isinstance(line_df.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"line_df must have a DatetimeIndex\")\n",
    "    line_df = line_df.copy()\n",
    "    if line_df.index.tz is not None:\n",
    "        line_df.index = line_df.index.tz_convert(None)\n",
    "\n",
    "    depths = np.asarray(heatmap_df.index)\n",
    "    heatmap_timestamps = pd.to_datetime(heatmap_df.columns)\n",
    "    z = heatmap_df.to_numpy()\n",
    "\n",
    "    n = len(line_df.columns)\n",
    "    fig = make_subplots(\n",
    "        rows=n + 1,\n",
    "        cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.02,\n",
    "        row_heights=[0.1] * n + [0.7],  # n small rows + large heatmap row\n",
    "    )\n",
    "\n",
    "    mode = \"lines+markers\" if show_markers else \"lines\"\n",
    "\n",
    "    # Define units for each variable\n",
    "    units = {\n",
    "        'sea_surface_temperature': '°C',\n",
    "        'elevation_deg': 'degrees',\n",
    "        'sv_hourly': 'dB'\n",
    "    }\n",
    "\n",
    "    for i, col in enumerate(line_df.columns, start=1):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=line_df.index, y=line_df[col], name=str(col), mode=mode, showlegend=True),\n",
    "            row=i, col=1\n",
    "        )\n",
    "\n",
    "        # Add units as y-axis titles\n",
    "        unit = units.get(col, '')  # Default to empty string if variable not found\n",
    "        fig.update_yaxes(\n",
    "            title_text=unit,\n",
    "            row=i, col=1\n",
    "        )\n",
    "\n",
    "        if i < len(line_df.columns):\n",
    "            fig.update_xaxes(showticklabels=False, row=i, col=1)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=z, x=heatmap_timestamps, y=depths, colorscale=colorscale,\n",
    "            zmin=np.nanmin(z), zmax=np.nanmax(z),\n",
    "            hovertemplate=\"t=%{x}<br>depth=%{y}<br>value=%{z}<extra></extra>\",\n",
    "        ),\n",
    "        row=n + 1, col=1\n",
    "    )\n",
    "    fig.update_yaxes(autorange=\"reversed\", row=n + 1, col=1, title_text=\"Depth\")\n",
    "\n",
    "    fig.update_layout(\n",
    "        margin=dict(l=40, r=40, t=60, b=40),  # Reduced left margin, increased top for legend\n",
    "        hovermode=\"x unified\",\n",
    "        template=\"plotly_white\",\n",
    "        # height=10 * n + 500,  # scale height with number of signals\n",
    "    )\n",
    "\n",
    "    # Enhanced horizontal legend positioning\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            orientation='h',\n",
    "            x=0,\n",
    "            y=1.02,\n",
    "            xanchor='left',\n",
    "            yanchor='bottom',\n",
    "            bgcolor='rgba(255,255,255,0.8)',\n",
    "            bordercolor='rgba(0,0,0,0.1)',\n",
    "            borderwidth=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    save_path = os.path.join(os.path.dirname(os.getcwd()), \"correlations.html\")\n",
    "    fig.write_html(save_path)\n",
    "    print(f\"Plot saved to: {save_path}\")\n",
    "    if show:\n",
    "        fig.show()\n",
    "    return fig\n",
    "# Uncomment this line in order to create the plot.\n",
    "# fig = plot_synchronized_heatmaps_from_df(heatmap_df=df, line_df=env_df)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because rendering the plot is computationally intensive and involves downloading approximately 1 GB of data, we present a static image of the result instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T19:32:51.371053Z",
     "start_time": "2025-08-15T19:32:51.362008Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename='../images/sonarai_example.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python:osdf",
   "language": "python",
   "name": "osdf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ]
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
